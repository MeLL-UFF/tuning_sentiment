INFO:__main__:inputFile: ../../language-detector/fasttext/twitter_edin_97_en, outputDir: ../outputs/edin_tuned/bert-ft, model: 1
INFO:__main__:iniciando...
INFO:__main__:running ft bert
INFO:filelock:Lock 140494376442512 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpb9trfkd8
INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:filelock:Lock 140494376442512 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:filelock:Lock 140494376440448 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpfwg13rxn
INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
INFO:filelock:Lock 140494376440448 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

INFO:filelock:Lock 140494376521056 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
INFO:transformers.file_utils:https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpgery2mdh
INFO:transformers.file_utils:storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:filelock:Lock 140494376521056 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock
INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertModel.

INFO:transformers.modeling_utils:All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
WARNING:transformers.modeling_utils:Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:transformers.data.datasets.language_modeling:Creating features from dataset file at ../../language-detector/fasttext/twitter_edin_97_en
INFO:transformers.training_args:PyTorch: setting up devices
INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.
WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.
INFO:transformers.trainer:***** Running training *****
INFO:transformers.trainer:  Num examples = 6657717
INFO:transformers.trainer:  Num Epochs = 3
INFO:transformers.trainer:  Instantaneous batch size per device = 8
INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 8
INFO:transformers.trainer:  Gradient Accumulation steps = 1
INFO:transformers.trainer:  Total optimization steps = 2496645
INFO:transformers.trainer:{'loss': 2.9104166569709777, 'learning_rate': 4.998998656196616e-05, 'epoch': 0.000600806282030485, 'step': 500}
INFO:transformers.trainer:{'loss': 2.801254701137543, 'learning_rate': 4.997997312393232e-05, 'epoch': 0.00120161256406097, 'step': 1000}
INFO:transformers.trainer:{'loss': 2.7800501375198365, 'learning_rate': 4.9969959685898474e-05, 'epoch': 0.0018024188460914548, 'step': 1500}
INFO:transformers.trainer:{'loss': 2.758346115589142, 'learning_rate': 4.995994624786464e-05, 'epoch': 0.00240322512812194, 'step': 2000}
INFO:transformers.trainer:{'loss': 2.767091588497162, 'learning_rate': 4.9949932809830794e-05, 'epoch': 0.0030040314101524245, 'step': 2500}
INFO:transformers.trainer:{'loss': 2.7922155896425247, 'learning_rate': 4.993991937179696e-05, 'epoch': 0.0036048376921829096, 'step': 3000}
INFO:transformers.trainer:{'loss': 2.7457657704353333, 'learning_rate': 4.992990593376311e-05, 'epoch': 0.004205643974213394, 'step': 3500}
INFO:transformers.trainer:{'loss': 2.8070679008960724, 'learning_rate': 4.991989249572927e-05, 'epoch': 0.00480645025624388, 'step': 4000}
INFO:transformers.trainer:{'loss': 2.768924326181412, 'learning_rate': 4.990987905769543e-05, 'epoch': 0.005407256538274364, 'step': 4500}
INFO:transformers.trainer:{'loss': 2.7438865180015566, 'learning_rate': 4.9899865619661587e-05, 'epoch': 0.006008062820304849, 'step': 5000}
INFO:transformers.trainer:{'loss': 2.7206937527656554, 'learning_rate': 4.9889852181627744e-05, 'epoch': 0.006608869102335334, 'step': 5500}
INFO:transformers.trainer:{'loss': 2.691521273136139, 'learning_rate': 4.98798387435939e-05, 'epoch': 0.007209675384365819, 'step': 6000}
INFO:transformers.trainer:{'loss': 2.7004910188913347, 'learning_rate': 4.9869825305560065e-05, 'epoch': 0.007810481666396304, 'step': 6500}
INFO:transformers.trainer:{'loss': 2.7179887174367905, 'learning_rate': 4.985981186752622e-05, 'epoch': 0.008411287948426788, 'step': 7000}
INFO:transformers.trainer:{'loss': 2.721534850358963, 'learning_rate': 4.9849798429492385e-05, 'epoch': 0.009012094230457273, 'step': 7500}
INFO:transformers.trainer:{'loss': 2.7033734874725344, 'learning_rate': 4.9839784991458536e-05, 'epoch': 0.00961290051248776, 'step': 8000}
INFO:transformers.trainer:{'loss': 2.6611315343379975, 'learning_rate': 4.98297715534247e-05, 'epoch': 0.010213706794518244, 'step': 8500}
INFO:transformers.trainer:{'loss': 2.663805815219879, 'learning_rate': 4.981975811539086e-05, 'epoch': 0.010814513076548729, 'step': 9000}
INFO:transformers.trainer:{'loss': 2.667488575220108, 'learning_rate': 4.980974467735702e-05, 'epoch': 0.011415319358579213, 'step': 9500}
INFO:transformers.trainer:{'loss': 2.722258910655975, 'learning_rate': 4.979973123932317e-05, 'epoch': 0.012016125640609698, 'step': 10000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-10000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-10000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-10000/pytorch_model.bin
INFO:transformers.trainer:{'loss': 2.638130213022232, 'learning_rate': 4.9789717801289335e-05, 'epoch': 0.012616931922640183, 'step': 10500}
INFO:transformers.trainer:{'loss': 2.709126893281937, 'learning_rate': 4.977970436325549e-05, 'epoch': 0.013217738204670667, 'step': 11000}
INFO:transformers.trainer:{'loss': 2.660363569974899, 'learning_rate': 4.976969092522165e-05, 'epoch': 0.013818544486701154, 'step': 11500}
INFO:transformers.trainer:{'loss': 2.70741504406929, 'learning_rate': 4.9759677487187806e-05, 'epoch': 0.014419350768731638, 'step': 12000}
INFO:transformers.trainer:{'loss': 2.6865292541980743, 'learning_rate': 4.974966404915396e-05, 'epoch': 0.015020157050762123, 'step': 12500}
INFO:transformers.trainer:{'loss': 2.6739824554920197, 'learning_rate': 4.973965061112013e-05, 'epoch': 0.015620963332792608, 'step': 13000}
INFO:transformers.trainer:{'loss': 2.6404122576713562, 'learning_rate': 4.9729637173086284e-05, 'epoch': 0.016221769614823092, 'step': 13500}
INFO:transformers.trainer:{'loss': 2.6592519807815553, 'learning_rate': 4.971962373505245e-05, 'epoch': 0.016822575896853577, 'step': 14000}
INFO:transformers.trainer:{'loss': 2.68546316742897, 'learning_rate': 4.97096102970186e-05, 'epoch': 0.01742338217888406, 'step': 14500}
INFO:transformers.trainer:{'loss': 2.6601474521160124, 'learning_rate': 4.969959685898476e-05, 'epoch': 0.018024188460914546, 'step': 15000}
INFO:transformers.trainer:{'loss': 2.723989629983902, 'learning_rate': 4.968958342095092e-05, 'epoch': 0.01862499474294503, 'step': 15500}
INFO:transformers.trainer:{'loss': 2.696502815961838, 'learning_rate': 4.9679569982917076e-05, 'epoch': 0.01922580102497552, 'step': 16000}
INFO:transformers.trainer:{'loss': 2.6968057132959364, 'learning_rate': 4.966955654488323e-05, 'epoch': 0.019826607307006004, 'step': 16500}
INFO:transformers.trainer:{'loss': 2.6645741646289824, 'learning_rate': 4.965954310684939e-05, 'epoch': 0.020427413589036488, 'step': 17000}
INFO:transformers.trainer:{'loss': 2.7035816330909728, 'learning_rate': 4.9649529668815554e-05, 'epoch': 0.021028219871066973, 'step': 17500}
INFO:transformers.trainer:{'loss': 2.6894548556804656, 'learning_rate': 4.963951623078171e-05, 'epoch': 0.021629026153097457, 'step': 18000}
INFO:transformers.trainer:{'loss': 2.6685499036312104, 'learning_rate': 4.962950279274787e-05, 'epoch': 0.022229832435127942, 'step': 18500}
INFO:transformers.trainer:{'loss': 2.7012638387680052, 'learning_rate': 4.9619489354714025e-05, 'epoch': 0.022830638717158427, 'step': 19000}
INFO:transformers.trainer:{'loss': 2.6592703759670258, 'learning_rate': 4.960947591668019e-05, 'epoch': 0.02343144499918891, 'step': 19500}
INFO:transformers.trainer:{'loss': 2.7272511768341063, 'learning_rate': 4.9599462478646346e-05, 'epoch': 0.024032251281219396, 'step': 20000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-20000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-20000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-20000/pytorch_model.bin
INFO:transformers.trainer:{'loss': 2.654584052503109, 'learning_rate': 4.958944904061251e-05, 'epoch': 0.02463305756324988, 'step': 20500}
INFO:transformers.trainer:{'loss': 2.64357835829258, 'learning_rate': 4.957943560257866e-05, 'epoch': 0.025233863845280365, 'step': 21000}
INFO:transformers.trainer:{'loss': 2.70276092338562, 'learning_rate': 4.9569422164544824e-05, 'epoch': 0.02583467012731085, 'step': 21500}
INFO:transformers.trainer:{'loss': 2.6907365367412566, 'learning_rate': 4.955940872651098e-05, 'epoch': 0.026435476409341335, 'step': 22000}
INFO:transformers.trainer:{'loss': 2.68953272151947, 'learning_rate': 4.954939528847714e-05, 'epoch': 0.027036282691371823, 'step': 22500}
INFO:transformers.trainer:{'loss': 2.7431477370262147, 'learning_rate': 4.9539381850443295e-05, 'epoch': 0.027637088973402307, 'step': 23000}
INFO:transformers.trainer:{'loss': 2.6346221288442613, 'learning_rate': 4.952936841240945e-05, 'epoch': 0.028237895255432792, 'step': 23500}
INFO:transformers.trainer:{'loss': 2.6327211237549784, 'learning_rate': 4.9519354974375616e-05, 'epoch': 0.028838701537463277, 'step': 24000}
INFO:transformers.trainer:{'loss': 2.6814515566825867, 'learning_rate': 4.950934153634177e-05, 'epoch': 0.02943950781949376, 'step': 24500}
INFO:transformers.trainer:{'loss': 2.6927242555618287, 'learning_rate': 4.949932809830794e-05, 'epoch': 0.030040314101524246, 'step': 25000}
INFO:transformers.trainer:{'loss': 2.675818021297455, 'learning_rate': 4.948931466027409e-05, 'epoch': 0.03064112038355473, 'step': 25500}
INFO:transformers.trainer:{'loss': 2.6407143337726593, 'learning_rate': 4.947930122224025e-05, 'epoch': 0.031241926665585215, 'step': 26000}
INFO:transformers.trainer:{'loss': 2.68835673224926, 'learning_rate': 4.946928778420641e-05, 'epoch': 0.0318427329476157, 'step': 26500}
INFO:transformers.trainer:{'loss': 2.6918250082731245, 'learning_rate': 4.9459274346172565e-05, 'epoch': 0.032443539229646184, 'step': 27000}
INFO:transformers.trainer:{'loss': 2.630460840702057, 'learning_rate': 4.944926090813872e-05, 'epoch': 0.03304434551167667, 'step': 27500}
INFO:transformers.trainer:{'loss': 2.6089031831622123, 'learning_rate': 4.943924747010488e-05, 'epoch': 0.033645151793707154, 'step': 28000}
INFO:transformers.trainer:{'loss': 2.6750185444355012, 'learning_rate': 4.942923403207104e-05, 'epoch': 0.03424595807573764, 'step': 28500}
INFO:transformers.trainer:{'loss': 2.712099715471268, 'learning_rate': 4.94192205940372e-05, 'epoch': 0.03484676435776812, 'step': 29000}
INFO:transformers.trainer:{'loss': 2.66641587805748, 'learning_rate': 4.940920715600336e-05, 'epoch': 0.03544757063979861, 'step': 29500}
INFO:transformers.trainer:{'loss': 2.6695949598550794, 'learning_rate': 4.9399193717969514e-05, 'epoch': 0.03604837692182909, 'step': 30000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-30000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-30000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-30000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-10000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.648869553565979, 'learning_rate': 4.938918027993568e-05, 'epoch': 0.03664918320385958, 'step': 30500}
INFO:transformers.trainer:{'loss': 2.690336232542992, 'learning_rate': 4.9379166841901835e-05, 'epoch': 0.03724998948589006, 'step': 31000}
INFO:transformers.trainer:{'loss': 2.70412832903862, 'learning_rate': 4.9369153403868e-05, 'epoch': 0.037850795767920546, 'step': 31500}
INFO:transformers.trainer:{'loss': 2.666305497407913, 'learning_rate': 4.935913996583415e-05, 'epoch': 0.03845160204995104, 'step': 32000}
INFO:transformers.trainer:{'loss': 2.7037755863666533, 'learning_rate': 4.934912652780031e-05, 'epoch': 0.03905240833198152, 'step': 32500}
INFO:transformers.trainer:{'loss': 2.625042552947998, 'learning_rate': 4.933911308976647e-05, 'epoch': 0.03965321461401201, 'step': 33000}
INFO:transformers.trainer:{'loss': 2.6251106581687926, 'learning_rate': 4.932909965173263e-05, 'epoch': 0.04025402089604249, 'step': 33500}
INFO:transformers.trainer:{'loss': 2.6485129461288452, 'learning_rate': 4.9319086213698784e-05, 'epoch': 0.040854827178072976, 'step': 34000}
INFO:transformers.trainer:{'loss': 2.6750214247703554, 'learning_rate': 4.930907277566494e-05, 'epoch': 0.04145563346010346, 'step': 34500}
INFO:transformers.trainer:{'loss': 2.707833349466324, 'learning_rate': 4.9299059337631105e-05, 'epoch': 0.042056439742133946, 'step': 35000}
INFO:transformers.trainer:{'loss': 2.6412005772590637, 'learning_rate': 4.928904589959726e-05, 'epoch': 0.04265724602416443, 'step': 35500}
INFO:transformers.trainer:{'loss': 2.6497649595737456, 'learning_rate': 4.927903246156342e-05, 'epoch': 0.043258052306194915, 'step': 36000}
INFO:transformers.trainer:{'loss': 2.6446122269630434, 'learning_rate': 4.9269019023529577e-05, 'epoch': 0.0438588585882254, 'step': 36500}
INFO:transformers.trainer:{'loss': 2.6722456259727476, 'learning_rate': 4.925900558549574e-05, 'epoch': 0.044459664870255884, 'step': 37000}
INFO:transformers.trainer:{'loss': 2.68228358066082, 'learning_rate': 4.92489921474619e-05, 'epoch': 0.04506047115228637, 'step': 37500}
INFO:transformers.trainer:{'loss': 2.6392718077898025, 'learning_rate': 4.9238978709428054e-05, 'epoch': 0.04566127743431685, 'step': 38000}
INFO:transformers.trainer:{'loss': 2.6610928506851197, 'learning_rate': 4.922896527139421e-05, 'epoch': 0.04626208371634734, 'step': 38500}
INFO:transformers.trainer:{'loss': 2.668123101234436, 'learning_rate': 4.921895183336037e-05, 'epoch': 0.04686288999837782, 'step': 39000}
INFO:transformers.trainer:{'loss': 2.671785305261612, 'learning_rate': 4.920893839532653e-05, 'epoch': 0.04746369628040831, 'step': 39500}
INFO:transformers.trainer:{'loss': 2.653753132581711, 'learning_rate': 4.919892495729269e-05, 'epoch': 0.04806450256243879, 'step': 40000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-40000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-40000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-40000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-20000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6578729507923127, 'learning_rate': 4.9188911519258847e-05, 'epoch': 0.04866530884446928, 'step': 40500}
INFO:transformers.trainer:{'loss': 2.6845983583927153, 'learning_rate': 4.9178898081225004e-05, 'epoch': 0.04926611512649976, 'step': 41000}
INFO:transformers.trainer:{'loss': 2.655870681643486, 'learning_rate': 4.916888464319117e-05, 'epoch': 0.049866921408530246, 'step': 41500}
INFO:transformers.trainer:{'loss': 2.6511256465911863, 'learning_rate': 4.9158871205157325e-05, 'epoch': 0.05046772769056073, 'step': 42000}
INFO:transformers.trainer:{'loss': 2.677856420278549, 'learning_rate': 4.914885776712348e-05, 'epoch': 0.051068533972591215, 'step': 42500}
INFO:transformers.trainer:{'loss': 2.712809505820274, 'learning_rate': 4.913884432908964e-05, 'epoch': 0.0516693402546217, 'step': 43000}
INFO:transformers.trainer:{'loss': 2.661321057319641, 'learning_rate': 4.91288308910558e-05, 'epoch': 0.052270146536652184, 'step': 43500}
INFO:transformers.trainer:{'loss': 2.7142799932956696, 'learning_rate': 4.911881745302196e-05, 'epoch': 0.05287095281868267, 'step': 44000}
INFO:transformers.trainer:{'loss': 2.6640116736888886, 'learning_rate': 4.910880401498812e-05, 'epoch': 0.053471759100713154, 'step': 44500}
INFO:transformers.trainer:{'loss': 2.7158919496536256, 'learning_rate': 4.9098790576954274e-05, 'epoch': 0.054072565382743645, 'step': 45000}
INFO:transformers.trainer:{'loss': 2.6607702808380127, 'learning_rate': 4.908877713892043e-05, 'epoch': 0.05467337166477413, 'step': 45500}
INFO:transformers.trainer:{'loss': 2.6387709941864013, 'learning_rate': 4.9078763700886595e-05, 'epoch': 0.055274177946804615, 'step': 46000}
INFO:transformers.trainer:{'loss': 2.68597843170166, 'learning_rate': 4.906875026285275e-05, 'epoch': 0.0558749842288351, 'step': 46500}
INFO:transformers.trainer:{'loss': 2.649780865073204, 'learning_rate': 4.905873682481891e-05, 'epoch': 0.056475790510865584, 'step': 47000}
INFO:transformers.trainer:{'loss': 2.620241762161255, 'learning_rate': 4.9048723386785066e-05, 'epoch': 0.05707659679289607, 'step': 47500}
INFO:transformers.trainer:{'loss': 2.649793662548065, 'learning_rate': 4.903870994875123e-05, 'epoch': 0.05767740307492655, 'step': 48000}
INFO:transformers.trainer:{'loss': 2.645603744626045, 'learning_rate': 4.902869651071739e-05, 'epoch': 0.05827820935695704, 'step': 48500}
INFO:transformers.trainer:{'loss': 2.6820873000621797, 'learning_rate': 4.9018683072683544e-05, 'epoch': 0.05887901563898752, 'step': 49000}
INFO:transformers.trainer:{'loss': 2.6880471626520155, 'learning_rate': 4.90086696346497e-05, 'epoch': 0.05947982192101801, 'step': 49500}
INFO:transformers.trainer:{'loss': 2.664897749066353, 'learning_rate': 4.899865619661586e-05, 'epoch': 0.06008062820304849, 'step': 50000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-50000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-50000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-50000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-30000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.638856128573418, 'learning_rate': 4.898864275858202e-05, 'epoch': 0.060681434485078976, 'step': 50500}
INFO:transformers.trainer:{'loss': 2.66542165184021, 'learning_rate': 4.897862932054818e-05, 'epoch': 0.06128224076710946, 'step': 51000}
INFO:transformers.trainer:{'loss': 2.6295515277385713, 'learning_rate': 4.8968615882514336e-05, 'epoch': 0.061883047049139946, 'step': 51500}
INFO:transformers.trainer:{'loss': 2.6509743030071258, 'learning_rate': 4.895860244448049e-05, 'epoch': 0.06248385333117043, 'step': 52000}
INFO:transformers.trainer:{'loss': 2.715406826496124, 'learning_rate': 4.894858900644666e-05, 'epoch': 0.06308465961320092, 'step': 52500}
INFO:transformers.trainer:{'loss': 2.6504163867235184, 'learning_rate': 4.8938575568412814e-05, 'epoch': 0.0636854658952314, 'step': 53000}
INFO:transformers.trainer:{'loss': 2.6416623258590697, 'learning_rate': 4.892856213037897e-05, 'epoch': 0.06428627217726189, 'step': 53500}
INFO:transformers.trainer:{'loss': 2.6114220815896987, 'learning_rate': 4.891854869234513e-05, 'epoch': 0.06488707845929237, 'step': 54000}
INFO:transformers.trainer:{'loss': 2.6434974809885023, 'learning_rate': 4.890853525431129e-05, 'epoch': 0.06548788474132286, 'step': 54500}
INFO:transformers.trainer:{'loss': 2.6693162364959715, 'learning_rate': 4.889852181627745e-05, 'epoch': 0.06608869102335334, 'step': 55000}
INFO:transformers.trainer:{'loss': 2.708031612753868, 'learning_rate': 4.8888508378243606e-05, 'epoch': 0.06668949730538383, 'step': 55500}
INFO:transformers.trainer:{'loss': 2.612893476843834, 'learning_rate': 4.887849494020976e-05, 'epoch': 0.06729030358741431, 'step': 56000}
INFO:transformers.trainer:{'loss': 2.673136374235153, 'learning_rate': 4.886848150217592e-05, 'epoch': 0.0678911098694448, 'step': 56500}
INFO:transformers.trainer:{'loss': 2.6473289053440094, 'learning_rate': 4.8858468064142084e-05, 'epoch': 0.06849191615147528, 'step': 57000}
INFO:transformers.trainer:{'loss': 2.6455386810302732, 'learning_rate': 4.884845462610824e-05, 'epoch': 0.06909272243350577, 'step': 57500}
INFO:transformers.trainer:{'loss': 2.692778972864151, 'learning_rate': 4.88384411880744e-05, 'epoch': 0.06969352871553625, 'step': 58000}
INFO:transformers.trainer:{'loss': 2.6875625122785567, 'learning_rate': 4.8828427750040555e-05, 'epoch': 0.07029433499756674, 'step': 58500}
INFO:transformers.trainer:{'loss': 2.604834517598152, 'learning_rate': 4.881841431200672e-05, 'epoch': 0.07089514127959722, 'step': 59000}
INFO:transformers.trainer:{'loss': 2.6233788839578627, 'learning_rate': 4.8808400873972876e-05, 'epoch': 0.0714959475616277, 'step': 59500}
INFO:transformers.trainer:{'loss': 2.6444992693662646, 'learning_rate': 4.879838743593903e-05, 'epoch': 0.07209675384365818, 'step': 60000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-60000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-60000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-60000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-40000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.658553371191025, 'learning_rate': 4.878837399790519e-05, 'epoch': 0.07269756012568868, 'step': 60500}
INFO:transformers.trainer:{'loss': 2.6363864569664, 'learning_rate': 4.877836055987135e-05, 'epoch': 0.07329836640771915, 'step': 61000}
INFO:transformers.trainer:{'loss': 2.597419763803482, 'learning_rate': 4.876834712183751e-05, 'epoch': 0.07389917268974965, 'step': 61500}
INFO:transformers.trainer:{'loss': 2.680014954209328, 'learning_rate': 4.875833368380366e-05, 'epoch': 0.07449997897178012, 'step': 62000}
INFO:transformers.trainer:{'loss': 2.6868870183229445, 'learning_rate': 4.8748320245769825e-05, 'epoch': 0.07510078525381061, 'step': 62500}
INFO:transformers.trainer:{'loss': 2.6481173664331434, 'learning_rate': 4.873830680773598e-05, 'epoch': 0.07570159153584109, 'step': 63000}
INFO:transformers.trainer:{'loss': 2.618182786345482, 'learning_rate': 4.8728293369702146e-05, 'epoch': 0.07630239781787158, 'step': 63500}
INFO:transformers.trainer:{'loss': 2.683050094366074, 'learning_rate': 4.87182799316683e-05, 'epoch': 0.07690320409990208, 'step': 64000}
INFO:transformers.trainer:{'loss': 2.678088608980179, 'learning_rate': 4.870826649363446e-05, 'epoch': 0.07750401038193255, 'step': 64500}
INFO:transformers.trainer:{'loss': 2.6548181587457655, 'learning_rate': 4.869825305560062e-05, 'epoch': 0.07810481666396304, 'step': 65000}
INFO:transformers.trainer:{'loss': 2.640314851760864, 'learning_rate': 4.8688239617566774e-05, 'epoch': 0.07870562294599352, 'step': 65500}
INFO:transformers.trainer:{'loss': 2.693315272092819, 'learning_rate': 4.867822617953294e-05, 'epoch': 0.07930642922802401, 'step': 66000}
INFO:transformers.trainer:{'loss': 2.6825026166439057, 'learning_rate': 4.866821274149909e-05, 'epoch': 0.07990723551005449, 'step': 66500}
INFO:transformers.trainer:{'loss': 2.5881787538528442, 'learning_rate': 4.865819930346525e-05, 'epoch': 0.08050804179208498, 'step': 67000}
INFO:transformers.trainer:{'loss': 2.6601555468440057, 'learning_rate': 4.864818586543141e-05, 'epoch': 0.08110884807411546, 'step': 67500}
INFO:transformers.trainer:{'loss': 2.6889019811153414, 'learning_rate': 4.863817242739757e-05, 'epoch': 0.08170965435614595, 'step': 68000}
INFO:transformers.trainer:{'loss': 2.6937486019134522, 'learning_rate': 4.8628158989363723e-05, 'epoch': 0.08231046063817643, 'step': 68500}
INFO:transformers.trainer:{'loss': 2.681323971390724, 'learning_rate': 4.861814555132989e-05, 'epoch': 0.08291126692020692, 'step': 69000}
INFO:transformers.trainer:{'loss': 2.6801707100868226, 'learning_rate': 4.8608132113296044e-05, 'epoch': 0.0835120732022374, 'step': 69500}
INFO:transformers.trainer:{'loss': 2.643451714873314, 'learning_rate': 4.859811867526221e-05, 'epoch': 0.08411287948426789, 'step': 70000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-70000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-70000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-70000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-50000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.71414075422287, 'learning_rate': 4.8588105237228365e-05, 'epoch': 0.08471368576629837, 'step': 70500}
INFO:transformers.trainer:{'loss': 2.649527932405472, 'learning_rate': 4.857809179919452e-05, 'epoch': 0.08531449204832886, 'step': 71000}
INFO:transformers.trainer:{'loss': 2.6874038023352624, 'learning_rate': 4.856807836116068e-05, 'epoch': 0.08591529833035934, 'step': 71500}
INFO:transformers.trainer:{'loss': 2.636470912694931, 'learning_rate': 4.8558064923126836e-05, 'epoch': 0.08651610461238983, 'step': 72000}
INFO:transformers.trainer:{'loss': 2.6405519312620163, 'learning_rate': 4.8548051485093e-05, 'epoch': 0.08711691089442031, 'step': 72500}
INFO:transformers.trainer:{'loss': 2.665633706688881, 'learning_rate': 4.853803804705915e-05, 'epoch': 0.0877177171764508, 'step': 73000}
INFO:transformers.trainer:{'loss': 2.670729946613312, 'learning_rate': 4.8528024609025314e-05, 'epoch': 0.08831852345848128, 'step': 73500}
INFO:transformers.trainer:{'loss': 2.66196211540699, 'learning_rate': 4.851801117099147e-05, 'epoch': 0.08891932974051177, 'step': 74000}
INFO:transformers.trainer:{'loss': 2.6566186004877093, 'learning_rate': 4.8507997732957635e-05, 'epoch': 0.08952013602254225, 'step': 74500}
INFO:transformers.trainer:{'loss': 2.673087464570999, 'learning_rate': 4.849798429492379e-05, 'epoch': 0.09012094230457274, 'step': 75000}
INFO:transformers.trainer:{'loss': 2.6822586846351624, 'learning_rate': 4.848797085688995e-05, 'epoch': 0.09072174858660322, 'step': 75500}
INFO:transformers.trainer:{'loss': 2.718804570913315, 'learning_rate': 4.8477957418856107e-05, 'epoch': 0.0913225548686337, 'step': 76000}
INFO:transformers.trainer:{'loss': 2.7041065130233766, 'learning_rate': 4.8467943980822264e-05, 'epoch': 0.09192336115066418, 'step': 76500}
INFO:transformers.trainer:{'loss': 2.6191507645845413, 'learning_rate': 4.845793054278843e-05, 'epoch': 0.09252416743269468, 'step': 77000}
INFO:transformers.trainer:{'loss': 2.7208681272268294, 'learning_rate': 4.844791710475458e-05, 'epoch': 0.09312497371472517, 'step': 77500}
INFO:transformers.trainer:{'loss': 2.7193898792266844, 'learning_rate': 4.843790366672074e-05, 'epoch': 0.09372577999675565, 'step': 78000}
INFO:transformers.trainer:{'loss': 2.637304072022438, 'learning_rate': 4.84278902286869e-05, 'epoch': 0.09432658627878614, 'step': 78500}
INFO:transformers.trainer:{'loss': 2.664237202167511, 'learning_rate': 4.841787679065306e-05, 'epoch': 0.09492739256081661, 'step': 79000}
INFO:transformers.trainer:{'loss': 2.660283595085144, 'learning_rate': 4.840786335261921e-05, 'epoch': 0.0955281988428471, 'step': 79500}
INFO:transformers.trainer:{'loss': 2.6570632523298263, 'learning_rate': 4.839784991458538e-05, 'epoch': 0.09612900512487758, 'step': 80000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-80000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-80000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-80000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-60000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.633696799516678, 'learning_rate': 4.8387836476551534e-05, 'epoch': 0.09672981140690808, 'step': 80500}
INFO:transformers.trainer:{'loss': 2.7362914727926255, 'learning_rate': 4.83778230385177e-05, 'epoch': 0.09733061768893855, 'step': 81000}
INFO:transformers.trainer:{'loss': 2.6620588172674178, 'learning_rate': 4.8367809600483855e-05, 'epoch': 0.09793142397096904, 'step': 81500}
INFO:transformers.trainer:{'loss': 2.6922174981832505, 'learning_rate': 4.835779616245001e-05, 'epoch': 0.09853223025299952, 'step': 82000}
INFO:transformers.trainer:{'loss': 2.6660105546712876, 'learning_rate': 4.834778272441617e-05, 'epoch': 0.09913303653503001, 'step': 82500}
INFO:transformers.trainer:{'loss': 2.6870681140422823, 'learning_rate': 4.8337769286382326e-05, 'epoch': 0.09973384281706049, 'step': 83000}
INFO:transformers.trainer:{'loss': 2.5842087507247924, 'learning_rate': 4.832775584834849e-05, 'epoch': 0.10033464909909098, 'step': 83500}
INFO:transformers.trainer:{'loss': 2.7004764741659164, 'learning_rate': 4.831774241031464e-05, 'epoch': 0.10093545538112146, 'step': 84000}
INFO:transformers.trainer:{'loss': 2.6531489386558533, 'learning_rate': 4.8307728972280804e-05, 'epoch': 0.10153626166315195, 'step': 84500}
INFO:transformers.trainer:{'loss': 2.7022471051216126, 'learning_rate': 4.829771553424696e-05, 'epoch': 0.10213706794518243, 'step': 85000}
INFO:transformers.trainer:{'loss': 2.731821521997452, 'learning_rate': 4.8287702096213125e-05, 'epoch': 0.10273787422721292, 'step': 85500}
INFO:transformers.trainer:{'loss': 2.7079611494541167, 'learning_rate': 4.8277688658179275e-05, 'epoch': 0.1033386805092434, 'step': 86000}
INFO:transformers.trainer:{'loss': 2.623985496580601, 'learning_rate': 4.826767522014544e-05, 'epoch': 0.10393948679127389, 'step': 86500}
INFO:transformers.trainer:{'loss': 2.7006543345451357, 'learning_rate': 4.8257661782111596e-05, 'epoch': 0.10454029307330437, 'step': 87000}
INFO:transformers.trainer:{'loss': 2.686150683403015, 'learning_rate': 4.824764834407775e-05, 'epoch': 0.10514109935533486, 'step': 87500}
INFO:transformers.trainer:{'loss': 2.6739344043731688, 'learning_rate': 4.823763490604392e-05, 'epoch': 0.10574190563736534, 'step': 88000}
INFO:transformers.trainer:{'loss': 2.6395358777046205, 'learning_rate': 4.822762146801007e-05, 'epoch': 0.10634271191939583, 'step': 88500}
INFO:transformers.trainer:{'loss': 2.6593295857906343, 'learning_rate': 4.821760802997623e-05, 'epoch': 0.10694351820142631, 'step': 89000}
INFO:transformers.trainer:{'loss': 2.677052642226219, 'learning_rate': 4.820759459194239e-05, 'epoch': 0.1075443244834568, 'step': 89500}
INFO:transformers.trainer:{'loss': 2.652433682560921, 'learning_rate': 4.819758115390855e-05, 'epoch': 0.10814513076548729, 'step': 90000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-90000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-90000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-90000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-70000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6415530844926836, 'learning_rate': 4.81875677158747e-05, 'epoch': 0.10874593704751777, 'step': 90500}
INFO:transformers.trainer:{'loss': 2.6921054487228395, 'learning_rate': 4.8177554277840866e-05, 'epoch': 0.10934674332954826, 'step': 91000}
INFO:transformers.trainer:{'loss': 2.6731609464883803, 'learning_rate': 4.816754083980702e-05, 'epoch': 0.10994754961157874, 'step': 91500}
INFO:transformers.trainer:{'loss': 2.668373095035553, 'learning_rate': 4.815752740177319e-05, 'epoch': 0.11054835589360923, 'step': 92000}
INFO:transformers.trainer:{'loss': 2.6732481384277342, 'learning_rate': 4.814751396373934e-05, 'epoch': 0.1111491621756397, 'step': 92500}
INFO:transformers.trainer:{'loss': 2.6808993663787843, 'learning_rate': 4.81375005257055e-05, 'epoch': 0.1117499684576702, 'step': 93000}
INFO:transformers.trainer:{'loss': 2.6908380403518675, 'learning_rate': 4.812748708767166e-05, 'epoch': 0.11235077473970068, 'step': 93500}
INFO:transformers.trainer:{'loss': 2.6748153442144393, 'learning_rate': 4.8117473649637815e-05, 'epoch': 0.11295158102173117, 'step': 94000}
INFO:transformers.trainer:{'loss': 2.690426327943802, 'learning_rate': 4.810746021160398e-05, 'epoch': 0.11355238730376165, 'step': 94500}
INFO:transformers.trainer:{'loss': 2.646011963248253, 'learning_rate': 4.809744677357013e-05, 'epoch': 0.11415319358579214, 'step': 95000}
INFO:transformers.trainer:{'loss': 2.633121192932129, 'learning_rate': 4.808743333553629e-05, 'epoch': 0.11475399986782261, 'step': 95500}
INFO:transformers.trainer:{'loss': 2.6708989579677582, 'learning_rate': 4.807741989750245e-05, 'epoch': 0.1153548061498531, 'step': 96000}
INFO:transformers.trainer:{'loss': 2.6684042682647706, 'learning_rate': 4.8067406459468614e-05, 'epoch': 0.11595561243188358, 'step': 96500}
INFO:transformers.trainer:{'loss': 2.5500710620880125, 'learning_rate': 4.8057393021434764e-05, 'epoch': 0.11655641871391408, 'step': 97000}
INFO:transformers.trainer:{'loss': 2.6581588269472123, 'learning_rate': 4.804737958340093e-05, 'epoch': 0.11715722499594455, 'step': 97500}
INFO:transformers.trainer:{'loss': 2.691002073287964, 'learning_rate': 4.8037366145367085e-05, 'epoch': 0.11775803127797504, 'step': 98000}
INFO:transformers.trainer:{'loss': 2.6342159514427186, 'learning_rate': 4.802735270733324e-05, 'epoch': 0.11835883756000552, 'step': 98500}
INFO:transformers.trainer:{'loss': 2.631087248086929, 'learning_rate': 4.80173392692994e-05, 'epoch': 0.11895964384203601, 'step': 99000}
INFO:transformers.trainer:{'loss': 2.6452186648845673, 'learning_rate': 4.8007325831265556e-05, 'epoch': 0.11956045012406649, 'step': 99500}
INFO:transformers.trainer:{'loss': 2.7311622899770738, 'learning_rate': 4.799731239323172e-05, 'epoch': 0.12016125640609698, 'step': 100000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-100000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-100000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-100000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-80000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6995512779951096, 'learning_rate': 4.798729895519788e-05, 'epoch': 0.12076206268812746, 'step': 100500}
INFO:transformers.trainer:{'loss': 2.6651869403123856, 'learning_rate': 4.797728551716404e-05, 'epoch': 0.12136286897015795, 'step': 101000}
INFO:transformers.trainer:{'loss': 2.6870840377807617, 'learning_rate': 4.796727207913019e-05, 'epoch': 0.12196367525218843, 'step': 101500}
INFO:transformers.trainer:{'loss': 2.7004755470752717, 'learning_rate': 4.7957258641096355e-05, 'epoch': 0.12256448153421892, 'step': 102000}
INFO:transformers.trainer:{'loss': 2.6619338850975036, 'learning_rate': 4.794724520306251e-05, 'epoch': 0.12316528781624941, 'step': 102500}
INFO:transformers.trainer:{'loss': 2.710076223254204, 'learning_rate': 4.7937231765028676e-05, 'epoch': 0.12376609409827989, 'step': 103000}
INFO:transformers.trainer:{'loss': 2.6292962559461595, 'learning_rate': 4.7927218326994826e-05, 'epoch': 0.12436690038031038, 'step': 103500}
INFO:transformers.trainer:{'loss': 2.6438421779870986, 'learning_rate': 4.791720488896099e-05, 'epoch': 0.12496770666234086, 'step': 104000}
INFO:transformers.trainer:{'loss': 2.6218434225320815, 'learning_rate': 4.790719145092715e-05, 'epoch': 0.12556851294437135, 'step': 104500}
INFO:transformers.trainer:{'loss': 2.6523519642353057, 'learning_rate': 4.7897178012893304e-05, 'epoch': 0.12616931922640184, 'step': 105000}
INFO:transformers.trainer:{'loss': 2.6706056849956514, 'learning_rate': 4.788716457485946e-05, 'epoch': 0.1267701255084323, 'step': 105500}
INFO:transformers.trainer:{'loss': 2.6645762219429017, 'learning_rate': 4.787715113682562e-05, 'epoch': 0.1273709317904628, 'step': 106000}
INFO:transformers.trainer:{'loss': 2.683761251807213, 'learning_rate': 4.786713769879178e-05, 'epoch': 0.1279717380724933, 'step': 106500}
INFO:transformers.trainer:{'loss': 2.677768609881401, 'learning_rate': 4.785712426075794e-05, 'epoch': 0.12857254435452378, 'step': 107000}
INFO:transformers.trainer:{'loss': 2.6565300076007845, 'learning_rate': 4.78471108227241e-05, 'epoch': 0.12917335063655425, 'step': 107500}
INFO:transformers.trainer:{'loss': 2.7034071669578554, 'learning_rate': 4.7837097384690254e-05, 'epoch': 0.12977415691858474, 'step': 108000}
INFO:transformers.trainer:{'loss': 2.685576143026352, 'learning_rate': 4.782708394665642e-05, 'epoch': 0.13037496320061523, 'step': 108500}
INFO:transformers.trainer:{'loss': 2.6082680139541625, 'learning_rate': 4.7817070508622574e-05, 'epoch': 0.13097576948264572, 'step': 109000}
INFO:transformers.trainer:{'loss': 2.619965822696686, 'learning_rate': 4.780705707058873e-05, 'epoch': 0.13157657576467618, 'step': 109500}
INFO:transformers.trainer:{'loss': 2.6533389489650725, 'learning_rate': 4.779704363255489e-05, 'epoch': 0.13217738204670668, 'step': 110000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-110000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-110000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-110000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-90000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6845035316944124, 'learning_rate': 4.7787030194521046e-05, 'epoch': 0.13277818832873717, 'step': 110500}
INFO:transformers.trainer:{'loss': 2.6770365982055666, 'learning_rate': 4.777701675648721e-05, 'epoch': 0.13337899461076766, 'step': 111000}
INFO:transformers.trainer:{'loss': 2.6844373120069505, 'learning_rate': 4.7767003318453367e-05, 'epoch': 0.13397980089279812, 'step': 111500}
INFO:transformers.trainer:{'loss': 2.680815114378929, 'learning_rate': 4.7756989880419524e-05, 'epoch': 0.13458060717482861, 'step': 112000}
INFO:transformers.trainer:{'loss': 2.6605536091327666, 'learning_rate': 4.774697644238568e-05, 'epoch': 0.1351814134568591, 'step': 112500}
INFO:transformers.trainer:{'loss': 2.68776172375679, 'learning_rate': 4.7736963004351845e-05, 'epoch': 0.1357822197388896, 'step': 113000}
INFO:transformers.trainer:{'loss': 2.6460087325572967, 'learning_rate': 4.7726949566318e-05, 'epoch': 0.13638302602092006, 'step': 113500}
INFO:transformers.trainer:{'loss': 2.6295483484268187, 'learning_rate': 4.7716936128284165e-05, 'epoch': 0.13698383230295055, 'step': 114000}
INFO:transformers.trainer:{'loss': 2.658252932429314, 'learning_rate': 4.7706922690250316e-05, 'epoch': 0.13758463858498104, 'step': 114500}
INFO:transformers.trainer:{'loss': 2.6271125741004946, 'learning_rate': 4.769690925221648e-05, 'epoch': 0.13818544486701154, 'step': 115000}
INFO:transformers.trainer:{'loss': 2.665955240368843, 'learning_rate': 4.7686895814182637e-05, 'epoch': 0.13878625114904203, 'step': 115500}
INFO:transformers.trainer:{'loss': 2.6858282470703125, 'learning_rate': 4.7676882376148794e-05, 'epoch': 0.1393870574310725, 'step': 116000}
INFO:transformers.trainer:{'loss': 2.680780444860458, 'learning_rate': 4.766686893811495e-05, 'epoch': 0.13998786371310298, 'step': 116500}
INFO:transformers.trainer:{'loss': 2.6287493562698363, 'learning_rate': 4.765685550008111e-05, 'epoch': 0.14058866999513348, 'step': 117000}
INFO:transformers.trainer:{'loss': 2.6124651383161543, 'learning_rate': 4.764684206204727e-05, 'epoch': 0.14118947627716397, 'step': 117500}
INFO:transformers.trainer:{'loss': 2.6705622260570525, 'learning_rate': 4.763682862401343e-05, 'epoch': 0.14179028255919443, 'step': 118000}
INFO:transformers.trainer:{'loss': 2.647188797533512, 'learning_rate': 4.7626815185979586e-05, 'epoch': 0.14239108884122492, 'step': 118500}
INFO:transformers.trainer:{'loss': 2.625957804441452, 'learning_rate': 4.761680174794574e-05, 'epoch': 0.1429918951232554, 'step': 119000}
INFO:transformers.trainer:{'loss': 2.6821324397325514, 'learning_rate': 4.760678830991191e-05, 'epoch': 0.1435927014052859, 'step': 119500}
INFO:transformers.trainer:{'loss': 2.7357803593873977, 'learning_rate': 4.7596774871878064e-05, 'epoch': 0.14419350768731637, 'step': 120000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-120000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-120000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-120000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-100000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6561044495105746, 'learning_rate': 4.758676143384422e-05, 'epoch': 0.14479431396934686, 'step': 120500}
INFO:transformers.trainer:{'loss': 2.7121538606882094, 'learning_rate': 4.757674799581038e-05, 'epoch': 0.14539512025137735, 'step': 121000}
INFO:transformers.trainer:{'loss': 2.6446784427165984, 'learning_rate': 4.7566734557776535e-05, 'epoch': 0.14599592653340784, 'step': 121500}
INFO:transformers.trainer:{'loss': 2.6971042017936706, 'learning_rate': 4.75567211197427e-05, 'epoch': 0.1465967328154383, 'step': 122000}
INFO:transformers.trainer:{'loss': 2.6621533765792846, 'learning_rate': 4.7546707681708856e-05, 'epoch': 0.1471975390974688, 'step': 122500}
INFO:transformers.trainer:{'loss': 2.688110192537308, 'learning_rate': 4.753669424367501e-05, 'epoch': 0.1477983453794993, 'step': 123000}
INFO:transformers.trainer:{'loss': 2.6813496416807174, 'learning_rate': 4.752668080564117e-05, 'epoch': 0.14839915166152978, 'step': 123500}
INFO:transformers.trainer:{'loss': 2.7032970391511917, 'learning_rate': 4.7516667367607334e-05, 'epoch': 0.14899995794356025, 'step': 124000}
INFO:transformers.trainer:{'loss': 2.66815513753891, 'learning_rate': 4.750665392957349e-05, 'epoch': 0.14960076422559074, 'step': 124500}
INFO:transformers.trainer:{'loss': 2.6343021347522737, 'learning_rate': 4.749664049153965e-05, 'epoch': 0.15020157050762123, 'step': 125000}
INFO:transformers.trainer:{'loss': 2.650439415216446, 'learning_rate': 4.7486627053505805e-05, 'epoch': 0.15080237678965172, 'step': 125500}
INFO:transformers.trainer:{'loss': 2.642521263837814, 'learning_rate': 4.747661361547196e-05, 'epoch': 0.15140318307168218, 'step': 126000}
INFO:transformers.trainer:{'loss': 2.5960882151126863, 'learning_rate': 4.7466600177438126e-05, 'epoch': 0.15200398935371268, 'step': 126500}
INFO:transformers.trainer:{'loss': 2.636918601155281, 'learning_rate': 4.745658673940428e-05, 'epoch': 0.15260479563574317, 'step': 127000}
INFO:transformers.trainer:{'loss': 2.6419485807418823, 'learning_rate': 4.744657330137044e-05, 'epoch': 0.15320560191777366, 'step': 127500}
INFO:transformers.trainer:{'loss': 2.6568049277067183, 'learning_rate': 4.74365598633366e-05, 'epoch': 0.15380640819980415, 'step': 128000}
INFO:transformers.trainer:{'loss': 2.627375654220581, 'learning_rate': 4.742654642530276e-05, 'epoch': 0.15440721448183461, 'step': 128500}
INFO:transformers.trainer:{'loss': 2.6616835931539535, 'learning_rate': 4.741653298726892e-05, 'epoch': 0.1550080207638651, 'step': 129000}
INFO:transformers.trainer:{'loss': 2.613163403868675, 'learning_rate': 4.7406519549235075e-05, 'epoch': 0.1556088270458956, 'step': 129500}
INFO:transformers.trainer:{'loss': 2.6626285450458527, 'learning_rate': 4.739650611120123e-05, 'epoch': 0.1562096333279261, 'step': 130000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-130000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-130000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-130000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-110000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6552370734214783, 'learning_rate': 4.7386492673167396e-05, 'epoch': 0.15681043960995655, 'step': 130500}
INFO:transformers.trainer:{'loss': 2.642627926826477, 'learning_rate': 4.737647923513355e-05, 'epoch': 0.15741124589198704, 'step': 131000}
INFO:transformers.trainer:{'loss': 2.67458450627327, 'learning_rate': 4.736646579709971e-05, 'epoch': 0.15801205217401754, 'step': 131500}
INFO:transformers.trainer:{'loss': 2.6494030039310457, 'learning_rate': 4.735645235906587e-05, 'epoch': 0.15861285845604803, 'step': 132000}
INFO:transformers.trainer:{'loss': 2.6890922343730925, 'learning_rate': 4.7346438921032024e-05, 'epoch': 0.1592136647380785, 'step': 132500}
INFO:transformers.trainer:{'loss': 2.62750557076931, 'learning_rate': 4.733642548299819e-05, 'epoch': 0.15981447102010898, 'step': 133000}
INFO:transformers.trainer:{'loss': 2.6322202821969984, 'learning_rate': 4.7326412044964345e-05, 'epoch': 0.16041527730213948, 'step': 133500}
INFO:transformers.trainer:{'loss': 2.6862625707387924, 'learning_rate': 4.73163986069305e-05, 'epoch': 0.16101608358416997, 'step': 134000}
INFO:transformers.trainer:{'loss': 2.624190510869026, 'learning_rate': 4.730638516889666e-05, 'epoch': 0.16161688986620043, 'step': 134500}
INFO:transformers.trainer:{'loss': 2.6355611453056333, 'learning_rate': 4.729637173086282e-05, 'epoch': 0.16221769614823092, 'step': 135000}
INFO:transformers.trainer:{'loss': 2.6787348041534425, 'learning_rate': 4.728635829282898e-05, 'epoch': 0.1628185024302614, 'step': 135500}
INFO:transformers.trainer:{'loss': 2.6103070858716966, 'learning_rate': 4.727634485479514e-05, 'epoch': 0.1634193087122919, 'step': 136000}
INFO:transformers.trainer:{'loss': 2.6518906090259553, 'learning_rate': 4.7266331416761294e-05, 'epoch': 0.16402011499432237, 'step': 136500}
INFO:transformers.trainer:{'loss': 2.626155274629593, 'learning_rate': 4.725631797872745e-05, 'epoch': 0.16462092127635286, 'step': 137000}
INFO:transformers.trainer:{'loss': 2.662884251952171, 'learning_rate': 4.7246304540693615e-05, 'epoch': 0.16522172755838335, 'step': 137500}
INFO:transformers.trainer:{'loss': 2.6998560996055603, 'learning_rate': 4.723629110265977e-05, 'epoch': 0.16582253384041384, 'step': 138000}
INFO:transformers.trainer:{'loss': 2.650499892473221, 'learning_rate': 4.722627766462593e-05, 'epoch': 0.1664233401224443, 'step': 138500}
INFO:transformers.trainer:{'loss': 2.6599491798877715, 'learning_rate': 4.7216264226592086e-05, 'epoch': 0.1670241464044748, 'step': 139000}
INFO:transformers.trainer:{'loss': 2.6315364459753035, 'learning_rate': 4.720625078855825e-05, 'epoch': 0.1676249526865053, 'step': 139500}
INFO:transformers.trainer:{'loss': 2.6852269217967986, 'learning_rate': 4.719623735052441e-05, 'epoch': 0.16822575896853578, 'step': 140000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-140000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-140000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-140000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-120000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.602843373179436, 'learning_rate': 4.7186223912490564e-05, 'epoch': 0.16882656525056625, 'step': 140500}
INFO:transformers.trainer:{'loss': 2.6741466242074967, 'learning_rate': 4.717621047445672e-05, 'epoch': 0.16942737153259674, 'step': 141000}
INFO:transformers.trainer:{'loss': 2.6472108743190765, 'learning_rate': 4.7166197036422885e-05, 'epoch': 0.17002817781462723, 'step': 141500}
INFO:transformers.trainer:{'loss': 2.6656048567295074, 'learning_rate': 4.715618359838904e-05, 'epoch': 0.17062898409665772, 'step': 142000}
INFO:transformers.trainer:{'loss': 2.6142805776596068, 'learning_rate': 4.71461701603552e-05, 'epoch': 0.1712297903786882, 'step': 142500}
INFO:transformers.trainer:{'loss': 2.634585831284523, 'learning_rate': 4.7136156722321356e-05, 'epoch': 0.17183059666071868, 'step': 143000}
INFO:transformers.trainer:{'loss': 2.662471371173859, 'learning_rate': 4.7126143284287514e-05, 'epoch': 0.17243140294274917, 'step': 143500}
INFO:transformers.trainer:{'loss': 2.6419711956977845, 'learning_rate': 4.711612984625368e-05, 'epoch': 0.17303220922477966, 'step': 144000}
INFO:transformers.trainer:{'loss': 2.6869976716041566, 'learning_rate': 4.7106116408219834e-05, 'epoch': 0.17363301550681015, 'step': 144500}
INFO:transformers.trainer:{'loss': 2.6355006783008577, 'learning_rate': 4.709610297018599e-05, 'epoch': 0.17423382178884061, 'step': 145000}
INFO:transformers.trainer:{'loss': 2.6525651297569275, 'learning_rate': 4.708608953215215e-05, 'epoch': 0.1748346280708711, 'step': 145500}
INFO:transformers.trainer:{'loss': 2.605718526363373, 'learning_rate': 4.707607609411831e-05, 'epoch': 0.1754354343529016, 'step': 146000}
INFO:transformers.trainer:{'loss': 2.6304112861156463, 'learning_rate': 4.706606265608447e-05, 'epoch': 0.1760362406349321, 'step': 146500}
INFO:transformers.trainer:{'loss': 2.675172941148281, 'learning_rate': 4.7056049218050627e-05, 'epoch': 0.17663704691696255, 'step': 147000}
INFO:transformers.trainer:{'loss': 2.6758746975660324, 'learning_rate': 4.7046035780016784e-05, 'epoch': 0.17723785319899305, 'step': 147500}
INFO:transformers.trainer:{'loss': 2.608241802215576, 'learning_rate': 4.703602234198294e-05, 'epoch': 0.17783865948102354, 'step': 148000}
INFO:transformers.trainer:{'loss': 2.6918773345947264, 'learning_rate': 4.7026008903949104e-05, 'epoch': 0.17843946576305403, 'step': 148500}
INFO:transformers.trainer:{'loss': 2.6589628641605376, 'learning_rate': 4.7015995465915255e-05, 'epoch': 0.1790402720450845, 'step': 149000}
INFO:transformers.trainer:{'loss': 2.651695908665657, 'learning_rate': 4.700598202788142e-05, 'epoch': 0.17964107832711498, 'step': 149500}
INFO:transformers.trainer:{'loss': 2.7148057987689973, 'learning_rate': 4.6995968589847576e-05, 'epoch': 0.18024188460914548, 'step': 150000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-150000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-150000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-150000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-130000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.70132759475708, 'learning_rate': 4.698595515181374e-05, 'epoch': 0.18084269089117597, 'step': 150500}
INFO:transformers.trainer:{'loss': 2.725017341375351, 'learning_rate': 4.6975941713779897e-05, 'epoch': 0.18144349717320643, 'step': 151000}
INFO:transformers.trainer:{'loss': 2.640137993693352, 'learning_rate': 4.6965928275746054e-05, 'epoch': 0.18204430345523692, 'step': 151500}
INFO:transformers.trainer:{'loss': 2.656369087934494, 'learning_rate': 4.695591483771221e-05, 'epoch': 0.1826451097372674, 'step': 152000}
INFO:transformers.trainer:{'loss': 2.655987605690956, 'learning_rate': 4.6945901399678375e-05, 'epoch': 0.1832459160192979, 'step': 152500}
INFO:transformers.trainer:{'loss': 2.673892981648445, 'learning_rate': 4.693588796164453e-05, 'epoch': 0.18384672230132837, 'step': 153000}
INFO:transformers.trainer:{'loss': 2.650951800584793, 'learning_rate': 4.692587452361069e-05, 'epoch': 0.18444752858335886, 'step': 153500}
INFO:transformers.trainer:{'loss': 2.640515324115753, 'learning_rate': 4.6915861085576846e-05, 'epoch': 0.18504833486538935, 'step': 154000}
INFO:transformers.trainer:{'loss': 2.6631230581998824, 'learning_rate': 4.6905847647543e-05, 'epoch': 0.18564914114741984, 'step': 154500}
INFO:transformers.trainer:{'loss': 2.6327440217733384, 'learning_rate': 4.689583420950917e-05, 'epoch': 0.18624994742945034, 'step': 155000}
INFO:transformers.trainer:{'loss': 2.6510543644428255, 'learning_rate': 4.688582077147532e-05, 'epoch': 0.1868507537114808, 'step': 155500}
INFO:transformers.trainer:{'loss': 2.634540702342987, 'learning_rate': 4.687580733344148e-05, 'epoch': 0.1874515599935113, 'step': 156000}
INFO:transformers.trainer:{'loss': 2.724914100289345, 'learning_rate': 4.686579389540764e-05, 'epoch': 0.18805236627554178, 'step': 156500}
INFO:transformers.trainer:{'loss': 2.696421451807022, 'learning_rate': 4.68557804573738e-05, 'epoch': 0.18865317255757227, 'step': 157000}
INFO:transformers.trainer:{'loss': 2.7061480162143705, 'learning_rate': 4.684576701933996e-05, 'epoch': 0.18925397883960274, 'step': 157500}
INFO:transformers.trainer:{'loss': 2.7178649053573607, 'learning_rate': 4.6835753581306116e-05, 'epoch': 0.18985478512163323, 'step': 158000}
INFO:transformers.trainer:{'loss': 2.6105501637458803, 'learning_rate': 4.682574014327227e-05, 'epoch': 0.19045559140366372, 'step': 158500}
INFO:transformers.trainer:{'loss': 2.63982879781723, 'learning_rate': 4.681572670523843e-05, 'epoch': 0.1910563976856942, 'step': 159000}
INFO:transformers.trainer:{'loss': 2.613707726240158, 'learning_rate': 4.6805713267204594e-05, 'epoch': 0.19165720396772468, 'step': 159500}
INFO:transformers.trainer:{'loss': 2.712181728363037, 'learning_rate': 4.6795699829170744e-05, 'epoch': 0.19225801024975517, 'step': 160000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-160000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-160000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-160000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-140000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.621610968708992, 'learning_rate': 4.678568639113691e-05, 'epoch': 0.19285881653178566, 'step': 160500}
INFO:transformers.trainer:{'loss': 2.682605428814888, 'learning_rate': 4.6775672953103065e-05, 'epoch': 0.19345962281381615, 'step': 161000}
INFO:transformers.trainer:{'loss': 2.6672772850990296, 'learning_rate': 4.676565951506923e-05, 'epoch': 0.19406042909584661, 'step': 161500}
INFO:transformers.trainer:{'loss': 2.666592832684517, 'learning_rate': 4.675564607703538e-05, 'epoch': 0.1946612353778771, 'step': 162000}
INFO:transformers.trainer:{'loss': 2.667928104877472, 'learning_rate': 4.674563263900154e-05, 'epoch': 0.1952620416599076, 'step': 162500}
INFO:transformers.trainer:{'loss': 2.666483476638794, 'learning_rate': 4.67356192009677e-05, 'epoch': 0.1958628479419381, 'step': 163000}
INFO:transformers.trainer:{'loss': 2.6349209355115892, 'learning_rate': 4.6725605762933864e-05, 'epoch': 0.19646365422396855, 'step': 163500}
INFO:transformers.trainer:{'loss': 2.6532442133426666, 'learning_rate': 4.671559232490002e-05, 'epoch': 0.19706446050599905, 'step': 164000}
INFO:transformers.trainer:{'loss': 2.6838524782657625, 'learning_rate': 4.670557888686618e-05, 'epoch': 0.19766526678802954, 'step': 164500}
INFO:transformers.trainer:{'loss': 2.637251678943634, 'learning_rate': 4.6695565448832335e-05, 'epoch': 0.19826607307006003, 'step': 165000}
INFO:transformers.trainer:{'loss': 2.6876742532253264, 'learning_rate': 4.668555201079849e-05, 'epoch': 0.1988668793520905, 'step': 165500}
INFO:transformers.trainer:{'loss': 2.6231902956962587, 'learning_rate': 4.6675538572764656e-05, 'epoch': 0.19946768563412098, 'step': 166000}
INFO:transformers.trainer:{'loss': 2.682393031477928, 'learning_rate': 4.6665525134730806e-05, 'epoch': 0.20006849191615148, 'step': 166500}
INFO:transformers.trainer:{'loss': 2.6816611602306364, 'learning_rate': 4.665551169669697e-05, 'epoch': 0.20066929819818197, 'step': 167000}
INFO:transformers.trainer:{'loss': 2.6720920495986937, 'learning_rate': 4.664549825866313e-05, 'epoch': 0.20127010448021246, 'step': 167500}
INFO:transformers.trainer:{'loss': 2.615527979969978, 'learning_rate': 4.663548482062929e-05, 'epoch': 0.20187091076224292, 'step': 168000}
INFO:transformers.trainer:{'loss': 2.6440842801332476, 'learning_rate': 4.662547138259544e-05, 'epoch': 0.2024717170442734, 'step': 168500}
INFO:transformers.trainer:{'loss': 2.669961639523506, 'learning_rate': 4.6615457944561605e-05, 'epoch': 0.2030725233263039, 'step': 169000}
INFO:transformers.trainer:{'loss': 2.6437886741161347, 'learning_rate': 4.660544450652776e-05, 'epoch': 0.2036733296083344, 'step': 169500}
INFO:transformers.trainer:{'loss': 2.6506090306043624, 'learning_rate': 4.659543106849392e-05, 'epoch': 0.20427413589036486, 'step': 170000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-170000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-170000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-170000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-150000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6550049134492872, 'learning_rate': 4.658541763046008e-05, 'epoch': 0.20487494217239535, 'step': 170500}
INFO:transformers.trainer:{'loss': 2.637268082022667, 'learning_rate': 4.657540419242623e-05, 'epoch': 0.20547574845442584, 'step': 171000}
INFO:transformers.trainer:{'loss': 2.665850856184959, 'learning_rate': 4.65653907543924e-05, 'epoch': 0.20607655473645634, 'step': 171500}
INFO:transformers.trainer:{'loss': 2.6525012509822847, 'learning_rate': 4.6555377316358554e-05, 'epoch': 0.2066773610184868, 'step': 172000}
INFO:transformers.trainer:{'loss': 2.629819495677948, 'learning_rate': 4.654536387832472e-05, 'epoch': 0.2072781673005173, 'step': 172500}
INFO:transformers.trainer:{'loss': 2.6871863176822663, 'learning_rate': 4.653535044029087e-05, 'epoch': 0.20787897358254778, 'step': 173000}
INFO:transformers.trainer:{'loss': 2.617113543391228, 'learning_rate': 4.652533700225703e-05, 'epoch': 0.20847977986457827, 'step': 173500}
INFO:transformers.trainer:{'loss': 2.6871481635570524, 'learning_rate': 4.651532356422319e-05, 'epoch': 0.20908058614660874, 'step': 174000}
INFO:transformers.trainer:{'loss': 2.6906666437387465, 'learning_rate': 4.650531012618935e-05, 'epoch': 0.20968139242863923, 'step': 174500}
INFO:transformers.trainer:{'loss': 2.639172877192497, 'learning_rate': 4.649529668815551e-05, 'epoch': 0.21028219871066972, 'step': 175000}
INFO:transformers.trainer:{'loss': 2.6802814824581147, 'learning_rate': 4.648528325012167e-05, 'epoch': 0.2108830049927002, 'step': 175500}
INFO:transformers.trainer:{'loss': 2.677869389057159, 'learning_rate': 4.6475269812087824e-05, 'epoch': 0.21148381127473068, 'step': 176000}
INFO:transformers.trainer:{'loss': 2.650639256775379, 'learning_rate': 4.646525637405398e-05, 'epoch': 0.21208461755676117, 'step': 176500}
INFO:transformers.trainer:{'loss': 2.662706891655922, 'learning_rate': 4.6455242936020145e-05, 'epoch': 0.21268542383879166, 'step': 177000}
INFO:transformers.trainer:{'loss': 2.6561080050468444, 'learning_rate': 4.6445229497986296e-05, 'epoch': 0.21328623012082215, 'step': 177500}
INFO:transformers.trainer:{'loss': 2.6617316493988037, 'learning_rate': 4.643521605995246e-05, 'epoch': 0.21388703640285261, 'step': 178000}
INFO:transformers.trainer:{'loss': 2.5944164412021635, 'learning_rate': 4.6425202621918616e-05, 'epoch': 0.2144878426848831, 'step': 178500}
INFO:transformers.trainer:{'loss': 2.663723845720291, 'learning_rate': 4.641518918388478e-05, 'epoch': 0.2150886489669136, 'step': 179000}
INFO:transformers.trainer:{'loss': 2.6147832452058792, 'learning_rate': 4.640517574585093e-05, 'epoch': 0.2156894552489441, 'step': 179500}
INFO:transformers.trainer:{'loss': 2.629289406657219, 'learning_rate': 4.6395162307817094e-05, 'epoch': 0.21629026153097458, 'step': 180000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-180000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-180000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-180000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-160000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6514696136713027, 'learning_rate': 4.638514886978325e-05, 'epoch': 0.21689106781300505, 'step': 180500}
INFO:transformers.trainer:{'loss': 2.647189764380455, 'learning_rate': 4.637513543174941e-05, 'epoch': 0.21749187409503554, 'step': 181000}
INFO:transformers.trainer:{'loss': 2.61383025431633, 'learning_rate': 4.636512199371557e-05, 'epoch': 0.21809268037706603, 'step': 181500}
INFO:transformers.trainer:{'loss': 2.6310950034856795, 'learning_rate': 4.635510855568172e-05, 'epoch': 0.21869348665909652, 'step': 182000}
INFO:transformers.trainer:{'loss': 2.6447365913391114, 'learning_rate': 4.6345095117647886e-05, 'epoch': 0.21929429294112698, 'step': 182500}
INFO:transformers.trainer:{'loss': 2.706436371564865, 'learning_rate': 4.6335081679614044e-05, 'epoch': 0.21989509922315748, 'step': 183000}
INFO:transformers.trainer:{'loss': 2.6594793124198914, 'learning_rate': 4.632506824158021e-05, 'epoch': 0.22049590550518797, 'step': 183500}
INFO:transformers.trainer:{'loss': 2.6173818091154097, 'learning_rate': 4.631505480354636e-05, 'epoch': 0.22109671178721846, 'step': 184000}
INFO:transformers.trainer:{'loss': 2.644928329706192, 'learning_rate': 4.630504136551252e-05, 'epoch': 0.22169751806924892, 'step': 184500}
INFO:transformers.trainer:{'loss': 2.6950037319660187, 'learning_rate': 4.629502792747868e-05, 'epoch': 0.2222983243512794, 'step': 185000}
INFO:transformers.trainer:{'loss': 2.6528569346666337, 'learning_rate': 4.6285014489444836e-05, 'epoch': 0.2228991306333099, 'step': 185500}
INFO:transformers.trainer:{'loss': 2.638015806078911, 'learning_rate': 4.627500105141099e-05, 'epoch': 0.2234999369153404, 'step': 186000}
INFO:transformers.trainer:{'loss': 2.6367465361356737, 'learning_rate': 4.626498761337715e-05, 'epoch': 0.22410074319737086, 'step': 186500}
INFO:transformers.trainer:{'loss': 2.6169539999961855, 'learning_rate': 4.6254974175343314e-05, 'epoch': 0.22470154947940135, 'step': 187000}
INFO:transformers.trainer:{'loss': 2.699834393501282, 'learning_rate': 4.624496073730947e-05, 'epoch': 0.22530235576143184, 'step': 187500}
INFO:transformers.trainer:{'loss': 2.6472175529003144, 'learning_rate': 4.6234947299275635e-05, 'epoch': 0.22590316204346234, 'step': 188000}
INFO:transformers.trainer:{'loss': 2.6512213909626006, 'learning_rate': 4.6224933861241785e-05, 'epoch': 0.2265039683254928, 'step': 188500}
INFO:transformers.trainer:{'loss': 2.680735356450081, 'learning_rate': 4.621492042320795e-05, 'epoch': 0.2271047746075233, 'step': 189000}
INFO:transformers.trainer:{'loss': 2.6498261108994483, 'learning_rate': 4.6204906985174106e-05, 'epoch': 0.22770558088955378, 'step': 189500}
INFO:transformers.trainer:{'loss': 2.640607315182686, 'learning_rate': 4.619489354714027e-05, 'epoch': 0.22830638717158427, 'step': 190000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-190000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-190000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-190000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-170000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.728638278365135, 'learning_rate': 4.618488010910642e-05, 'epoch': 0.22890719345361474, 'step': 190500}
INFO:transformers.trainer:{'loss': 2.6335223982334135, 'learning_rate': 4.6174866671072584e-05, 'epoch': 0.22950799973564523, 'step': 191000}
INFO:transformers.trainer:{'loss': 2.6743395782709123, 'learning_rate': 4.616485323303874e-05, 'epoch': 0.23010880601767572, 'step': 191500}
INFO:transformers.trainer:{'loss': 2.64075825214386, 'learning_rate': 4.61548397950049e-05, 'epoch': 0.2307096122997062, 'step': 192000}
INFO:transformers.trainer:{'loss': 2.6497590889930724, 'learning_rate': 4.6144826356971055e-05, 'epoch': 0.2313104185817367, 'step': 192500}
INFO:transformers.trainer:{'loss': 2.716583327174187, 'learning_rate': 4.613481291893721e-05, 'epoch': 0.23191122486376717, 'step': 193000}
INFO:transformers.trainer:{'loss': 2.6522984915971755, 'learning_rate': 4.6124799480903376e-05, 'epoch': 0.23251203114579766, 'step': 193500}
INFO:transformers.trainer:{'loss': 2.6694847373962403, 'learning_rate': 4.611478604286953e-05, 'epoch': 0.23311283742782815, 'step': 194000}
INFO:transformers.trainer:{'loss': 2.6853154371976853, 'learning_rate': 4.61047726048357e-05, 'epoch': 0.23371364370985864, 'step': 194500}
INFO:transformers.trainer:{'loss': 2.638935690522194, 'learning_rate': 4.609475916680185e-05, 'epoch': 0.2343144499918891, 'step': 195000}
INFO:transformers.trainer:{'loss': 2.640443688631058, 'learning_rate': 4.608474572876801e-05, 'epoch': 0.2349152562739196, 'step': 195500}
INFO:transformers.trainer:{'loss': 2.637107523202896, 'learning_rate': 4.607473229073417e-05, 'epoch': 0.2355160625559501, 'step': 196000}
INFO:transformers.trainer:{'loss': 2.625983195066452, 'learning_rate': 4.6064718852700325e-05, 'epoch': 0.23611686883798058, 'step': 196500}
INFO:transformers.trainer:{'loss': 2.668793545484543, 'learning_rate': 4.605470541466648e-05, 'epoch': 0.23671767512001105, 'step': 197000}
INFO:transformers.trainer:{'loss': 2.6507681112289427, 'learning_rate': 4.604469197663264e-05, 'epoch': 0.23731848140204154, 'step': 197500}
INFO:transformers.trainer:{'loss': 2.6794995527267456, 'learning_rate': 4.60346785385988e-05, 'epoch': 0.23791928768407203, 'step': 198000}
INFO:transformers.trainer:{'loss': 2.6993446029424666, 'learning_rate': 4.602466510056496e-05, 'epoch': 0.23852009396610252, 'step': 198500}
INFO:transformers.trainer:{'loss': 2.624441831588745, 'learning_rate': 4.601465166253112e-05, 'epoch': 0.23912090024813298, 'step': 199000}
INFO:transformers.trainer:{'loss': 2.6668159177303314, 'learning_rate': 4.6004638224497274e-05, 'epoch': 0.23972170653016348, 'step': 199500}
INFO:transformers.trainer:{'loss': 2.6245063679218292, 'learning_rate': 4.599462478646344e-05, 'epoch': 0.24032251281219397, 'step': 200000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-200000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-200000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-200000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-180000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6368924491405488, 'learning_rate': 4.5984611348429595e-05, 'epoch': 0.24092331909422446, 'step': 200500}
INFO:transformers.trainer:{'loss': 2.6722162849903106, 'learning_rate': 4.597459791039576e-05, 'epoch': 0.24152412537625492, 'step': 201000}
INFO:transformers.trainer:{'loss': 2.6232236719131468, 'learning_rate': 4.596458447236191e-05, 'epoch': 0.2421249316582854, 'step': 201500}
INFO:transformers.trainer:{'loss': 2.6643948311805725, 'learning_rate': 4.595457103432807e-05, 'epoch': 0.2427257379403159, 'step': 202000}
INFO:transformers.trainer:{'loss': 2.662760035276413, 'learning_rate': 4.594455759629423e-05, 'epoch': 0.2433265442223464, 'step': 202500}
INFO:transformers.trainer:{'loss': 2.6186752960681914, 'learning_rate': 4.593454415826039e-05, 'epoch': 0.24392735050437686, 'step': 203000}
INFO:transformers.trainer:{'loss': 2.6500428584814073, 'learning_rate': 4.5924530720226544e-05, 'epoch': 0.24452815678640735, 'step': 203500}
INFO:transformers.trainer:{'loss': 2.6446253275871277, 'learning_rate': 4.59145172821927e-05, 'epoch': 0.24512896306843784, 'step': 204000}
INFO:transformers.trainer:{'loss': 2.613840204715729, 'learning_rate': 4.5904503844158865e-05, 'epoch': 0.24572976935046834, 'step': 204500}
INFO:transformers.trainer:{'loss': 2.6516406866312026, 'learning_rate': 4.589449040612502e-05, 'epoch': 0.24633057563249883, 'step': 205000}
INFO:transformers.trainer:{'loss': 2.6840668213367462, 'learning_rate': 4.588447696809118e-05, 'epoch': 0.2469313819145293, 'step': 205500}
INFO:transformers.trainer:{'loss': 2.6729262413978576, 'learning_rate': 4.5874463530057336e-05, 'epoch': 0.24753218819655978, 'step': 206000}
INFO:transformers.trainer:{'loss': 2.6453324860334395, 'learning_rate': 4.58644500920235e-05, 'epoch': 0.24813299447859027, 'step': 206500}
INFO:transformers.trainer:{'loss': 2.624789733052254, 'learning_rate': 4.585443665398966e-05, 'epoch': 0.24873380076062077, 'step': 207000}
INFO:transformers.trainer:{'loss': 2.6434058878421784, 'learning_rate': 4.5844423215955814e-05, 'epoch': 0.24933460704265123, 'step': 207500}
INFO:transformers.trainer:{'loss': 2.6623974462747575, 'learning_rate': 4.583440977792197e-05, 'epoch': 0.24993541332468172, 'step': 208000}
INFO:transformers.trainer:{'loss': 2.618667034983635, 'learning_rate': 4.582439633988813e-05, 'epoch': 0.2505362196067122, 'step': 208500}
INFO:transformers.trainer:{'loss': 2.6440873079299925, 'learning_rate': 4.581438290185429e-05, 'epoch': 0.2511370258887427, 'step': 209000}
INFO:transformers.trainer:{'loss': 2.6664716663360597, 'learning_rate': 4.580436946382045e-05, 'epoch': 0.25173783217077317, 'step': 209500}
INFO:transformers.trainer:{'loss': 2.636389751672745, 'learning_rate': 4.5794356025786606e-05, 'epoch': 0.2523386384528037, 'step': 210000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-210000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-210000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-210000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-190000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.668793245434761, 'learning_rate': 4.5784342587752763e-05, 'epoch': 0.25293944473483415, 'step': 210500}
INFO:transformers.trainer:{'loss': 2.6794504923820495, 'learning_rate': 4.577432914971893e-05, 'epoch': 0.2535402510168646, 'step': 211000}
INFO:transformers.trainer:{'loss': 2.6345555787086488, 'learning_rate': 4.5764315711685084e-05, 'epoch': 0.25414105729889513, 'step': 211500}
INFO:transformers.trainer:{'loss': 2.683007658958435, 'learning_rate': 4.575430227365124e-05, 'epoch': 0.2547418635809256, 'step': 212000}
INFO:transformers.trainer:{'loss': 2.6401370068788528, 'learning_rate': 4.57442888356174e-05, 'epoch': 0.25534266986295606, 'step': 212500}
INFO:transformers.trainer:{'loss': 2.661431402921677, 'learning_rate': 4.573427539758356e-05, 'epoch': 0.2559434761449866, 'step': 213000}
INFO:transformers.trainer:{'loss': 2.6223049595355987, 'learning_rate': 4.572426195954972e-05, 'epoch': 0.25654428242701705, 'step': 213500}
INFO:transformers.trainer:{'loss': 2.6319107505083084, 'learning_rate': 4.5714248521515876e-05, 'epoch': 0.25714508870904756, 'step': 214000}
INFO:transformers.trainer:{'loss': 2.6642921812534333, 'learning_rate': 4.5704235083482033e-05, 'epoch': 0.25774589499107803, 'step': 214500}
INFO:transformers.trainer:{'loss': 2.6044668947458267, 'learning_rate': 4.569422164544819e-05, 'epoch': 0.2583467012731085, 'step': 215000}
INFO:transformers.trainer:{'loss': 2.64855026781559, 'learning_rate': 4.5684208207414354e-05, 'epoch': 0.258947507555139, 'step': 215500}
INFO:transformers.trainer:{'loss': 2.60286509847641, 'learning_rate': 4.567419476938051e-05, 'epoch': 0.2595483138371695, 'step': 216000}
INFO:transformers.trainer:{'loss': 2.660416311621666, 'learning_rate': 4.566418133134667e-05, 'epoch': 0.26014912011919994, 'step': 216500}
INFO:transformers.trainer:{'loss': 2.662970787167549, 'learning_rate': 4.5654167893312826e-05, 'epoch': 0.26074992640123046, 'step': 217000}
INFO:transformers.trainer:{'loss': 2.6327916202545167, 'learning_rate': 4.564415445527899e-05, 'epoch': 0.2613507326832609, 'step': 217500}
INFO:transformers.trainer:{'loss': 2.6638758640289306, 'learning_rate': 4.5634141017245146e-05, 'epoch': 0.26195153896529144, 'step': 218000}
INFO:transformers.trainer:{'loss': 2.6237292046546936, 'learning_rate': 4.5624127579211304e-05, 'epoch': 0.2625523452473219, 'step': 218500}
INFO:transformers.trainer:{'loss': 2.6642558965682985, 'learning_rate': 4.561411414117746e-05, 'epoch': 0.26315315152935237, 'step': 219000}
INFO:transformers.trainer:{'loss': 2.6330157192945483, 'learning_rate': 4.560410070314362e-05, 'epoch': 0.2637539578113829, 'step': 219500}
INFO:transformers.trainer:{'loss': 2.64690749168396, 'learning_rate': 4.559408726510978e-05, 'epoch': 0.26435476409341335, 'step': 220000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-220000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-220000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-220000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-200000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6319647690057755, 'learning_rate': 4.558407382707594e-05, 'epoch': 0.26495557037544387, 'step': 220500}
INFO:transformers.trainer:{'loss': 2.662181936502457, 'learning_rate': 4.5574060389042096e-05, 'epoch': 0.26555637665747434, 'step': 221000}
INFO:transformers.trainer:{'loss': 2.679037801027298, 'learning_rate': 4.556404695100825e-05, 'epoch': 0.2661571829395048, 'step': 221500}
INFO:transformers.trainer:{'loss': 2.653556983947754, 'learning_rate': 4.5554033512974417e-05, 'epoch': 0.2667579892215353, 'step': 222000}
INFO:transformers.trainer:{'loss': 2.6827763562202454, 'learning_rate': 4.5544020074940574e-05, 'epoch': 0.2673587955035658, 'step': 222500}
INFO:transformers.trainer:{'loss': 2.6432909717559814, 'learning_rate': 4.553400663690673e-05, 'epoch': 0.26795960178559625, 'step': 223000}
INFO:transformers.trainer:{'loss': 2.62457590341568, 'learning_rate': 4.552399319887289e-05, 'epoch': 0.26856040806762677, 'step': 223500}
INFO:transformers.trainer:{'loss': 2.654598138213158, 'learning_rate': 4.551397976083905e-05, 'epoch': 0.26916121434965723, 'step': 224000}
INFO:transformers.trainer:{'loss': 2.6573912209272383, 'learning_rate': 4.550396632280521e-05, 'epoch': 0.26976202063168775, 'step': 224500}
INFO:transformers.trainer:{'loss': 2.68537329351902, 'learning_rate': 4.5493952884771366e-05, 'epoch': 0.2703628269137182, 'step': 225000}
INFO:transformers.trainer:{'loss': 2.6279660106897356, 'learning_rate': 4.548393944673752e-05, 'epoch': 0.2709636331957487, 'step': 225500}
INFO:transformers.trainer:{'loss': 2.705003785133362, 'learning_rate': 4.547392600870368e-05, 'epoch': 0.2715644394777792, 'step': 226000}
INFO:transformers.trainer:{'loss': 2.663568450808525, 'learning_rate': 4.5463912570669844e-05, 'epoch': 0.27216524575980966, 'step': 226500}
INFO:transformers.trainer:{'loss': 2.633881402492523, 'learning_rate': 4.5453899132636e-05, 'epoch': 0.2727660520418401, 'step': 227000}
INFO:transformers.trainer:{'loss': 2.6446697767972944, 'learning_rate': 4.544388569460216e-05, 'epoch': 0.27336685832387064, 'step': 227500}
INFO:transformers.trainer:{'loss': 2.654360428214073, 'learning_rate': 4.5433872256568315e-05, 'epoch': 0.2739676646059011, 'step': 228000}
INFO:transformers.trainer:{'loss': 2.6712441636323927, 'learning_rate': 4.542385881853448e-05, 'epoch': 0.2745684708879316, 'step': 228500}
INFO:transformers.trainer:{'loss': 2.5899608161449432, 'learning_rate': 4.5413845380500636e-05, 'epoch': 0.2751692771699621, 'step': 229000}
INFO:transformers.trainer:{'loss': 2.6609933795928957, 'learning_rate': 4.540383194246679e-05, 'epoch': 0.27577008345199255, 'step': 229500}
INFO:transformers.trainer:{'loss': 2.681983719468117, 'learning_rate': 4.539381850443295e-05, 'epoch': 0.2763708897340231, 'step': 230000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-230000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-230000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-230000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-210000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.670653241753578, 'learning_rate': 4.538380506639911e-05, 'epoch': 0.27697169601605354, 'step': 230500}
INFO:transformers.trainer:{'loss': 2.6825409202575683, 'learning_rate': 4.537379162836527e-05, 'epoch': 0.27757250229808406, 'step': 231000}
INFO:transformers.trainer:{'loss': 2.6516427681446078, 'learning_rate': 4.536377819033143e-05, 'epoch': 0.2781733085801145, 'step': 231500}
INFO:transformers.trainer:{'loss': 2.645875334024429, 'learning_rate': 4.5353764752297585e-05, 'epoch': 0.278774114862145, 'step': 232000}
INFO:transformers.trainer:{'loss': 2.62461610352993, 'learning_rate': 4.534375131426374e-05, 'epoch': 0.2793749211441755, 'step': 232500}
INFO:transformers.trainer:{'loss': 2.6532842054367065, 'learning_rate': 4.5333737876229906e-05, 'epoch': 0.27997572742620597, 'step': 233000}
INFO:transformers.trainer:{'loss': 2.6765263612270354, 'learning_rate': 4.532372443819606e-05, 'epoch': 0.28057653370823643, 'step': 233500}
INFO:transformers.trainer:{'loss': 2.6841722671985626, 'learning_rate': 4.531371100016222e-05, 'epoch': 0.28117733999026695, 'step': 234000}
INFO:transformers.trainer:{'loss': 2.62353419148922, 'learning_rate': 4.530369756212838e-05, 'epoch': 0.2817781462722974, 'step': 234500}
INFO:transformers.trainer:{'loss': 2.6181429228782656, 'learning_rate': 4.529368412409454e-05, 'epoch': 0.28237895255432793, 'step': 235000}
INFO:transformers.trainer:{'loss': 2.654298134922981, 'learning_rate': 4.52836706860607e-05, 'epoch': 0.2829797588363584, 'step': 235500}
INFO:transformers.trainer:{'loss': 2.6174468171596526, 'learning_rate': 4.5273657248026855e-05, 'epoch': 0.28358056511838886, 'step': 236000}
INFO:transformers.trainer:{'loss': 2.6319658405780793, 'learning_rate': 4.526364380999301e-05, 'epoch': 0.2841813714004194, 'step': 236500}
INFO:transformers.trainer:{'loss': 2.6720128331184387, 'learning_rate': 4.525363037195917e-05, 'epoch': 0.28478217768244984, 'step': 237000}
INFO:transformers.trainer:{'loss': 2.6240856899023055, 'learning_rate': 4.524361693392533e-05, 'epoch': 0.2853829839644803, 'step': 237500}
INFO:transformers.trainer:{'loss': 2.648276589035988, 'learning_rate': 4.523360349589149e-05, 'epoch': 0.2859837902465108, 'step': 238000}
INFO:transformers.trainer:{'loss': 2.690438409090042, 'learning_rate': 4.522359005785765e-05, 'epoch': 0.2865845965285413, 'step': 238500}
INFO:transformers.trainer:{'loss': 2.6599920105934145, 'learning_rate': 4.5213576619823804e-05, 'epoch': 0.2871854028105718, 'step': 239000}
INFO:transformers.trainer:{'loss': 2.599861776471138, 'learning_rate': 4.520356318178997e-05, 'epoch': 0.2877862090926023, 'step': 239500}
INFO:transformers.trainer:{'loss': 2.6554128885269166, 'learning_rate': 4.5193549743756125e-05, 'epoch': 0.28838701537463274, 'step': 240000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-240000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-240000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-240000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-220000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.607826452374458, 'learning_rate': 4.518353630572228e-05, 'epoch': 0.28898782165666326, 'step': 240500}
INFO:transformers.trainer:{'loss': 2.7105425226688387, 'learning_rate': 4.517352286768844e-05, 'epoch': 0.2895886279386937, 'step': 241000}
INFO:transformers.trainer:{'loss': 2.6092483236789703, 'learning_rate': 4.5163509429654596e-05, 'epoch': 0.2901894342207242, 'step': 241500}
INFO:transformers.trainer:{'loss': 2.588679860115051, 'learning_rate': 4.515349599162076e-05, 'epoch': 0.2907902405027547, 'step': 242000}
INFO:transformers.trainer:{'loss': 2.679392449021339, 'learning_rate': 4.514348255358691e-05, 'epoch': 0.29139104678478517, 'step': 242500}
INFO:transformers.trainer:{'loss': 2.6897549185752867, 'learning_rate': 4.5133469115553074e-05, 'epoch': 0.2919918530668157, 'step': 243000}
INFO:transformers.trainer:{'loss': 2.6875183136463163, 'learning_rate': 4.512345567751923e-05, 'epoch': 0.29259265934884615, 'step': 243500}
INFO:transformers.trainer:{'loss': 2.6924115118980407, 'learning_rate': 4.5113442239485395e-05, 'epoch': 0.2931934656308766, 'step': 244000}
INFO:transformers.trainer:{'loss': 2.5678601262569427, 'learning_rate': 4.510342880145155e-05, 'epoch': 0.29379427191290713, 'step': 244500}
INFO:transformers.trainer:{'loss': 2.6720773289203645, 'learning_rate': 4.509341536341771e-05, 'epoch': 0.2943950781949376, 'step': 245000}
INFO:transformers.trainer:{'loss': 2.724572411775589, 'learning_rate': 4.5083401925383866e-05, 'epoch': 0.2949958844769681, 'step': 245500}
INFO:transformers.trainer:{'loss': 2.684974730014801, 'learning_rate': 4.507338848735002e-05, 'epoch': 0.2955966907589986, 'step': 246000}
INFO:transformers.trainer:{'loss': 2.639428309202194, 'learning_rate': 4.506337504931619e-05, 'epoch': 0.29619749704102905, 'step': 246500}
INFO:transformers.trainer:{'loss': 2.634834533214569, 'learning_rate': 4.505336161128234e-05, 'epoch': 0.29679830332305956, 'step': 247000}
INFO:transformers.trainer:{'loss': 2.651801918387413, 'learning_rate': 4.50433481732485e-05, 'epoch': 0.29739910960509003, 'step': 247500}
INFO:transformers.trainer:{'loss': 2.6684354873895644, 'learning_rate': 4.503333473521466e-05, 'epoch': 0.2979999158871205, 'step': 248000}
INFO:transformers.trainer:{'loss': 2.667577060699463, 'learning_rate': 4.502332129718082e-05, 'epoch': 0.298600722169151, 'step': 248500}
INFO:transformers.trainer:{'loss': 2.674848072767258, 'learning_rate': 4.501330785914697e-05, 'epoch': 0.2992015284511815, 'step': 249000}
INFO:transformers.trainer:{'loss': 2.6499470716714857, 'learning_rate': 4.5003294421113136e-05, 'epoch': 0.299802334733212, 'step': 249500}
INFO:transformers.trainer:{'loss': 2.7076499168872834, 'learning_rate': 4.4993280983079293e-05, 'epoch': 0.30040314101524246, 'step': 250000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-250000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-250000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-250000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-230000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.69905507683754, 'learning_rate': 4.498326754504546e-05, 'epoch': 0.3010039472972729, 'step': 250500}
INFO:transformers.trainer:{'loss': 2.713226022005081, 'learning_rate': 4.4973254107011614e-05, 'epoch': 0.30160475357930344, 'step': 251000}
INFO:transformers.trainer:{'loss': 2.638332328915596, 'learning_rate': 4.496324066897777e-05, 'epoch': 0.3022055598613339, 'step': 251500}
INFO:transformers.trainer:{'loss': 2.6421906871795655, 'learning_rate': 4.495322723094393e-05, 'epoch': 0.30280636614336437, 'step': 252000}
INFO:transformers.trainer:{'loss': 2.653443828701973, 'learning_rate': 4.4943213792910086e-05, 'epoch': 0.3034071724253949, 'step': 252500}
INFO:transformers.trainer:{'loss': 2.671531194806099, 'learning_rate': 4.493320035487625e-05, 'epoch': 0.30400797870742535, 'step': 253000}
INFO:transformers.trainer:{'loss': 2.6852285703420637, 'learning_rate': 4.49231869168424e-05, 'epoch': 0.30460878498945587, 'step': 253500}
INFO:transformers.trainer:{'loss': 2.654496132850647, 'learning_rate': 4.4913173478808564e-05, 'epoch': 0.30520959127148634, 'step': 254000}
INFO:transformers.trainer:{'loss': 2.661286147236824, 'learning_rate': 4.490316004077472e-05, 'epoch': 0.3058103975535168, 'step': 254500}
INFO:transformers.trainer:{'loss': 2.640339434146881, 'learning_rate': 4.4893146602740884e-05, 'epoch': 0.3064112038355473, 'step': 255000}
INFO:transformers.trainer:{'loss': 2.671446484088898, 'learning_rate': 4.4883133164707035e-05, 'epoch': 0.3070120101175778, 'step': 255500}
INFO:transformers.trainer:{'loss': 2.669333069562912, 'learning_rate': 4.48731197266732e-05, 'epoch': 0.3076128163996083, 'step': 256000}
INFO:transformers.trainer:{'loss': 2.650184390306473, 'learning_rate': 4.4863106288639356e-05, 'epoch': 0.30821362268163877, 'step': 256500}
INFO:transformers.trainer:{'loss': 2.630957071185112, 'learning_rate': 4.485309285060551e-05, 'epoch': 0.30881442896366923, 'step': 257000}
INFO:transformers.trainer:{'loss': 2.638585573196411, 'learning_rate': 4.4843079412571677e-05, 'epoch': 0.30941523524569975, 'step': 257500}
INFO:transformers.trainer:{'loss': 2.688666380047798, 'learning_rate': 4.483306597453783e-05, 'epoch': 0.3100160415277302, 'step': 258000}
INFO:transformers.trainer:{'loss': 2.616152849435806, 'learning_rate': 4.482305253650399e-05, 'epoch': 0.3106168478097607, 'step': 258500}
INFO:transformers.trainer:{'loss': 2.6070130813121795, 'learning_rate': 4.481303909847015e-05, 'epoch': 0.3112176540917912, 'step': 259000}
INFO:transformers.trainer:{'loss': 2.620211966753006, 'learning_rate': 4.480302566043631e-05, 'epoch': 0.31181846037382166, 'step': 259500}
INFO:transformers.trainer:{'loss': 2.656479732990265, 'learning_rate': 4.479301222240246e-05, 'epoch': 0.3124192666558522, 'step': 260000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-260000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-260000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-260000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-240000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6390113286972046, 'learning_rate': 4.4782998784368626e-05, 'epoch': 0.31302007293788264, 'step': 260500}
INFO:transformers.trainer:{'loss': 2.6691364631652834, 'learning_rate': 4.477298534633478e-05, 'epoch': 0.3136208792199131, 'step': 261000}
INFO:transformers.trainer:{'loss': 2.666253264784813, 'learning_rate': 4.4762971908300947e-05, 'epoch': 0.3142216855019436, 'step': 261500}
INFO:transformers.trainer:{'loss': 2.6703728796243666, 'learning_rate': 4.47529584702671e-05, 'epoch': 0.3148224917839741, 'step': 262000}
INFO:transformers.trainer:{'loss': 2.627309091091156, 'learning_rate': 4.474294503223326e-05, 'epoch': 0.31542329806600455, 'step': 262500}
INFO:transformers.trainer:{'loss': 2.6268888083696367, 'learning_rate': 4.473293159419942e-05, 'epoch': 0.3160241043480351, 'step': 263000}
INFO:transformers.trainer:{'loss': 2.6628558626174925, 'learning_rate': 4.4722918156165575e-05, 'epoch': 0.31662491063006554, 'step': 263500}
INFO:transformers.trainer:{'loss': 2.628441509842873, 'learning_rate': 4.471290471813174e-05, 'epoch': 0.31722571691209606, 'step': 264000}
INFO:transformers.trainer:{'loss': 2.6390668897628786, 'learning_rate': 4.470289128009789e-05, 'epoch': 0.3178265231941265, 'step': 264500}
INFO:transformers.trainer:{'loss': 2.664497857809067, 'learning_rate': 4.469287784206405e-05, 'epoch': 0.318427329476157, 'step': 265000}
INFO:transformers.trainer:{'loss': 2.6516457952260972, 'learning_rate': 4.468286440403021e-05, 'epoch': 0.3190281357581875, 'step': 265500}
INFO:transformers.trainer:{'loss': 2.668456592202187, 'learning_rate': 4.4672850965996374e-05, 'epoch': 0.31962894204021797, 'step': 266000}
INFO:transformers.trainer:{'loss': 2.672729677438736, 'learning_rate': 4.4662837527962524e-05, 'epoch': 0.32022974832224843, 'step': 266500}
INFO:transformers.trainer:{'loss': 2.686310760498047, 'learning_rate': 4.465282408992869e-05, 'epoch': 0.32083055460427895, 'step': 267000}
INFO:transformers.trainer:{'loss': 2.6258007917404176, 'learning_rate': 4.4642810651894845e-05, 'epoch': 0.3214313608863094, 'step': 267500}
INFO:transformers.trainer:{'loss': 2.6476958661079406, 'learning_rate': 4.4632797213861e-05, 'epoch': 0.32203216716833993, 'step': 268000}
INFO:transformers.trainer:{'loss': 2.62640402507782, 'learning_rate': 4.4622783775827166e-05, 'epoch': 0.3226329734503704, 'step': 268500}
INFO:transformers.trainer:{'loss': 2.6671610733270645, 'learning_rate': 4.4612770337793316e-05, 'epoch': 0.32323377973240086, 'step': 269000}
INFO:transformers.trainer:{'loss': 2.672035975933075, 'learning_rate': 4.460275689975948e-05, 'epoch': 0.3238345860144314, 'step': 269500}
INFO:transformers.trainer:{'loss': 2.6287343838214876, 'learning_rate': 4.459274346172564e-05, 'epoch': 0.32443539229646184, 'step': 270000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-270000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-270000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-270000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-250000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6575386304855346, 'learning_rate': 4.45827300236918e-05, 'epoch': 0.32503619857849236, 'step': 270500}
INFO:transformers.trainer:{'loss': 2.6229547789096834, 'learning_rate': 4.457271658565795e-05, 'epoch': 0.3256370048605228, 'step': 271000}
INFO:transformers.trainer:{'loss': 2.6550654925107957, 'learning_rate': 4.4562703147624115e-05, 'epoch': 0.3262378111425533, 'step': 271500}
INFO:transformers.trainer:{'loss': 2.6371331140995027, 'learning_rate': 4.455268970959027e-05, 'epoch': 0.3268386174245838, 'step': 272000}
INFO:transformers.trainer:{'loss': 2.5981050509214403, 'learning_rate': 4.4542676271556436e-05, 'epoch': 0.3274394237066143, 'step': 272500}
INFO:transformers.trainer:{'loss': 2.6560163296461106, 'learning_rate': 4.4532662833522586e-05, 'epoch': 0.32804022998864474, 'step': 273000}
INFO:transformers.trainer:{'loss': 2.656434083223343, 'learning_rate': 4.452264939548875e-05, 'epoch': 0.32864103627067526, 'step': 273500}
INFO:transformers.trainer:{'loss': 2.638991608142853, 'learning_rate': 4.451263595745491e-05, 'epoch': 0.3292418425527057, 'step': 274000}
INFO:transformers.trainer:{'loss': 2.662927985191345, 'learning_rate': 4.4502622519421064e-05, 'epoch': 0.32984264883473624, 'step': 274500}
INFO:transformers.trainer:{'loss': 2.6689544471502304, 'learning_rate': 4.449260908138723e-05, 'epoch': 0.3304434551167667, 'step': 275000}
INFO:transformers.trainer:{'loss': 2.6004908936619757, 'learning_rate': 4.448259564335338e-05, 'epoch': 0.33104426139879717, 'step': 275500}
INFO:transformers.trainer:{'loss': 2.628301793336868, 'learning_rate': 4.447258220531954e-05, 'epoch': 0.3316450676808277, 'step': 276000}
INFO:transformers.trainer:{'loss': 2.6588171525001525, 'learning_rate': 4.44625687672857e-05, 'epoch': 0.33224587396285815, 'step': 276500}
INFO:transformers.trainer:{'loss': 2.6331442012786863, 'learning_rate': 4.445255532925186e-05, 'epoch': 0.3328466802448886, 'step': 277000}
INFO:transformers.trainer:{'loss': 2.6556546070575715, 'learning_rate': 4.444254189121801e-05, 'epoch': 0.33344748652691913, 'step': 277500}
INFO:transformers.trainer:{'loss': 2.615653660058975, 'learning_rate': 4.443252845318418e-05, 'epoch': 0.3340482928089496, 'step': 278000}
INFO:transformers.trainer:{'loss': 2.6611564626693727, 'learning_rate': 4.4422515015150334e-05, 'epoch': 0.3346490990909801, 'step': 278500}
INFO:transformers.trainer:{'loss': 2.6847607251405714, 'learning_rate': 4.441250157711649e-05, 'epoch': 0.3352499053730106, 'step': 279000}
INFO:transformers.trainer:{'loss': 2.643622466802597, 'learning_rate': 4.440248813908265e-05, 'epoch': 0.33585071165504105, 'step': 279500}
INFO:transformers.trainer:{'loss': 2.68922721016407, 'learning_rate': 4.4392474701048805e-05, 'epoch': 0.33645151793707156, 'step': 280000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-280000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-280000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-280000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-260000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6930919234752655, 'learning_rate': 4.438246126301497e-05, 'epoch': 0.33705232421910203, 'step': 280500}
INFO:transformers.trainer:{'loss': 2.5995604100227356, 'learning_rate': 4.4372447824981126e-05, 'epoch': 0.3376531305011325, 'step': 281000}
INFO:transformers.trainer:{'loss': 2.6787061666250227, 'learning_rate': 4.436243438694729e-05, 'epoch': 0.338253936783163, 'step': 281500}
INFO:transformers.trainer:{'loss': 2.603840658426285, 'learning_rate': 4.435242094891344e-05, 'epoch': 0.3388547430651935, 'step': 282000}
INFO:transformers.trainer:{'loss': 2.6467104749679566, 'learning_rate': 4.4342407510879604e-05, 'epoch': 0.339455549347224, 'step': 282500}
INFO:transformers.trainer:{'loss': 2.6797281794548033, 'learning_rate': 4.433239407284576e-05, 'epoch': 0.34005635562925446, 'step': 283000}
INFO:transformers.trainer:{'loss': 2.643476884841919, 'learning_rate': 4.4322380634811925e-05, 'epoch': 0.3406571619112849, 'step': 283500}
INFO:transformers.trainer:{'loss': 2.662362192749977, 'learning_rate': 4.4312367196778075e-05, 'epoch': 0.34125796819331544, 'step': 284000}
INFO:transformers.trainer:{'loss': 2.6699689140319824, 'learning_rate': 4.430235375874424e-05, 'epoch': 0.3418587744753459, 'step': 284500}
INFO:transformers.trainer:{'loss': 2.662404371500015, 'learning_rate': 4.4292340320710396e-05, 'epoch': 0.3424595807573764, 'step': 285000}
INFO:transformers.trainer:{'loss': 2.630639401435852, 'learning_rate': 4.4282326882676553e-05, 'epoch': 0.3430603870394069, 'step': 285500}
INFO:transformers.trainer:{'loss': 2.6676189687252045, 'learning_rate': 4.427231344464271e-05, 'epoch': 0.34366119332143735, 'step': 286000}
INFO:transformers.trainer:{'loss': 2.6354020198583603, 'learning_rate': 4.426230000660887e-05, 'epoch': 0.34426199960346787, 'step': 286500}
INFO:transformers.trainer:{'loss': 2.663237198114395, 'learning_rate': 4.425228656857503e-05, 'epoch': 0.34486280588549834, 'step': 287000}
INFO:transformers.trainer:{'loss': 2.627815866231918, 'learning_rate': 4.424227313054119e-05, 'epoch': 0.3454636121675288, 'step': 287500}
INFO:transformers.trainer:{'loss': 2.596885047316551, 'learning_rate': 4.423225969250735e-05, 'epoch': 0.3460644184495593, 'step': 288000}
INFO:transformers.trainer:{'loss': 2.648026609539986, 'learning_rate': 4.42222462544735e-05, 'epoch': 0.3466652247315898, 'step': 288500}
INFO:transformers.trainer:{'loss': 2.6617332895994186, 'learning_rate': 4.4212232816439666e-05, 'epoch': 0.3472660310136203, 'step': 289000}
INFO:transformers.trainer:{'loss': 2.6362981090545654, 'learning_rate': 4.4202219378405823e-05, 'epoch': 0.34786683729565077, 'step': 289500}
INFO:transformers.trainer:{'loss': 2.6625603353977203, 'learning_rate': 4.419220594037198e-05, 'epoch': 0.34846764357768123, 'step': 290000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-290000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-290000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-290000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-270000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6697219812870028, 'learning_rate': 4.418219250233814e-05, 'epoch': 0.34906844985971175, 'step': 290500}
INFO:transformers.trainer:{'loss': 2.6065215941667557, 'learning_rate': 4.4172179064304295e-05, 'epoch': 0.3496692561417422, 'step': 291000}
INFO:transformers.trainer:{'loss': 2.6417367124557494, 'learning_rate': 4.416216562627046e-05, 'epoch': 0.3502700624237727, 'step': 291500}
INFO:transformers.trainer:{'loss': 2.6293921695947646, 'learning_rate': 4.4152152188236616e-05, 'epoch': 0.3508708687058032, 'step': 292000}
INFO:transformers.trainer:{'loss': 2.662729520022869, 'learning_rate': 4.414213875020277e-05, 'epoch': 0.35147167498783366, 'step': 292500}
INFO:transformers.trainer:{'loss': 2.690732716321945, 'learning_rate': 4.413212531216893e-05, 'epoch': 0.3520724812698642, 'step': 293000}
INFO:transformers.trainer:{'loss': 2.646036251306534, 'learning_rate': 4.4122111874135094e-05, 'epoch': 0.35267328755189464, 'step': 293500}
INFO:transformers.trainer:{'loss': 2.612134078145027, 'learning_rate': 4.411209843610125e-05, 'epoch': 0.3532740938339251, 'step': 294000}
INFO:transformers.trainer:{'loss': 2.6594546744823457, 'learning_rate': 4.4102084998067414e-05, 'epoch': 0.3538749001159556, 'step': 294500}
INFO:transformers.trainer:{'loss': 2.642796207785606, 'learning_rate': 4.4092071560033565e-05, 'epoch': 0.3544757063979861, 'step': 295000}
INFO:transformers.trainer:{'loss': 2.6708527661561967, 'learning_rate': 4.408205812199973e-05, 'epoch': 0.3550765126800166, 'step': 295500}
INFO:transformers.trainer:{'loss': 2.6688263278007507, 'learning_rate': 4.4072044683965886e-05, 'epoch': 0.3556773189620471, 'step': 296000}
INFO:transformers.trainer:{'loss': 2.635052899003029, 'learning_rate': 4.406203124593204e-05, 'epoch': 0.35627812524407754, 'step': 296500}
INFO:transformers.trainer:{'loss': 2.6943820941448213, 'learning_rate': 4.40520178078982e-05, 'epoch': 0.35687893152610806, 'step': 297000}
INFO:transformers.trainer:{'loss': 2.601771270751953, 'learning_rate': 4.404200436986436e-05, 'epoch': 0.3574797378081385, 'step': 297500}
INFO:transformers.trainer:{'loss': 2.6744640035629272, 'learning_rate': 4.403199093183052e-05, 'epoch': 0.358080544090169, 'step': 298000}
INFO:transformers.trainer:{'loss': 2.669541184425354, 'learning_rate': 4.402197749379668e-05, 'epoch': 0.3586813503721995, 'step': 298500}
INFO:transformers.trainer:{'loss': 2.6306014660596846, 'learning_rate': 4.4011964055762835e-05, 'epoch': 0.35928215665422997, 'step': 299000}
INFO:transformers.trainer:{'loss': 2.6408399510383607, 'learning_rate': 4.400195061772899e-05, 'epoch': 0.3598829629362605, 'step': 299500}
INFO:transformers.trainer:{'loss': 2.6519531724452974, 'learning_rate': 4.3991937179695156e-05, 'epoch': 0.36048376921829095, 'step': 300000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-300000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-300000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-300000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-280000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6591734421253204, 'learning_rate': 4.398192374166131e-05, 'epoch': 0.3610845755003214, 'step': 300500}
INFO:transformers.trainer:{'loss': 2.7056111769676208, 'learning_rate': 4.397191030362747e-05, 'epoch': 0.36168538178235193, 'step': 301000}
INFO:transformers.trainer:{'loss': 2.6782017648220062, 'learning_rate': 4.396189686559363e-05, 'epoch': 0.3622861880643824, 'step': 301500}
INFO:transformers.trainer:{'loss': 2.696083919763565, 'learning_rate': 4.3951883427559784e-05, 'epoch': 0.36288699434641286, 'step': 302000}
INFO:transformers.trainer:{'loss': 2.62230187690258, 'learning_rate': 4.394186998952595e-05, 'epoch': 0.3634878006284434, 'step': 302500}
INFO:transformers.trainer:{'loss': 2.6074752143621445, 'learning_rate': 4.3931856551492105e-05, 'epoch': 0.36408860691047384, 'step': 303000}
INFO:transformers.trainer:{'loss': 2.6198548409938813, 'learning_rate': 4.392184311345826e-05, 'epoch': 0.36468941319250436, 'step': 303500}
INFO:transformers.trainer:{'loss': 2.6786002436876295, 'learning_rate': 4.391182967542442e-05, 'epoch': 0.3652902194745348, 'step': 304000}
INFO:transformers.trainer:{'loss': 2.6606390628814696, 'learning_rate': 4.390181623739058e-05, 'epoch': 0.3658910257565653, 'step': 304500}
INFO:transformers.trainer:{'loss': 2.5647327194213867, 'learning_rate': 4.389180279935674e-05, 'epoch': 0.3664918320385958, 'step': 305000}
INFO:transformers.trainer:{'loss': 2.6577898414134977, 'learning_rate': 4.38817893613229e-05, 'epoch': 0.3670926383206263, 'step': 305500}
INFO:transformers.trainer:{'loss': 2.6785710072517395, 'learning_rate': 4.3871775923289054e-05, 'epoch': 0.36769344460265674, 'step': 306000}
INFO:transformers.trainer:{'loss': 2.6465750551223755, 'learning_rate': 4.386176248525521e-05, 'epoch': 0.36829425088468726, 'step': 306500}
INFO:transformers.trainer:{'loss': 2.639886686205864, 'learning_rate': 4.3851749047221375e-05, 'epoch': 0.3688950571667177, 'step': 307000}
INFO:transformers.trainer:{'loss': 2.653511729836464, 'learning_rate': 4.384173560918753e-05, 'epoch': 0.36949586344874824, 'step': 307500}
INFO:transformers.trainer:{'loss': 2.69752499127388, 'learning_rate': 4.383172217115369e-05, 'epoch': 0.3700966697307787, 'step': 308000}
INFO:transformers.trainer:{'loss': 2.643058255434036, 'learning_rate': 4.3821708733119846e-05, 'epoch': 0.37069747601280917, 'step': 308500}
INFO:transformers.trainer:{'loss': 2.6095870500802993, 'learning_rate': 4.381169529508601e-05, 'epoch': 0.3712982822948397, 'step': 309000}
INFO:transformers.trainer:{'loss': 2.6805641386508943, 'learning_rate': 4.380168185705217e-05, 'epoch': 0.37189908857687015, 'step': 309500}
INFO:transformers.trainer:{'loss': 2.5738491481542587, 'learning_rate': 4.3791668419018324e-05, 'epoch': 0.37249989485890067, 'step': 310000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-310000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-310000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-310000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-290000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.623291214108467, 'learning_rate': 4.378165498098448e-05, 'epoch': 0.37310070114093113, 'step': 310500}
INFO:transformers.trainer:{'loss': 2.6769570294618608, 'learning_rate': 4.3771641542950645e-05, 'epoch': 0.3737015074229616, 'step': 311000}
INFO:transformers.trainer:{'loss': 2.589626195669174, 'learning_rate': 4.37616281049168e-05, 'epoch': 0.3743023137049921, 'step': 311500}
INFO:transformers.trainer:{'loss': 2.5723066588640213, 'learning_rate': 4.375161466688296e-05, 'epoch': 0.3749031199870226, 'step': 312000}
INFO:transformers.trainer:{'loss': 2.5924431381225586, 'learning_rate': 4.3741601228849116e-05, 'epoch': 0.37550392626905305, 'step': 312500}
INFO:transformers.trainer:{'loss': 2.6293805508613586, 'learning_rate': 4.373158779081527e-05, 'epoch': 0.37610473255108356, 'step': 313000}
INFO:transformers.trainer:{'loss': 2.6159833648204804, 'learning_rate': 4.372157435278144e-05, 'epoch': 0.37670553883311403, 'step': 313500}
INFO:transformers.trainer:{'loss': 2.6432095420360566, 'learning_rate': 4.3711560914747594e-05, 'epoch': 0.37730634511514455, 'step': 314000}
INFO:transformers.trainer:{'loss': 2.6652934950590135, 'learning_rate': 4.370154747671375e-05, 'epoch': 0.377907151397175, 'step': 314500}
INFO:transformers.trainer:{'loss': 2.64890986096859, 'learning_rate': 4.369153403867991e-05, 'epoch': 0.3785079576792055, 'step': 315000}
INFO:transformers.trainer:{'loss': 2.6994952750205994, 'learning_rate': 4.368152060064607e-05, 'epoch': 0.379108763961236, 'step': 315500}
INFO:transformers.trainer:{'loss': 2.6373776953220367, 'learning_rate': 4.367150716261223e-05, 'epoch': 0.37970957024326646, 'step': 316000}
INFO:transformers.trainer:{'loss': 2.6422558876276017, 'learning_rate': 4.3661493724578386e-05, 'epoch': 0.3803103765252969, 'step': 316500}
INFO:transformers.trainer:{'loss': 2.6366206194162367, 'learning_rate': 4.365148028654454e-05, 'epoch': 0.38091118280732744, 'step': 317000}
INFO:transformers.trainer:{'loss': 2.6083578025102616, 'learning_rate': 4.36414668485107e-05, 'epoch': 0.3815119890893579, 'step': 317500}
INFO:transformers.trainer:{'loss': 2.592961052894592, 'learning_rate': 4.3631453410476864e-05, 'epoch': 0.3821127953713884, 'step': 318000}
INFO:transformers.trainer:{'loss': 2.615015760183334, 'learning_rate': 4.362143997244302e-05, 'epoch': 0.3827136016534189, 'step': 318500}
INFO:transformers.trainer:{'loss': 2.639222604393959, 'learning_rate': 4.361142653440918e-05, 'epoch': 0.38331440793544935, 'step': 319000}
INFO:transformers.trainer:{'loss': 2.709304285764694, 'learning_rate': 4.3601413096375335e-05, 'epoch': 0.38391521421747987, 'step': 319500}
INFO:transformers.trainer:{'loss': 2.672976754069328, 'learning_rate': 4.35913996583415e-05, 'epoch': 0.38451602049951034, 'step': 320000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-320000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-320000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-320000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-300000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.604245151400566, 'learning_rate': 4.3581386220307656e-05, 'epoch': 0.38511682678154086, 'step': 320500}
INFO:transformers.trainer:{'loss': 2.677027967095375, 'learning_rate': 4.3571372782273813e-05, 'epoch': 0.3857176330635713, 'step': 321000}
INFO:transformers.trainer:{'loss': 2.6391713248491286, 'learning_rate': 4.356135934423997e-05, 'epoch': 0.3863184393456018, 'step': 321500}
INFO:transformers.trainer:{'loss': 2.686498639345169, 'learning_rate': 4.3551345906206134e-05, 'epoch': 0.3869192456276323, 'step': 322000}
INFO:transformers.trainer:{'loss': 2.6691139228343963, 'learning_rate': 4.354133246817229e-05, 'epoch': 0.38752005190966277, 'step': 322500}
INFO:transformers.trainer:{'loss': 2.658091402888298, 'learning_rate': 4.353131903013845e-05, 'epoch': 0.38812085819169323, 'step': 323000}
INFO:transformers.trainer:{'loss': 2.6266137080192564, 'learning_rate': 4.3521305592104606e-05, 'epoch': 0.38872166447372375, 'step': 323500}
INFO:transformers.trainer:{'loss': 2.6162934494018555, 'learning_rate': 4.351129215407076e-05, 'epoch': 0.3893224707557542, 'step': 324000}
INFO:transformers.trainer:{'loss': 2.635772595643997, 'learning_rate': 4.3501278716036926e-05, 'epoch': 0.38992327703778473, 'step': 324500}
INFO:transformers.trainer:{'loss': 2.6403748404979708, 'learning_rate': 4.3491265278003083e-05, 'epoch': 0.3905240833198152, 'step': 325000}
INFO:transformers.trainer:{'loss': 2.643136020064354, 'learning_rate': 4.348125183996924e-05, 'epoch': 0.39112488960184566, 'step': 325500}
INFO:transformers.trainer:{'loss': 2.627593895196915, 'learning_rate': 4.34712384019354e-05, 'epoch': 0.3917256958838762, 'step': 326000}
INFO:transformers.trainer:{'loss': 2.6252031843662262, 'learning_rate': 4.346122496390156e-05, 'epoch': 0.39232650216590664, 'step': 326500}
INFO:transformers.trainer:{'loss': 2.631408140182495, 'learning_rate': 4.345121152586772e-05, 'epoch': 0.3929273084479371, 'step': 327000}
INFO:transformers.trainer:{'loss': 2.6314819442033768, 'learning_rate': 4.3441198087833876e-05, 'epoch': 0.3935281147299676, 'step': 327500}
INFO:transformers.trainer:{'loss': 2.683646985054016, 'learning_rate': 4.343118464980003e-05, 'epoch': 0.3941289210119981, 'step': 328000}
INFO:transformers.trainer:{'loss': 2.6579620815515517, 'learning_rate': 4.342117121176619e-05, 'epoch': 0.3947297272940286, 'step': 328500}
INFO:transformers.trainer:{'loss': 2.651744490027428, 'learning_rate': 4.3411157773732354e-05, 'epoch': 0.3953305335760591, 'step': 329000}
INFO:transformers.trainer:{'loss': 2.641428874850273, 'learning_rate': 4.3401144335698504e-05, 'epoch': 0.39593133985808954, 'step': 329500}
INFO:transformers.trainer:{'loss': 2.665642508983612, 'learning_rate': 4.339113089766467e-05, 'epoch': 0.39653214614012006, 'step': 330000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-330000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-330000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-330000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-310000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6771482236385347, 'learning_rate': 4.3381117459630825e-05, 'epoch': 0.3971329524221505, 'step': 330500}
INFO:transformers.trainer:{'loss': 2.6726628658771516, 'learning_rate': 4.337110402159699e-05, 'epoch': 0.397733758704181, 'step': 331000}
INFO:transformers.trainer:{'loss': 2.596375319004059, 'learning_rate': 4.3361090583563146e-05, 'epoch': 0.3983345649862115, 'step': 331500}
INFO:transformers.trainer:{'loss': 2.6521912584304808, 'learning_rate': 4.33510771455293e-05, 'epoch': 0.39893537126824197, 'step': 332000}
INFO:transformers.trainer:{'loss': 2.645941292285919, 'learning_rate': 4.334106370749546e-05, 'epoch': 0.3995361775502725, 'step': 332500}
INFO:transformers.trainer:{'loss': 2.6913036786317823, 'learning_rate': 4.3331050269461624e-05, 'epoch': 0.40013698383230295, 'step': 333000}
INFO:transformers.trainer:{'loss': 2.636645927071571, 'learning_rate': 4.332103683142778e-05, 'epoch': 0.4007377901143334, 'step': 333500}
INFO:transformers.trainer:{'loss': 2.638247129917145, 'learning_rate': 4.331102339339394e-05, 'epoch': 0.40133859639636393, 'step': 334000}
INFO:transformers.trainer:{'loss': 2.631580213904381, 'learning_rate': 4.3301009955360095e-05, 'epoch': 0.4019394026783944, 'step': 334500}
INFO:transformers.trainer:{'loss': 2.664227211713791, 'learning_rate': 4.329099651732625e-05, 'epoch': 0.4025402089604249, 'step': 335000}
INFO:transformers.trainer:{'loss': 2.620207193374634, 'learning_rate': 4.3280983079292416e-05, 'epoch': 0.4031410152424554, 'step': 335500}
INFO:transformers.trainer:{'loss': 2.6063433250188828, 'learning_rate': 4.3270969641258566e-05, 'epoch': 0.40374182152448584, 'step': 336000}
INFO:transformers.trainer:{'loss': 2.709213000535965, 'learning_rate': 4.326095620322473e-05, 'epoch': 0.40434262780651636, 'step': 336500}
INFO:transformers.trainer:{'loss': 2.6476157531738282, 'learning_rate': 4.325094276519089e-05, 'epoch': 0.4049434340885468, 'step': 337000}
INFO:transformers.trainer:{'loss': 2.63527896130085, 'learning_rate': 4.324092932715705e-05, 'epoch': 0.4055442403705773, 'step': 337500}
INFO:transformers.trainer:{'loss': 2.644204774022102, 'learning_rate': 4.323091588912321e-05, 'epoch': 0.4061450466526078, 'step': 338000}
INFO:transformers.trainer:{'loss': 2.6212224190235136, 'learning_rate': 4.3220902451089365e-05, 'epoch': 0.4067458529346383, 'step': 338500}
INFO:transformers.trainer:{'loss': 2.616585763335228, 'learning_rate': 4.321088901305552e-05, 'epoch': 0.4073466592166688, 'step': 339000}
INFO:transformers.trainer:{'loss': 2.6616505786180498, 'learning_rate': 4.320087557502168e-05, 'epoch': 0.40794746549869926, 'step': 339500}
INFO:transformers.trainer:{'loss': 2.664251910328865, 'learning_rate': 4.319086213698784e-05, 'epoch': 0.4085482717807297, 'step': 340000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-340000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-340000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-340000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-320000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.68866734957695, 'learning_rate': 4.318084869895399e-05, 'epoch': 0.40914907806276024, 'step': 340500}
INFO:transformers.trainer:{'loss': 2.5970481823682787, 'learning_rate': 4.317083526092016e-05, 'epoch': 0.4097498843447907, 'step': 341000}
INFO:transformers.trainer:{'loss': 2.6899126353263854, 'learning_rate': 4.3160821822886314e-05, 'epoch': 0.41035069062682117, 'step': 341500}
INFO:transformers.trainer:{'loss': 2.626663007736206, 'learning_rate': 4.315080838485248e-05, 'epoch': 0.4109514969088517, 'step': 342000}
INFO:transformers.trainer:{'loss': 2.6790741450786593, 'learning_rate': 4.314079494681863e-05, 'epoch': 0.41155230319088215, 'step': 342500}
INFO:transformers.trainer:{'loss': 2.653406377792358, 'learning_rate': 4.313078150878479e-05, 'epoch': 0.41215310947291267, 'step': 343000}
INFO:transformers.trainer:{'loss': 2.6272225719690323, 'learning_rate': 4.312076807075095e-05, 'epoch': 0.41275391575494313, 'step': 343500}
INFO:transformers.trainer:{'loss': 2.655743975520134, 'learning_rate': 4.311075463271711e-05, 'epoch': 0.4133547220369736, 'step': 344000}
INFO:transformers.trainer:{'loss': 2.659188777089119, 'learning_rate': 4.310074119468327e-05, 'epoch': 0.4139555283190041, 'step': 344500}
INFO:transformers.trainer:{'loss': 2.6274130532741546, 'learning_rate': 4.309072775664943e-05, 'epoch': 0.4145563346010346, 'step': 345000}
INFO:transformers.trainer:{'loss': 2.6981570904254912, 'learning_rate': 4.3080714318615584e-05, 'epoch': 0.41515714088306505, 'step': 345500}
INFO:transformers.trainer:{'loss': 2.664719210624695, 'learning_rate': 4.307070088058174e-05, 'epoch': 0.41575794716509556, 'step': 346000}
INFO:transformers.trainer:{'loss': 2.6251060090065, 'learning_rate': 4.3060687442547905e-05, 'epoch': 0.41635875344712603, 'step': 346500}
INFO:transformers.trainer:{'loss': 2.5943042283058166, 'learning_rate': 4.3050674004514055e-05, 'epoch': 0.41695955972915655, 'step': 347000}
INFO:transformers.trainer:{'loss': 2.656532574772835, 'learning_rate': 4.304066056648022e-05, 'epoch': 0.417560366011187, 'step': 347500}
INFO:transformers.trainer:{'loss': 2.6552445166110994, 'learning_rate': 4.3030647128446376e-05, 'epoch': 0.4181611722932175, 'step': 348000}
INFO:transformers.trainer:{'loss': 2.6126265773773194, 'learning_rate': 4.302063369041254e-05, 'epoch': 0.418761978575248, 'step': 348500}
INFO:transformers.trainer:{'loss': 2.6382276153564455, 'learning_rate': 4.301062025237869e-05, 'epoch': 0.41936278485727846, 'step': 349000}
INFO:transformers.trainer:{'loss': 2.5926381160020826, 'learning_rate': 4.3000606814344854e-05, 'epoch': 0.419963591139309, 'step': 349500}
INFO:transformers.trainer:{'loss': 2.6499966604709626, 'learning_rate': 4.299059337631101e-05, 'epoch': 0.42056439742133944, 'step': 350000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-350000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-350000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-350000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-330000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6114993849992754, 'learning_rate': 4.298057993827717e-05, 'epoch': 0.4211652037033699, 'step': 350500}
INFO:transformers.trainer:{'loss': 2.5978250055313112, 'learning_rate': 4.297056650024333e-05, 'epoch': 0.4217660099854004, 'step': 351000}
INFO:transformers.trainer:{'loss': 2.710381761550903, 'learning_rate': 4.296055306220948e-05, 'epoch': 0.4223668162674309, 'step': 351500}
INFO:transformers.trainer:{'loss': 2.6299357563257217, 'learning_rate': 4.2950539624175646e-05, 'epoch': 0.42296762254946135, 'step': 352000}
INFO:transformers.trainer:{'loss': 2.65084686088562, 'learning_rate': 4.29405261861418e-05, 'epoch': 0.42356842883149187, 'step': 352500}
INFO:transformers.trainer:{'loss': 2.633662237882614, 'learning_rate': 4.293051274810797e-05, 'epoch': 0.42416923511352234, 'step': 353000}
INFO:transformers.trainer:{'loss': 2.604109080910683, 'learning_rate': 4.292049931007412e-05, 'epoch': 0.42477004139555286, 'step': 353500}
INFO:transformers.trainer:{'loss': 2.6405884186029436, 'learning_rate': 4.291048587204028e-05, 'epoch': 0.4253708476775833, 'step': 354000}
INFO:transformers.trainer:{'loss': 2.6173810760974883, 'learning_rate': 4.290047243400644e-05, 'epoch': 0.4259716539596138, 'step': 354500}
INFO:transformers.trainer:{'loss': 2.662998594522476, 'learning_rate': 4.28904589959726e-05, 'epoch': 0.4265724602416443, 'step': 355000}
INFO:transformers.trainer:{'loss': 2.647634834885597, 'learning_rate': 4.288044555793875e-05, 'epoch': 0.42717326652367477, 'step': 355500}
INFO:transformers.trainer:{'loss': 2.629332348227501, 'learning_rate': 4.2870432119904916e-05, 'epoch': 0.42777407280570523, 'step': 356000}
INFO:transformers.trainer:{'loss': 2.6013569016456604, 'learning_rate': 4.2860418681871073e-05, 'epoch': 0.42837487908773575, 'step': 356500}
INFO:transformers.trainer:{'loss': 2.6733197482824327, 'learning_rate': 4.285040524383723e-05, 'epoch': 0.4289756853697662, 'step': 357000}
INFO:transformers.trainer:{'loss': 2.6167054098844527, 'learning_rate': 4.2840391805803394e-05, 'epoch': 0.42957649165179673, 'step': 357500}
INFO:transformers.trainer:{'loss': 2.6412533289194107, 'learning_rate': 4.2830378367769545e-05, 'epoch': 0.4301772979338272, 'step': 358000}
INFO:transformers.trainer:{'loss': 2.6481565856933593, 'learning_rate': 4.282036492973571e-05, 'epoch': 0.43077810421585766, 'step': 358500}
INFO:transformers.trainer:{'loss': 2.6399397941827774, 'learning_rate': 4.2810351491701865e-05, 'epoch': 0.4313789104978882, 'step': 359000}
INFO:transformers.trainer:{'loss': 2.599502928495407, 'learning_rate': 4.280033805366803e-05, 'epoch': 0.43197971677991864, 'step': 359500}
INFO:transformers.trainer:{'loss': 2.6289346165657044, 'learning_rate': 4.279032461563418e-05, 'epoch': 0.43258052306194916, 'step': 360000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-360000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-360000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-360000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-340000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.649971314072609, 'learning_rate': 4.2780311177600343e-05, 'epoch': 0.4331813293439796, 'step': 360500}
INFO:transformers.trainer:{'loss': 2.632283185362816, 'learning_rate': 4.27702977395665e-05, 'epoch': 0.4337821356260101, 'step': 361000}
INFO:transformers.trainer:{'loss': 2.662508394598961, 'learning_rate': 4.276028430153266e-05, 'epoch': 0.4343829419080406, 'step': 361500}
INFO:transformers.trainer:{'loss': 2.6315808066129684, 'learning_rate': 4.2750270863498815e-05, 'epoch': 0.4349837481900711, 'step': 362000}
INFO:transformers.trainer:{'loss': 2.626880232334137, 'learning_rate': 4.274025742546497e-05, 'epoch': 0.43558455447210154, 'step': 362500}
INFO:transformers.trainer:{'loss': 2.6102930035591125, 'learning_rate': 4.2730243987431136e-05, 'epoch': 0.43618536075413206, 'step': 363000}
INFO:transformers.trainer:{'loss': 2.644770463347435, 'learning_rate': 4.272023054939729e-05, 'epoch': 0.4367861670361625, 'step': 363500}
INFO:transformers.trainer:{'loss': 2.649226464390755, 'learning_rate': 4.2710217111363456e-05, 'epoch': 0.43738697331819304, 'step': 364000}
INFO:transformers.trainer:{'loss': 2.6421875323057176, 'learning_rate': 4.270020367332961e-05, 'epoch': 0.4379877796002235, 'step': 364500}
INFO:transformers.trainer:{'loss': 2.6210885384082796, 'learning_rate': 4.269019023529577e-05, 'epoch': 0.43858858588225397, 'step': 365000}
INFO:transformers.trainer:{'loss': 2.6422025108337404, 'learning_rate': 4.268017679726193e-05, 'epoch': 0.4391893921642845, 'step': 365500}
INFO:transformers.trainer:{'loss': 2.652750377178192, 'learning_rate': 4.2670163359228085e-05, 'epoch': 0.43979019844631495, 'step': 366000}
INFO:transformers.trainer:{'loss': 2.6279938378334045, 'learning_rate': 4.266014992119424e-05, 'epoch': 0.4403910047283454, 'step': 366500}
INFO:transformers.trainer:{'loss': 2.6303229912519455, 'learning_rate': 4.26501364831604e-05, 'epoch': 0.44099181101037593, 'step': 367000}
INFO:transformers.trainer:{'loss': 2.6330022133588793, 'learning_rate': 4.264012304512656e-05, 'epoch': 0.4415926172924064, 'step': 367500}
INFO:transformers.trainer:{'loss': 2.6241782472133637, 'learning_rate': 4.263010960709272e-05, 'epoch': 0.4421934235744369, 'step': 368000}
INFO:transformers.trainer:{'loss': 2.6392061339616775, 'learning_rate': 4.2620096169058884e-05, 'epoch': 0.4427942298564674, 'step': 368500}
INFO:transformers.trainer:{'loss': 2.5863763887882234, 'learning_rate': 4.2610082731025034e-05, 'epoch': 0.44339503613849784, 'step': 369000}
INFO:transformers.trainer:{'loss': 2.6197568094730377, 'learning_rate': 4.26000692929912e-05, 'epoch': 0.44399584242052836, 'step': 369500}
INFO:transformers.trainer:{'loss': 2.6435877294540404, 'learning_rate': 4.2590055854957355e-05, 'epoch': 0.4445966487025588, 'step': 370000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-370000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-370000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-370000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-350000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6095427141189576, 'learning_rate': 4.258004241692352e-05, 'epoch': 0.4451974549845893, 'step': 370500}
INFO:transformers.trainer:{'loss': 2.6285397408008575, 'learning_rate': 4.257002897888967e-05, 'epoch': 0.4457982612666198, 'step': 371000}
INFO:transformers.trainer:{'loss': 2.6420493900775908, 'learning_rate': 4.256001554085583e-05, 'epoch': 0.4463990675486503, 'step': 371500}
INFO:transformers.trainer:{'loss': 2.698915321111679, 'learning_rate': 4.255000210282199e-05, 'epoch': 0.4469998738306808, 'step': 372000}
INFO:transformers.trainer:{'loss': 2.65688298869133, 'learning_rate': 4.253998866478815e-05, 'epoch': 0.44760068011271126, 'step': 372500}
INFO:transformers.trainer:{'loss': 2.6044531030654907, 'learning_rate': 4.2529975226754304e-05, 'epoch': 0.4482014863947417, 'step': 373000}
INFO:transformers.trainer:{'loss': 2.6410250997543336, 'learning_rate': 4.251996178872046e-05, 'epoch': 0.44880229267677224, 'step': 373500}
INFO:transformers.trainer:{'loss': 2.565589065551758, 'learning_rate': 4.2509948350686625e-05, 'epoch': 0.4494030989588027, 'step': 374000}
INFO:transformers.trainer:{'loss': 2.5963123672008512, 'learning_rate': 4.249993491265278e-05, 'epoch': 0.4500039052408332, 'step': 374500}
INFO:transformers.trainer:{'loss': 2.6764059230089186, 'learning_rate': 4.2489921474618946e-05, 'epoch': 0.4506047115228637, 'step': 375000}
INFO:transformers.trainer:{'loss': 2.63136940741539, 'learning_rate': 4.2479908036585096e-05, 'epoch': 0.45120551780489415, 'step': 375500}
INFO:transformers.trainer:{'loss': 2.6332756333351135, 'learning_rate': 4.246989459855126e-05, 'epoch': 0.45180632408692467, 'step': 376000}
INFO:transformers.trainer:{'loss': 2.615703089594841, 'learning_rate': 4.245988116051742e-05, 'epoch': 0.45240713036895513, 'step': 376500}
INFO:transformers.trainer:{'loss': 2.653307348012924, 'learning_rate': 4.2449867722483574e-05, 'epoch': 0.4530079366509856, 'step': 377000}
INFO:transformers.trainer:{'loss': 2.6598552392721175, 'learning_rate': 4.243985428444973e-05, 'epoch': 0.4536087429330161, 'step': 377500}
INFO:transformers.trainer:{'loss': 2.685676323533058, 'learning_rate': 4.242984084641589e-05, 'epoch': 0.4542095492150466, 'step': 378000}
INFO:transformers.trainer:{'loss': 2.5963166382312775, 'learning_rate': 4.241982740838205e-05, 'epoch': 0.4548103554970771, 'step': 378500}
INFO:transformers.trainer:{'loss': 2.634335597991943, 'learning_rate': 4.240981397034821e-05, 'epoch': 0.45541116177910757, 'step': 379000}
INFO:transformers.trainer:{'loss': 2.6568675097227095, 'learning_rate': 4.2399800532314366e-05, 'epoch': 0.45601196806113803, 'step': 379500}
INFO:transformers.trainer:{'loss': 2.643999986767769, 'learning_rate': 4.238978709428052e-05, 'epoch': 0.45661277434316855, 'step': 380000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-380000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-380000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-380000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-360000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.584277186393738, 'learning_rate': 4.237977365624669e-05, 'epoch': 0.457213580625199, 'step': 380500}
INFO:transformers.trainer:{'loss': 2.6735818519592285, 'learning_rate': 4.2369760218212844e-05, 'epoch': 0.4578143869072295, 'step': 381000}
INFO:transformers.trainer:{'loss': 2.680657395005226, 'learning_rate': 4.235974678017901e-05, 'epoch': 0.45841519318926, 'step': 381500}
INFO:transformers.trainer:{'loss': 2.6047569146156313, 'learning_rate': 4.234973334214516e-05, 'epoch': 0.45901599947129046, 'step': 382000}
INFO:transformers.trainer:{'loss': 2.5728354942798615, 'learning_rate': 4.233971990411132e-05, 'epoch': 0.459616805753321, 'step': 382500}
INFO:transformers.trainer:{'loss': 2.579756908416748, 'learning_rate': 4.232970646607748e-05, 'epoch': 0.46021761203535144, 'step': 383000}
INFO:transformers.trainer:{'loss': 2.646555836558342, 'learning_rate': 4.2319693028043636e-05, 'epoch': 0.4608184183173819, 'step': 383500}
INFO:transformers.trainer:{'loss': 2.6423791762590407, 'learning_rate': 4.230967959000979e-05, 'epoch': 0.4614192245994124, 'step': 384000}
INFO:transformers.trainer:{'loss': 2.6618431817293167, 'learning_rate': 4.229966615197595e-05, 'epoch': 0.4620200308814429, 'step': 384500}
INFO:transformers.trainer:{'loss': 2.5811051893234254, 'learning_rate': 4.2289652713942114e-05, 'epoch': 0.4626208371634734, 'step': 385000}
INFO:transformers.trainer:{'loss': 2.6535626516342163, 'learning_rate': 4.227963927590827e-05, 'epoch': 0.4632216434455039, 'step': 385500}
INFO:transformers.trainer:{'loss': 2.653463281750679, 'learning_rate': 4.226962583787443e-05, 'epoch': 0.46382244972753434, 'step': 386000}
INFO:transformers.trainer:{'loss': 2.66289728140831, 'learning_rate': 4.2259612399840585e-05, 'epoch': 0.46442325600956486, 'step': 386500}
INFO:transformers.trainer:{'loss': 2.5794909583330154, 'learning_rate': 4.224959896180675e-05, 'epoch': 0.4650240622915953, 'step': 387000}
INFO:transformers.trainer:{'loss': 2.6317605851888657, 'learning_rate': 4.2239585523772906e-05, 'epoch': 0.4656248685736258, 'step': 387500}
INFO:transformers.trainer:{'loss': 2.6282204060554504, 'learning_rate': 4.222957208573906e-05, 'epoch': 0.4662256748556563, 'step': 388000}
INFO:transformers.trainer:{'loss': 2.6664854311943054, 'learning_rate': 4.221955864770522e-05, 'epoch': 0.46682648113768677, 'step': 388500}
INFO:transformers.trainer:{'loss': 2.5549259243011475, 'learning_rate': 4.220954520967138e-05, 'epoch': 0.4674272874197173, 'step': 389000}
INFO:transformers.trainer:{'loss': 2.659957217812538, 'learning_rate': 4.219953177163754e-05, 'epoch': 0.46802809370174775, 'step': 389500}
INFO:transformers.trainer:{'loss': 2.6417647567987443, 'learning_rate': 4.21895183336037e-05, 'epoch': 0.4686288999837782, 'step': 390000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-390000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-390000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-390000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-370000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6389399217367173, 'learning_rate': 4.2179504895569855e-05, 'epoch': 0.46922970626580873, 'step': 390500}
INFO:transformers.trainer:{'loss': 2.603344917535782, 'learning_rate': 4.216949145753601e-05, 'epoch': 0.4698305125478392, 'step': 391000}
INFO:transformers.trainer:{'loss': 2.6411125046014785, 'learning_rate': 4.2159478019502176e-05, 'epoch': 0.47043131882986966, 'step': 391500}
INFO:transformers.trainer:{'loss': 2.5968786907196044, 'learning_rate': 4.214946458146833e-05, 'epoch': 0.4710321251119002, 'step': 392000}
INFO:transformers.trainer:{'loss': 2.6018697835206988, 'learning_rate': 4.213945114343449e-05, 'epoch': 0.47163293139393064, 'step': 392500}
INFO:transformers.trainer:{'loss': 2.5713026007413866, 'learning_rate': 4.212943770540065e-05, 'epoch': 0.47223373767596116, 'step': 393000}
INFO:transformers.trainer:{'loss': 2.637488458156586, 'learning_rate': 4.211942426736681e-05, 'epoch': 0.4728345439579916, 'step': 393500}
INFO:transformers.trainer:{'loss': 2.644154319524765, 'learning_rate': 4.210941082933297e-05, 'epoch': 0.4734353502400221, 'step': 394000}
INFO:transformers.trainer:{'loss': 2.618546556711197, 'learning_rate': 4.2099397391299125e-05, 'epoch': 0.4740361565220526, 'step': 394500}
INFO:transformers.trainer:{'loss': 2.620842136025429, 'learning_rate': 4.208938395326528e-05, 'epoch': 0.4746369628040831, 'step': 395000}
INFO:transformers.trainer:{'loss': 2.605488983035088, 'learning_rate': 4.207937051523144e-05, 'epoch': 0.47523776908611354, 'step': 395500}
INFO:transformers.trainer:{'loss': 2.588607717394829, 'learning_rate': 4.2069357077197603e-05, 'epoch': 0.47583857536814406, 'step': 396000}
INFO:transformers.trainer:{'loss': 2.6288528653383256, 'learning_rate': 4.205934363916376e-05, 'epoch': 0.4764393816501745, 'step': 396500}
INFO:transformers.trainer:{'loss': 2.575712987422943, 'learning_rate': 4.204933020112992e-05, 'epoch': 0.47704018793220504, 'step': 397000}
INFO:transformers.trainer:{'loss': 2.614616038441658, 'learning_rate': 4.2039316763096075e-05, 'epoch': 0.4776409942142355, 'step': 397500}
INFO:transformers.trainer:{'loss': 2.664280134677887, 'learning_rate': 4.202930332506224e-05, 'epoch': 0.47824180049626597, 'step': 398000}
INFO:transformers.trainer:{'loss': 2.614261421442032, 'learning_rate': 4.2019289887028396e-05, 'epoch': 0.4788426067782965, 'step': 398500}
INFO:transformers.trainer:{'loss': 2.6208811945915222, 'learning_rate': 4.200927644899455e-05, 'epoch': 0.47944341306032695, 'step': 399000}
INFO:transformers.trainer:{'loss': 2.6511368273496627, 'learning_rate': 4.199926301096071e-05, 'epoch': 0.48004421934235747, 'step': 399500}
INFO:transformers.trainer:{'loss': 2.678174933075905, 'learning_rate': 4.198924957292687e-05, 'epoch': 0.48064502562438793, 'step': 400000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-400000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-400000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-400000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-380000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6860564416646957, 'learning_rate': 4.197923613489303e-05, 'epoch': 0.4812458319064184, 'step': 400500}
INFO:transformers.trainer:{'loss': 2.6227395985126494, 'learning_rate': 4.196922269685919e-05, 'epoch': 0.4818466381884489, 'step': 401000}
INFO:transformers.trainer:{'loss': 2.6060719509124755, 'learning_rate': 4.1959209258825345e-05, 'epoch': 0.4824474444704794, 'step': 401500}
INFO:transformers.trainer:{'loss': 2.6121569237709044, 'learning_rate': 4.19491958207915e-05, 'epoch': 0.48304825075250984, 'step': 402000}
INFO:transformers.trainer:{'loss': 2.65721458363533, 'learning_rate': 4.1939182382757666e-05, 'epoch': 0.48364905703454036, 'step': 402500}
INFO:transformers.trainer:{'loss': 2.6403963553905485, 'learning_rate': 4.192916894472382e-05, 'epoch': 0.4842498633165708, 'step': 403000}
INFO:transformers.trainer:{'loss': 2.6091830722093583, 'learning_rate': 4.191915550668998e-05, 'epoch': 0.48485066959860135, 'step': 403500}
INFO:transformers.trainer:{'loss': 2.6265236983299256, 'learning_rate': 4.190914206865614e-05, 'epoch': 0.4854514758806318, 'step': 404000}
INFO:transformers.trainer:{'loss': 2.6105214767456055, 'learning_rate': 4.18991286306223e-05, 'epoch': 0.4860522821626623, 'step': 404500}
INFO:transformers.trainer:{'loss': 2.599022619009018, 'learning_rate': 4.188911519258846e-05, 'epoch': 0.4866530884446928, 'step': 405000}
INFO:transformers.trainer:{'loss': 2.63047728395462, 'learning_rate': 4.1879101754554615e-05, 'epoch': 0.48725389472672326, 'step': 405500}
INFO:transformers.trainer:{'loss': 2.5841088635921476, 'learning_rate': 4.186908831652077e-05, 'epoch': 0.4878547010087537, 'step': 406000}
INFO:transformers.trainer:{'loss': 2.666567102909088, 'learning_rate': 4.185907487848693e-05, 'epoch': 0.48845550729078424, 'step': 406500}
INFO:transformers.trainer:{'loss': 2.612524644494057, 'learning_rate': 4.184906144045309e-05, 'epoch': 0.4890563135728147, 'step': 407000}
INFO:transformers.trainer:{'loss': 2.650298862695694, 'learning_rate': 4.183904800241925e-05, 'epoch': 0.4896571198548452, 'step': 407500}
INFO:transformers.trainer:{'loss': 2.597235696554184, 'learning_rate': 4.182903456438541e-05, 'epoch': 0.4902579261368757, 'step': 408000}
INFO:transformers.trainer:{'loss': 2.626190931558609, 'learning_rate': 4.1819021126351564e-05, 'epoch': 0.49085873241890615, 'step': 408500}
INFO:transformers.trainer:{'loss': 2.6281325031518934, 'learning_rate': 4.180900768831773e-05, 'epoch': 0.49145953870093667, 'step': 409000}
INFO:transformers.trainer:{'loss': 2.6240353467464446, 'learning_rate': 4.1798994250283885e-05, 'epoch': 0.49206034498296713, 'step': 409500}
INFO:transformers.trainer:{'loss': 2.6916617337465287, 'learning_rate': 4.178898081225004e-05, 'epoch': 0.49266115126499765, 'step': 410000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-410000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-410000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-410000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-390000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6281363703012466, 'learning_rate': 4.17789673742162e-05, 'epoch': 0.4932619575470281, 'step': 410500}
INFO:transformers.trainer:{'loss': 2.5857870906591414, 'learning_rate': 4.1768953936182356e-05, 'epoch': 0.4938627638290586, 'step': 411000}
INFO:transformers.trainer:{'loss': 2.5921000155210496, 'learning_rate': 4.175894049814852e-05, 'epoch': 0.4944635701110891, 'step': 411500}
INFO:transformers.trainer:{'loss': 2.5951982614994047, 'learning_rate': 4.174892706011467e-05, 'epoch': 0.49506437639311957, 'step': 412000}
INFO:transformers.trainer:{'loss': 2.608655997633934, 'learning_rate': 4.1738913622080834e-05, 'epoch': 0.49566518267515003, 'step': 412500}
INFO:transformers.trainer:{'loss': 2.6645795228481295, 'learning_rate': 4.172890018404699e-05, 'epoch': 0.49626598895718055, 'step': 413000}
INFO:transformers.trainer:{'loss': 2.598953597187996, 'learning_rate': 4.1718886746013155e-05, 'epoch': 0.496866795239211, 'step': 413500}
INFO:transformers.trainer:{'loss': 2.6331586163043976, 'learning_rate': 4.170887330797931e-05, 'epoch': 0.49746760152124153, 'step': 414000}
INFO:transformers.trainer:{'loss': 2.646218537569046, 'learning_rate': 4.169885986994547e-05, 'epoch': 0.498068407803272, 'step': 414500}
INFO:transformers.trainer:{'loss': 2.583945551276207, 'learning_rate': 4.1688846431911626e-05, 'epoch': 0.49866921408530246, 'step': 415000}
INFO:transformers.trainer:{'loss': 2.6476367574930193, 'learning_rate': 4.167883299387779e-05, 'epoch': 0.499270020367333, 'step': 415500}
INFO:transformers.trainer:{'loss': 2.6629740488529205, 'learning_rate': 4.166881955584395e-05, 'epoch': 0.49987082664936344, 'step': 416000}
INFO:transformers.trainer:{'loss': 2.639945165157318, 'learning_rate': 4.1658806117810104e-05, 'epoch': 0.500471632931394, 'step': 416500}
INFO:transformers.trainer:{'loss': 2.5927965445518493, 'learning_rate': 4.164879267977626e-05, 'epoch': 0.5010724392134244, 'step': 417000}
INFO:transformers.trainer:{'loss': 2.6110264201164246, 'learning_rate': 4.163877924174242e-05, 'epoch': 0.5016732454954549, 'step': 417500}
INFO:transformers.trainer:{'loss': 2.665273048043251, 'learning_rate': 4.162876580370858e-05, 'epoch': 0.5022740517774854, 'step': 418000}
INFO:transformers.trainer:{'loss': 2.618645092368126, 'learning_rate': 4.161875236567474e-05, 'epoch': 0.5028748580595158, 'step': 418500}
INFO:transformers.trainer:{'loss': 2.6257674318552016, 'learning_rate': 4.1608738927640896e-05, 'epoch': 0.5034756643415463, 'step': 419000}
INFO:transformers.trainer:{'loss': 2.6135677506923676, 'learning_rate': 4.159872548960705e-05, 'epoch': 0.5040764706235769, 'step': 419500}
INFO:transformers.trainer:{'loss': 2.6495985379219054, 'learning_rate': 4.158871205157322e-05, 'epoch': 0.5046772769056074, 'step': 420000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-420000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-420000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-420000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-400000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.636822142004967, 'learning_rate': 4.1578698613539374e-05, 'epoch': 0.5052780831876378, 'step': 420500}
INFO:transformers.trainer:{'loss': 2.6280578820705416, 'learning_rate': 4.156868517550553e-05, 'epoch': 0.5058788894696683, 'step': 421000}
INFO:transformers.trainer:{'loss': 2.611045537352562, 'learning_rate': 4.155867173747169e-05, 'epoch': 0.5064796957516988, 'step': 421500}
INFO:transformers.trainer:{'loss': 2.588544575929642, 'learning_rate': 4.1548658299437845e-05, 'epoch': 0.5070805020337292, 'step': 422000}
INFO:transformers.trainer:{'loss': 2.595961855649948, 'learning_rate': 4.153864486140401e-05, 'epoch': 0.5076813083157597, 'step': 422500}
INFO:transformers.trainer:{'loss': 2.6148209037780763, 'learning_rate': 4.152863142337016e-05, 'epoch': 0.5082821145977903, 'step': 423000}
INFO:transformers.trainer:{'loss': 2.6180228502750396, 'learning_rate': 4.151861798533632e-05, 'epoch': 0.5088829208798207, 'step': 423500}
INFO:transformers.trainer:{'loss': 2.6279965245723726, 'learning_rate': 4.150860454730248e-05, 'epoch': 0.5094837271618512, 'step': 424000}
INFO:transformers.trainer:{'loss': 2.6409793261289596, 'learning_rate': 4.1498591109268644e-05, 'epoch': 0.5100845334438817, 'step': 424500}
INFO:transformers.trainer:{'loss': 2.5711914411783217, 'learning_rate': 4.14885776712348e-05, 'epoch': 0.5106853397259121, 'step': 425000}
INFO:transformers.trainer:{'loss': 2.6314996780157087, 'learning_rate': 4.147856423320096e-05, 'epoch': 0.5112861460079426, 'step': 425500}
INFO:transformers.trainer:{'loss': 2.6044813002347946, 'learning_rate': 4.1468550795167115e-05, 'epoch': 0.5118869522899732, 'step': 426000}
INFO:transformers.trainer:{'loss': 2.5883192118406297, 'learning_rate': 4.145853735713327e-05, 'epoch': 0.5124877585720037, 'step': 426500}
INFO:transformers.trainer:{'loss': 2.595066686987877, 'learning_rate': 4.1448523919099436e-05, 'epoch': 0.5130885648540341, 'step': 427000}
INFO:transformers.trainer:{'loss': 2.633981437444687, 'learning_rate': 4.143851048106559e-05, 'epoch': 0.5136893711360646, 'step': 427500}
INFO:transformers.trainer:{'loss': 2.6771697556972502, 'learning_rate': 4.142849704303175e-05, 'epoch': 0.5142901774180951, 'step': 428000}
INFO:transformers.trainer:{'loss': 2.608572712421417, 'learning_rate': 4.141848360499791e-05, 'epoch': 0.5148909837001255, 'step': 428500}
INFO:transformers.trainer:{'loss': 2.619020318746567, 'learning_rate': 4.140847016696407e-05, 'epoch': 0.5154917899821561, 'step': 429000}
INFO:transformers.trainer:{'loss': 2.6246263525485993, 'learning_rate': 4.139845672893022e-05, 'epoch': 0.5160925962641866, 'step': 429500}
INFO:transformers.trainer:{'loss': 2.592931178331375, 'learning_rate': 4.1388443290896385e-05, 'epoch': 0.516693402546217, 'step': 430000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-430000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-430000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-430000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-410000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5911334283351897, 'learning_rate': 4.137842985286254e-05, 'epoch': 0.5172942088282475, 'step': 430500}
INFO:transformers.trainer:{'loss': 2.610268275499344, 'learning_rate': 4.1368416414828706e-05, 'epoch': 0.517895015110278, 'step': 431000}
INFO:transformers.trainer:{'loss': 2.5964563411474226, 'learning_rate': 4.1358402976794863e-05, 'epoch': 0.5184958213923084, 'step': 431500}
INFO:transformers.trainer:{'loss': 2.6395947222709655, 'learning_rate': 4.134838953876102e-05, 'epoch': 0.519096627674339, 'step': 432000}
INFO:transformers.trainer:{'loss': 2.7306952645778657, 'learning_rate': 4.133837610072718e-05, 'epoch': 0.5196974339563695, 'step': 432500}
INFO:transformers.trainer:{'loss': 2.6400110218524935, 'learning_rate': 4.1328362662693335e-05, 'epoch': 0.5202982402383999, 'step': 433000}
INFO:transformers.trainer:{'loss': 2.607613831281662, 'learning_rate': 4.13183492246595e-05, 'epoch': 0.5208990465204304, 'step': 433500}
INFO:transformers.trainer:{'loss': 2.6567708188295365, 'learning_rate': 4.130833578662565e-05, 'epoch': 0.5214998528024609, 'step': 434000}
INFO:transformers.trainer:{'loss': 2.628639889717102, 'learning_rate': 4.129832234859181e-05, 'epoch': 0.5221006590844914, 'step': 434500}
INFO:transformers.trainer:{'loss': 2.6523511456251145, 'learning_rate': 4.128830891055797e-05, 'epoch': 0.5227014653665218, 'step': 435000}
INFO:transformers.trainer:{'loss': 2.6128630323410036, 'learning_rate': 4.1278295472524133e-05, 'epoch': 0.5233022716485524, 'step': 435500}
INFO:transformers.trainer:{'loss': 2.618996693730354, 'learning_rate': 4.1268282034490284e-05, 'epoch': 0.5239030779305829, 'step': 436000}
INFO:transformers.trainer:{'loss': 2.62859713447094, 'learning_rate': 4.125826859645645e-05, 'epoch': 0.5245038842126133, 'step': 436500}
INFO:transformers.trainer:{'loss': 2.6390055611133576, 'learning_rate': 4.1248255158422605e-05, 'epoch': 0.5251046904946438, 'step': 437000}
INFO:transformers.trainer:{'loss': 2.7050540912151337, 'learning_rate': 4.123824172038876e-05, 'epoch': 0.5257054967766743, 'step': 437500}
INFO:transformers.trainer:{'loss': 2.5908428399562835, 'learning_rate': 4.1228228282354926e-05, 'epoch': 0.5263063030587047, 'step': 438000}
INFO:transformers.trainer:{'loss': 2.620800414085388, 'learning_rate': 4.1218214844321076e-05, 'epoch': 0.5269071093407353, 'step': 438500}
INFO:transformers.trainer:{'loss': 2.655425552845001, 'learning_rate': 4.120820140628724e-05, 'epoch': 0.5275079156227658, 'step': 439000}
INFO:transformers.trainer:{'loss': 2.6170454412698745, 'learning_rate': 4.11981879682534e-05, 'epoch': 0.5281087219047962, 'step': 439500}
INFO:transformers.trainer:{'loss': 2.606507470369339, 'learning_rate': 4.118817453021956e-05, 'epoch': 0.5287095281868267, 'step': 440000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-440000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-440000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-440000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-420000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.601909960269928, 'learning_rate': 4.117816109218571e-05, 'epoch': 0.5293103344688572, 'step': 440500}
INFO:transformers.trainer:{'loss': 2.6130018336772918, 'learning_rate': 4.1168147654151875e-05, 'epoch': 0.5299111407508877, 'step': 441000}
INFO:transformers.trainer:{'loss': 2.603348931670189, 'learning_rate': 4.115813421611803e-05, 'epoch': 0.5305119470329182, 'step': 441500}
INFO:transformers.trainer:{'loss': 2.6591634665727617, 'learning_rate': 4.1148120778084196e-05, 'epoch': 0.5311127533149487, 'step': 442000}
INFO:transformers.trainer:{'loss': 2.645877855062485, 'learning_rate': 4.1138107340050346e-05, 'epoch': 0.5317135595969792, 'step': 442500}
INFO:transformers.trainer:{'loss': 2.639830222249031, 'learning_rate': 4.112809390201651e-05, 'epoch': 0.5323143658790096, 'step': 443000}
INFO:transformers.trainer:{'loss': 2.5950010205507277, 'learning_rate': 4.111808046398267e-05, 'epoch': 0.5329151721610401, 'step': 443500}
INFO:transformers.trainer:{'loss': 2.6515702135562895, 'learning_rate': 4.1108067025948824e-05, 'epoch': 0.5335159784430706, 'step': 444000}
INFO:transformers.trainer:{'loss': 2.6363140947818757, 'learning_rate': 4.109805358791499e-05, 'epoch': 0.534116784725101, 'step': 444500}
INFO:transformers.trainer:{'loss': 2.6306425358057024, 'learning_rate': 4.108804014988114e-05, 'epoch': 0.5347175910071316, 'step': 445000}
INFO:transformers.trainer:{'loss': 2.693239441752434, 'learning_rate': 4.10780267118473e-05, 'epoch': 0.5353183972891621, 'step': 445500}
INFO:transformers.trainer:{'loss': 2.687721750974655, 'learning_rate': 4.106801327381346e-05, 'epoch': 0.5359192035711925, 'step': 446000}
INFO:transformers.trainer:{'loss': 2.5957437820434572, 'learning_rate': 4.105799983577962e-05, 'epoch': 0.536520009853223, 'step': 446500}
INFO:transformers.trainer:{'loss': 2.5512633286714554, 'learning_rate': 4.104798639774577e-05, 'epoch': 0.5371208161352535, 'step': 447000}
INFO:transformers.trainer:{'loss': 2.591134879589081, 'learning_rate': 4.103797295971194e-05, 'epoch': 0.5377216224172839, 'step': 447500}
INFO:transformers.trainer:{'loss': 2.600468239426613, 'learning_rate': 4.1027959521678094e-05, 'epoch': 0.5383224286993145, 'step': 448000}
INFO:transformers.trainer:{'loss': 2.5630393645763396, 'learning_rate': 4.101794608364425e-05, 'epoch': 0.538923234981345, 'step': 448500}
INFO:transformers.trainer:{'loss': 2.5754429291486742, 'learning_rate': 4.100793264561041e-05, 'epoch': 0.5395240412633755, 'step': 449000}
INFO:transformers.trainer:{'loss': 2.6237627153396605, 'learning_rate': 4.0997919207576565e-05, 'epoch': 0.5401248475454059, 'step': 449500}
INFO:transformers.trainer:{'loss': 2.6469482687711716, 'learning_rate': 4.098790576954273e-05, 'epoch': 0.5407256538274364, 'step': 450000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-450000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-450000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-450000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-430000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6493447008132933, 'learning_rate': 4.0977892331508886e-05, 'epoch': 0.541326460109467, 'step': 450500}
INFO:transformers.trainer:{'loss': 2.600624432682991, 'learning_rate': 4.096787889347505e-05, 'epoch': 0.5419272663914974, 'step': 451000}
INFO:transformers.trainer:{'loss': 2.657399675607681, 'learning_rate': 4.09578654554412e-05, 'epoch': 0.5425280726735279, 'step': 451500}
INFO:transformers.trainer:{'loss': 2.595554922103882, 'learning_rate': 4.0947852017407364e-05, 'epoch': 0.5431288789555584, 'step': 452000}
INFO:transformers.trainer:{'loss': 2.554998037099838, 'learning_rate': 4.093783857937352e-05, 'epoch': 0.5437296852375888, 'step': 452500}
INFO:transformers.trainer:{'loss': 2.607710011959076, 'learning_rate': 4.0927825141339685e-05, 'epoch': 0.5443304915196193, 'step': 453000}
INFO:transformers.trainer:{'loss': 2.6262260335683822, 'learning_rate': 4.0917811703305835e-05, 'epoch': 0.5449312978016498, 'step': 453500}
INFO:transformers.trainer:{'loss': 2.616760890722275, 'learning_rate': 4.0907798265272e-05, 'epoch': 0.5455321040836802, 'step': 454000}
INFO:transformers.trainer:{'loss': 2.569009879589081, 'learning_rate': 4.0897784827238156e-05, 'epoch': 0.5461329103657108, 'step': 454500}
INFO:transformers.trainer:{'loss': 2.594771870017052, 'learning_rate': 4.088777138920431e-05, 'epoch': 0.5467337166477413, 'step': 455000}
INFO:transformers.trainer:{'loss': 2.650191999912262, 'learning_rate': 4.087775795117047e-05, 'epoch': 0.5473345229297718, 'step': 455500}
INFO:transformers.trainer:{'loss': 2.6169293479919435, 'learning_rate': 4.086774451313663e-05, 'epoch': 0.5479353292118022, 'step': 456000}
INFO:transformers.trainer:{'loss': 2.6082426064014435, 'learning_rate': 4.085773107510279e-05, 'epoch': 0.5485361354938327, 'step': 456500}
INFO:transformers.trainer:{'loss': 2.6009897418022154, 'learning_rate': 4.084771763706895e-05, 'epoch': 0.5491369417758633, 'step': 457000}
INFO:transformers.trainer:{'loss': 2.5643356636762618, 'learning_rate': 4.083770419903511e-05, 'epoch': 0.5497377480578937, 'step': 457500}
INFO:transformers.trainer:{'loss': 2.616642212152481, 'learning_rate': 4.082769076100126e-05, 'epoch': 0.5503385543399242, 'step': 458000}
INFO:transformers.trainer:{'loss': 2.5899118993282317, 'learning_rate': 4.0817677322967426e-05, 'epoch': 0.5509393606219547, 'step': 458500}
INFO:transformers.trainer:{'loss': 2.5873987822532656, 'learning_rate': 4.080766388493358e-05, 'epoch': 0.5515401669039851, 'step': 459000}
INFO:transformers.trainer:{'loss': 2.64298585832119, 'learning_rate': 4.079765044689974e-05, 'epoch': 0.5521409731860156, 'step': 459500}
INFO:transformers.trainer:{'loss': 2.6445399622917174, 'learning_rate': 4.07876370088659e-05, 'epoch': 0.5527417794680461, 'step': 460000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-460000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-460000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-460000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-440000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5440581517219543, 'learning_rate': 4.0777623570832054e-05, 'epoch': 0.5533425857500766, 'step': 460500}
INFO:transformers.trainer:{'loss': 2.633028827905655, 'learning_rate': 4.076761013279822e-05, 'epoch': 0.5539433920321071, 'step': 461000}
INFO:transformers.trainer:{'loss': 2.5914246435165404, 'learning_rate': 4.0757596694764375e-05, 'epoch': 0.5545441983141376, 'step': 461500}
INFO:transformers.trainer:{'loss': 2.6095356538295746, 'learning_rate': 4.074758325673053e-05, 'epoch': 0.5551450045961681, 'step': 462000}
INFO:transformers.trainer:{'loss': 2.615742502450943, 'learning_rate': 4.073756981869669e-05, 'epoch': 0.5557458108781985, 'step': 462500}
INFO:transformers.trainer:{'loss': 2.6046513174772263, 'learning_rate': 4.072755638066285e-05, 'epoch': 0.556346617160229, 'step': 463000}
INFO:transformers.trainer:{'loss': 2.651381617188454, 'learning_rate': 4.071754294262901e-05, 'epoch': 0.5569474234422596, 'step': 463500}
INFO:transformers.trainer:{'loss': 2.6510030667185784, 'learning_rate': 4.0707529504595174e-05, 'epoch': 0.55754822972429, 'step': 464000}
INFO:transformers.trainer:{'loss': 2.6159191600084304, 'learning_rate': 4.0697516066561325e-05, 'epoch': 0.5581490360063205, 'step': 464500}
INFO:transformers.trainer:{'loss': 2.5839428046941757, 'learning_rate': 4.068750262852749e-05, 'epoch': 0.558749842288351, 'step': 465000}
INFO:transformers.trainer:{'loss': 2.6006842083930968, 'learning_rate': 4.0677489190493645e-05, 'epoch': 0.5593506485703814, 'step': 465500}
INFO:transformers.trainer:{'loss': 2.613906904578209, 'learning_rate': 4.06674757524598e-05, 'epoch': 0.5599514548524119, 'step': 466000}
INFO:transformers.trainer:{'loss': 2.590531030535698, 'learning_rate': 4.065746231442596e-05, 'epoch': 0.5605522611344425, 'step': 466500}
INFO:transformers.trainer:{'loss': 2.620558779001236, 'learning_rate': 4.064744887639212e-05, 'epoch': 0.5611530674164729, 'step': 467000}
INFO:transformers.trainer:{'loss': 2.6154466601610182, 'learning_rate': 4.063743543835828e-05, 'epoch': 0.5617538736985034, 'step': 467500}
INFO:transformers.trainer:{'loss': 2.6656264629364013, 'learning_rate': 4.062742200032444e-05, 'epoch': 0.5623546799805339, 'step': 468000}
INFO:transformers.trainer:{'loss': 2.5368189237117766, 'learning_rate': 4.06174085622906e-05, 'epoch': 0.5629554862625643, 'step': 468500}
INFO:transformers.trainer:{'loss': 2.6185960867404936, 'learning_rate': 4.060739512425675e-05, 'epoch': 0.5635562925445948, 'step': 469000}
INFO:transformers.trainer:{'loss': 2.5956823873519896, 'learning_rate': 4.0597381686222915e-05, 'epoch': 0.5641570988266253, 'step': 469500}
INFO:transformers.trainer:{'loss': 2.621994425058365, 'learning_rate': 4.058736824818907e-05, 'epoch': 0.5647579051086559, 'step': 470000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-470000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-470000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-470000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-450000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6240813022851945, 'learning_rate': 4.057735481015523e-05, 'epoch': 0.5653587113906863, 'step': 470500}
INFO:transformers.trainer:{'loss': 2.627528475046158, 'learning_rate': 4.056734137212139e-05, 'epoch': 0.5659595176727168, 'step': 471000}
INFO:transformers.trainer:{'loss': 2.5890904924273492, 'learning_rate': 4.0557327934087544e-05, 'epoch': 0.5665603239547473, 'step': 471500}
INFO:transformers.trainer:{'loss': 2.614741425871849, 'learning_rate': 4.054731449605371e-05, 'epoch': 0.5671611302367777, 'step': 472000}
INFO:transformers.trainer:{'loss': 2.6097381192445757, 'learning_rate': 4.0537301058019865e-05, 'epoch': 0.5677619365188082, 'step': 472500}
INFO:transformers.trainer:{'loss': 2.586816458582878, 'learning_rate': 4.052728761998602e-05, 'epoch': 0.5683627428008388, 'step': 473000}
INFO:transformers.trainer:{'loss': 2.5725439490079878, 'learning_rate': 4.051727418195218e-05, 'epoch': 0.5689635490828692, 'step': 473500}
INFO:transformers.trainer:{'loss': 2.5995088710784913, 'learning_rate': 4.050726074391834e-05, 'epoch': 0.5695643553648997, 'step': 474000}
INFO:transformers.trainer:{'loss': 2.6230375182628634, 'learning_rate': 4.04972473058845e-05, 'epoch': 0.5701651616469302, 'step': 474500}
INFO:transformers.trainer:{'loss': 2.6126716940402983, 'learning_rate': 4.0487233867850664e-05, 'epoch': 0.5707659679289606, 'step': 475000}
INFO:transformers.trainer:{'loss': 2.6579750475883483, 'learning_rate': 4.0477220429816814e-05, 'epoch': 0.5713667742109911, 'step': 475500}
INFO:transformers.trainer:{'loss': 2.676100685238838, 'learning_rate': 4.046720699178298e-05, 'epoch': 0.5719675804930217, 'step': 476000}
INFO:transformers.trainer:{'loss': 2.5546227370500563, 'learning_rate': 4.0457193553749135e-05, 'epoch': 0.5725683867750522, 'step': 476500}
INFO:transformers.trainer:{'loss': 2.6267068454027176, 'learning_rate': 4.044718011571529e-05, 'epoch': 0.5731691930570826, 'step': 477000}
INFO:transformers.trainer:{'loss': 2.626182543158531, 'learning_rate': 4.043716667768145e-05, 'epoch': 0.5737699993391131, 'step': 477500}
INFO:transformers.trainer:{'loss': 2.612119485139847, 'learning_rate': 4.0427153239647606e-05, 'epoch': 0.5743708056211436, 'step': 478000}
INFO:transformers.trainer:{'loss': 2.5542780129909515, 'learning_rate': 4.041713980161377e-05, 'epoch': 0.574971611903174, 'step': 478500}
INFO:transformers.trainer:{'loss': 2.6101703370809557, 'learning_rate': 4.040712636357993e-05, 'epoch': 0.5755724181852045, 'step': 479000}
INFO:transformers.trainer:{'loss': 2.6394182685613634, 'learning_rate': 4.0397112925546084e-05, 'epoch': 0.5761732244672351, 'step': 479500}
INFO:transformers.trainer:{'loss': 2.641220554351807, 'learning_rate': 4.038709948751224e-05, 'epoch': 0.5767740307492655, 'step': 480000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-480000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-480000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-480000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-460000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5968536440134047, 'learning_rate': 4.0377086049478405e-05, 'epoch': 0.577374837031296, 'step': 480500}
INFO:transformers.trainer:{'loss': 2.5334594925642016, 'learning_rate': 4.036707261144456e-05, 'epoch': 0.5779756433133265, 'step': 481000}
INFO:transformers.trainer:{'loss': 2.5840382521152496, 'learning_rate': 4.035705917341072e-05, 'epoch': 0.5785764495953569, 'step': 481500}
INFO:transformers.trainer:{'loss': 2.664920828938484, 'learning_rate': 4.0347045735376876e-05, 'epoch': 0.5791772558773874, 'step': 482000}
INFO:transformers.trainer:{'loss': 2.6091509059667586, 'learning_rate': 4.033703229734303e-05, 'epoch': 0.579778062159418, 'step': 482500}
INFO:transformers.trainer:{'loss': 2.5930501936674117, 'learning_rate': 4.03270188593092e-05, 'epoch': 0.5803788684414484, 'step': 483000}
INFO:transformers.trainer:{'loss': 2.5680221507549286, 'learning_rate': 4.0317005421275354e-05, 'epoch': 0.5809796747234789, 'step': 483500}
INFO:transformers.trainer:{'loss': 2.6024847820997237, 'learning_rate': 4.030699198324151e-05, 'epoch': 0.5815804810055094, 'step': 484000}
INFO:transformers.trainer:{'loss': 2.6237572220563887, 'learning_rate': 4.029697854520767e-05, 'epoch': 0.5821812872875399, 'step': 484500}
INFO:transformers.trainer:{'loss': 2.5884515563249586, 'learning_rate': 4.028696510717383e-05, 'epoch': 0.5827820935695703, 'step': 485000}
INFO:transformers.trainer:{'loss': 2.6580278577804566, 'learning_rate': 4.027695166913999e-05, 'epoch': 0.5833828998516009, 'step': 485500}
INFO:transformers.trainer:{'loss': 2.5971151113510134, 'learning_rate': 4.0266938231106146e-05, 'epoch': 0.5839837061336314, 'step': 486000}
INFO:transformers.trainer:{'loss': 2.6384184596538542, 'learning_rate': 4.02569247930723e-05, 'epoch': 0.5845845124156618, 'step': 486500}
INFO:transformers.trainer:{'loss': 2.5637047049999238, 'learning_rate': 4.024691135503846e-05, 'epoch': 0.5851853186976923, 'step': 487000}
INFO:transformers.trainer:{'loss': 2.583160719156265, 'learning_rate': 4.0236897917004624e-05, 'epoch': 0.5857861249797228, 'step': 487500}
INFO:transformers.trainer:{'loss': 2.634642989516258, 'learning_rate': 4.022688447897078e-05, 'epoch': 0.5863869312617532, 'step': 488000}
INFO:transformers.trainer:{'loss': 2.5943607682585714, 'learning_rate': 4.021687104093694e-05, 'epoch': 0.5869877375437837, 'step': 488500}
INFO:transformers.trainer:{'loss': 2.6040114946365356, 'learning_rate': 4.0206857602903095e-05, 'epoch': 0.5875885438258143, 'step': 489000}
INFO:transformers.trainer:{'loss': 2.624266193628311, 'learning_rate': 4.019684416486926e-05, 'epoch': 0.5881893501078447, 'step': 489500}
INFO:transformers.trainer:{'loss': 2.6171074209213256, 'learning_rate': 4.0186830726835416e-05, 'epoch': 0.5887901563898752, 'step': 490000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-490000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-490000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-490000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-470000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.630472934603691, 'learning_rate': 4.017681728880157e-05, 'epoch': 0.5893909626719057, 'step': 490500}
INFO:transformers.trainer:{'loss': 2.660506538748741, 'learning_rate': 4.016680385076773e-05, 'epoch': 0.5899917689539362, 'step': 491000}
INFO:transformers.trainer:{'loss': 2.571102929353714, 'learning_rate': 4.0156790412733894e-05, 'epoch': 0.5905925752359666, 'step': 491500}
INFO:transformers.trainer:{'loss': 2.649311820626259, 'learning_rate': 4.014677697470005e-05, 'epoch': 0.5911933815179972, 'step': 492000}
INFO:transformers.trainer:{'loss': 2.6264906655550004, 'learning_rate': 4.013676353666621e-05, 'epoch': 0.5917941878000277, 'step': 492500}
INFO:transformers.trainer:{'loss': 2.6170101159214973, 'learning_rate': 4.0126750098632365e-05, 'epoch': 0.5923949940820581, 'step': 493000}
INFO:transformers.trainer:{'loss': 2.6990209308862685, 'learning_rate': 4.011673666059852e-05, 'epoch': 0.5929958003640886, 'step': 493500}
INFO:transformers.trainer:{'loss': 2.5424391343593595, 'learning_rate': 4.0106723222564686e-05, 'epoch': 0.5935966066461191, 'step': 494000}
INFO:transformers.trainer:{'loss': 2.6782294574975967, 'learning_rate': 4.009670978453084e-05, 'epoch': 0.5941974129281495, 'step': 494500}
INFO:transformers.trainer:{'loss': 2.6047750017642977, 'learning_rate': 4.0086696346497e-05, 'epoch': 0.5947982192101801, 'step': 495000}
INFO:transformers.trainer:{'loss': 2.6077222139239313, 'learning_rate': 4.007668290846316e-05, 'epoch': 0.5953990254922106, 'step': 495500}
INFO:transformers.trainer:{'loss': 2.641015077114105, 'learning_rate': 4.006666947042932e-05, 'epoch': 0.595999831774241, 'step': 496000}
INFO:transformers.trainer:{'loss': 2.628077772021294, 'learning_rate': 4.005665603239548e-05, 'epoch': 0.5966006380562715, 'step': 496500}
INFO:transformers.trainer:{'loss': 2.6409717359542846, 'learning_rate': 4.0046642594361635e-05, 'epoch': 0.597201444338302, 'step': 497000}
INFO:transformers.trainer:{'loss': 2.571799220442772, 'learning_rate': 4.003662915632779e-05, 'epoch': 0.5978022506203324, 'step': 497500}
INFO:transformers.trainer:{'loss': 2.6089561280012132, 'learning_rate': 4.002661571829395e-05, 'epoch': 0.598403056902363, 'step': 498000}
INFO:transformers.trainer:{'loss': 2.5955202758312224, 'learning_rate': 4.001660228026011e-05, 'epoch': 0.5990038631843935, 'step': 498500}
INFO:transformers.trainer:{'loss': 2.5931755042076112, 'learning_rate': 4.0006588842226264e-05, 'epoch': 0.599604669466424, 'step': 499000}
INFO:transformers.trainer:{'loss': 2.6105793943405153, 'learning_rate': 3.999657540419243e-05, 'epoch': 0.6002054757484544, 'step': 499500}
INFO:transformers.trainer:{'loss': 2.639871755361557, 'learning_rate': 3.9986561966158585e-05, 'epoch': 0.6008062820304849, 'step': 500000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-500000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-500000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-500000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-480000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5856108080148696, 'learning_rate': 3.997654852812475e-05, 'epoch': 0.6014070883125154, 'step': 500500}
INFO:transformers.trainer:{'loss': 2.6350109177827834, 'learning_rate': 3.9966535090090905e-05, 'epoch': 0.6020078945945458, 'step': 501000}
INFO:transformers.trainer:{'loss': 2.6082775994539262, 'learning_rate': 3.995652165205706e-05, 'epoch': 0.6026087008765764, 'step': 501500}
INFO:transformers.trainer:{'loss': 2.673705189347267, 'learning_rate': 3.994650821402322e-05, 'epoch': 0.6032095071586069, 'step': 502000}
INFO:transformers.trainer:{'loss': 2.6023513479232787, 'learning_rate': 3.993649477598938e-05, 'epoch': 0.6038103134406373, 'step': 502500}
INFO:transformers.trainer:{'loss': 2.6306502205133437, 'learning_rate': 3.992648133795554e-05, 'epoch': 0.6044111197226678, 'step': 503000}
INFO:transformers.trainer:{'loss': 2.583315871119499, 'learning_rate': 3.99164678999217e-05, 'epoch': 0.6050119260046983, 'step': 503500}
INFO:transformers.trainer:{'loss': 2.6227294239997865, 'learning_rate': 3.9906454461887855e-05, 'epoch': 0.6056127322867287, 'step': 504000}
INFO:transformers.trainer:{'loss': 2.601256432533264, 'learning_rate': 3.989644102385401e-05, 'epoch': 0.6062135385687593, 'step': 504500}
INFO:transformers.trainer:{'loss': 2.6524429368972777, 'learning_rate': 3.9886427585820175e-05, 'epoch': 0.6068143448507898, 'step': 505000}
INFO:transformers.trainer:{'loss': 2.639505209326744, 'learning_rate': 3.9876414147786326e-05, 'epoch': 0.6074151511328203, 'step': 505500}
INFO:transformers.trainer:{'loss': 2.619543905735016, 'learning_rate': 3.986640070975249e-05, 'epoch': 0.6080159574148507, 'step': 506000}
INFO:transformers.trainer:{'loss': 2.5649211282730104, 'learning_rate': 3.985638727171865e-05, 'epoch': 0.6086167636968812, 'step': 506500}
INFO:transformers.trainer:{'loss': 2.5771053482294084, 'learning_rate': 3.984637383368481e-05, 'epoch': 0.6092175699789117, 'step': 507000}
INFO:transformers.trainer:{'loss': 2.6276728394031523, 'learning_rate': 3.983636039565097e-05, 'epoch': 0.6098183762609422, 'step': 507500}
INFO:transformers.trainer:{'loss': 2.6013381323814393, 'learning_rate': 3.9826346957617125e-05, 'epoch': 0.6104191825429727, 'step': 508000}
INFO:transformers.trainer:{'loss': 2.566494275569916, 'learning_rate': 3.981633351958328e-05, 'epoch': 0.6110199888250032, 'step': 508500}
INFO:transformers.trainer:{'loss': 2.6830328831672667, 'learning_rate': 3.980632008154944e-05, 'epoch': 0.6116207951070336, 'step': 509000}
INFO:transformers.trainer:{'loss': 2.5944338071346285, 'learning_rate': 3.97963066435156e-05, 'epoch': 0.6122216013890641, 'step': 509500}
INFO:transformers.trainer:{'loss': 2.611324942827225, 'learning_rate': 3.978629320548175e-05, 'epoch': 0.6128224076710946, 'step': 510000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-510000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-510000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-510000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-490000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.624373200893402, 'learning_rate': 3.977627976744792e-05, 'epoch': 0.613423213953125, 'step': 510500}
INFO:transformers.trainer:{'loss': 2.6069302996397017, 'learning_rate': 3.9766266329414074e-05, 'epoch': 0.6140240202351556, 'step': 511000}
INFO:transformers.trainer:{'loss': 2.6006582307815553, 'learning_rate': 3.975625289138024e-05, 'epoch': 0.6146248265171861, 'step': 511500}
INFO:transformers.trainer:{'loss': 2.58340432536602, 'learning_rate': 3.974623945334639e-05, 'epoch': 0.6152256327992166, 'step': 512000}
INFO:transformers.trainer:{'loss': 2.6203874837160113, 'learning_rate': 3.973622601531255e-05, 'epoch': 0.615826439081247, 'step': 512500}
INFO:transformers.trainer:{'loss': 2.585370472431183, 'learning_rate': 3.972621257727871e-05, 'epoch': 0.6164272453632775, 'step': 513000}
INFO:transformers.trainer:{'loss': 2.6070190987586974, 'learning_rate': 3.971619913924487e-05, 'epoch': 0.617028051645308, 'step': 513500}
INFO:transformers.trainer:{'loss': 2.6528396064043047, 'learning_rate': 3.970618570121103e-05, 'epoch': 0.6176288579273385, 'step': 514000}
INFO:transformers.trainer:{'loss': 2.6090391314029695, 'learning_rate': 3.969617226317719e-05, 'epoch': 0.618229664209369, 'step': 514500}
INFO:transformers.trainer:{'loss': 2.6485936605930327, 'learning_rate': 3.9686158825143344e-05, 'epoch': 0.6188304704913995, 'step': 515000}
INFO:transformers.trainer:{'loss': 2.5662204254865646, 'learning_rate': 3.96761453871095e-05, 'epoch': 0.6194312767734299, 'step': 515500}
INFO:transformers.trainer:{'loss': 2.6190604259967802, 'learning_rate': 3.9666131949075665e-05, 'epoch': 0.6200320830554604, 'step': 516000}
INFO:transformers.trainer:{'loss': 2.575322417616844, 'learning_rate': 3.9656118511041815e-05, 'epoch': 0.620632889337491, 'step': 516500}
INFO:transformers.trainer:{'loss': 2.6019851912260057, 'learning_rate': 3.964610507300798e-05, 'epoch': 0.6212336956195214, 'step': 517000}
INFO:transformers.trainer:{'loss': 2.6448061304092407, 'learning_rate': 3.9636091634974136e-05, 'epoch': 0.6218345019015519, 'step': 517500}
INFO:transformers.trainer:{'loss': 2.6411788189411163, 'learning_rate': 3.96260781969403e-05, 'epoch': 0.6224353081835824, 'step': 518000}
INFO:transformers.trainer:{'loss': 2.615636627435684, 'learning_rate': 3.961606475890646e-05, 'epoch': 0.6230361144656128, 'step': 518500}
INFO:transformers.trainer:{'loss': 2.6132587218284606, 'learning_rate': 3.9606051320872614e-05, 'epoch': 0.6236369207476433, 'step': 519000}
INFO:transformers.trainer:{'loss': 2.634147905111313, 'learning_rate': 3.959603788283877e-05, 'epoch': 0.6242377270296738, 'step': 519500}
INFO:transformers.trainer:{'loss': 2.5712609237432478, 'learning_rate': 3.958602444480493e-05, 'epoch': 0.6248385333117044, 'step': 520000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-520000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-520000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-520000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-500000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5859306164979934, 'learning_rate': 3.957601100677109e-05, 'epoch': 0.6254393395937348, 'step': 520500}
INFO:transformers.trainer:{'loss': 2.653499816775322, 'learning_rate': 3.956599756873724e-05, 'epoch': 0.6260401458757653, 'step': 521000}
INFO:transformers.trainer:{'loss': 2.6399741970300674, 'learning_rate': 3.9555984130703406e-05, 'epoch': 0.6266409521577958, 'step': 521500}
INFO:transformers.trainer:{'loss': 2.576945409178734, 'learning_rate': 3.954597069266956e-05, 'epoch': 0.6272417584398262, 'step': 522000}
INFO:transformers.trainer:{'loss': 2.6220242948532104, 'learning_rate': 3.953595725463573e-05, 'epoch': 0.6278425647218567, 'step': 522500}
INFO:transformers.trainer:{'loss': 2.6019728400707245, 'learning_rate': 3.952594381660188e-05, 'epoch': 0.6284433710038873, 'step': 523000}
INFO:transformers.trainer:{'loss': 2.624133134841919, 'learning_rate': 3.951593037856804e-05, 'epoch': 0.6290441772859177, 'step': 523500}
INFO:transformers.trainer:{'loss': 2.5960013459920885, 'learning_rate': 3.95059169405342e-05, 'epoch': 0.6296449835679482, 'step': 524000}
INFO:transformers.trainer:{'loss': 2.6050331743955613, 'learning_rate': 3.949590350250036e-05, 'epoch': 0.6302457898499787, 'step': 524500}
INFO:transformers.trainer:{'loss': 2.660768058538437, 'learning_rate': 3.948589006446652e-05, 'epoch': 0.6308465961320091, 'step': 525000}
INFO:transformers.trainer:{'loss': 2.5859178375005722, 'learning_rate': 3.9475876626432676e-05, 'epoch': 0.6314474024140396, 'step': 525500}
INFO:transformers.trainer:{'loss': 2.619594864845276, 'learning_rate': 3.946586318839883e-05, 'epoch': 0.6320482086960701, 'step': 526000}
INFO:transformers.trainer:{'loss': 2.6426124681234358, 'learning_rate': 3.945584975036499e-05, 'epoch': 0.6326490149781007, 'step': 526500}
INFO:transformers.trainer:{'loss': 2.583834054708481, 'learning_rate': 3.9445836312331154e-05, 'epoch': 0.6332498212601311, 'step': 527000}
INFO:transformers.trainer:{'loss': 2.643416866540909, 'learning_rate': 3.9435822874297304e-05, 'epoch': 0.6338506275421616, 'step': 527500}
INFO:transformers.trainer:{'loss': 2.586157512664795, 'learning_rate': 3.942580943626347e-05, 'epoch': 0.6344514338241921, 'step': 528000}
INFO:transformers.trainer:{'loss': 2.563140942454338, 'learning_rate': 3.9415795998229625e-05, 'epoch': 0.6350522401062225, 'step': 528500}
INFO:transformers.trainer:{'loss': 2.6064937114715576, 'learning_rate': 3.940578256019579e-05, 'epoch': 0.635653046388253, 'step': 529000}
INFO:transformers.trainer:{'loss': 2.55128864300251, 'learning_rate': 3.939576912216194e-05, 'epoch': 0.6362538526702836, 'step': 529500}
INFO:transformers.trainer:{'loss': 2.677157621383667, 'learning_rate': 3.93857556841281e-05, 'epoch': 0.636854658952314, 'step': 530000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-530000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-530000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-530000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-510000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5548025867938997, 'learning_rate': 3.937574224609426e-05, 'epoch': 0.6374554652343445, 'step': 530500}
INFO:transformers.trainer:{'loss': 2.6258995517492294, 'learning_rate': 3.936572880806042e-05, 'epoch': 0.638056271516375, 'step': 531000}
INFO:transformers.trainer:{'loss': 2.601682683467865, 'learning_rate': 3.935571537002658e-05, 'epoch': 0.6386570777984054, 'step': 531500}
INFO:transformers.trainer:{'loss': 2.578316384077072, 'learning_rate': 3.934570193199273e-05, 'epoch': 0.6392578840804359, 'step': 532000}
INFO:transformers.trainer:{'loss': 2.596468144416809, 'learning_rate': 3.9335688493958895e-05, 'epoch': 0.6398586903624665, 'step': 532500}
INFO:transformers.trainer:{'loss': 2.6029001301527024, 'learning_rate': 3.932567505592505e-05, 'epoch': 0.6404594966444969, 'step': 533000}
INFO:transformers.trainer:{'loss': 2.5679724149703977, 'learning_rate': 3.9315661617891216e-05, 'epoch': 0.6410603029265274, 'step': 533500}
INFO:transformers.trainer:{'loss': 2.638060350179672, 'learning_rate': 3.9305648179857367e-05, 'epoch': 0.6416611092085579, 'step': 534000}
INFO:transformers.trainer:{'loss': 2.562471164226532, 'learning_rate': 3.929563474182353e-05, 'epoch': 0.6422619154905884, 'step': 534500}
INFO:transformers.trainer:{'loss': 2.5999824991226195, 'learning_rate': 3.928562130378969e-05, 'epoch': 0.6428627217726188, 'step': 535000}
INFO:transformers.trainer:{'loss': 2.559007954239845, 'learning_rate': 3.927560786575585e-05, 'epoch': 0.6434635280546493, 'step': 535500}
INFO:transformers.trainer:{'loss': 2.614588700890541, 'learning_rate': 3.9265594427722e-05, 'epoch': 0.6440643343366799, 'step': 536000}
INFO:transformers.trainer:{'loss': 2.648325611948967, 'learning_rate': 3.9255580989688165e-05, 'epoch': 0.6446651406187103, 'step': 536500}
INFO:transformers.trainer:{'loss': 2.5424997465610506, 'learning_rate': 3.924556755165432e-05, 'epoch': 0.6452659469007408, 'step': 537000}
INFO:transformers.trainer:{'loss': 2.6560653500556946, 'learning_rate': 3.923555411362048e-05, 'epoch': 0.6458667531827713, 'step': 537500}
INFO:transformers.trainer:{'loss': 2.5933976089954376, 'learning_rate': 3.922554067558664e-05, 'epoch': 0.6464675594648017, 'step': 538000}
INFO:transformers.trainer:{'loss': 2.535505421280861, 'learning_rate': 3.9215527237552794e-05, 'epoch': 0.6470683657468322, 'step': 538500}
INFO:transformers.trainer:{'loss': 2.6256359837055205, 'learning_rate': 3.920551379951896e-05, 'epoch': 0.6476691720288628, 'step': 539000}
INFO:transformers.trainer:{'loss': 2.6198100326061247, 'learning_rate': 3.9195500361485115e-05, 'epoch': 0.6482699783108932, 'step': 539500}
INFO:transformers.trainer:{'loss': 2.55169962644577, 'learning_rate': 3.918548692345128e-05, 'epoch': 0.6488707845929237, 'step': 540000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-540000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-540000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-540000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-520000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5927020643949508, 'learning_rate': 3.917547348541743e-05, 'epoch': 0.6494715908749542, 'step': 540500}
INFO:transformers.trainer:{'loss': 2.6060761089324953, 'learning_rate': 3.916546004738359e-05, 'epoch': 0.6500723971569847, 'step': 541000}
INFO:transformers.trainer:{'loss': 2.6437704157829285, 'learning_rate': 3.915544660934975e-05, 'epoch': 0.6506732034390151, 'step': 541500}
INFO:transformers.trainer:{'loss': 2.6210601725578306, 'learning_rate': 3.914543317131591e-05, 'epoch': 0.6512740097210457, 'step': 542000}
INFO:transformers.trainer:{'loss': 2.5971348137855528, 'learning_rate': 3.9135419733282064e-05, 'epoch': 0.6518748160030762, 'step': 542500}
INFO:transformers.trainer:{'loss': 2.650611613035202, 'learning_rate': 3.912540629524822e-05, 'epoch': 0.6524756222851066, 'step': 543000}
INFO:transformers.trainer:{'loss': 2.5957173504829405, 'learning_rate': 3.9115392857214385e-05, 'epoch': 0.6530764285671371, 'step': 543500}
INFO:transformers.trainer:{'loss': 2.658178099274635, 'learning_rate': 3.910537941918054e-05, 'epoch': 0.6536772348491676, 'step': 544000}
INFO:transformers.trainer:{'loss': 2.603955518960953, 'learning_rate': 3.9095365981146706e-05, 'epoch': 0.654278041131198, 'step': 544500}
INFO:transformers.trainer:{'loss': 2.602575501680374, 'learning_rate': 3.9085352543112856e-05, 'epoch': 0.6548788474132285, 'step': 545000}
INFO:transformers.trainer:{'loss': 2.633733618080616, 'learning_rate': 3.907533910507902e-05, 'epoch': 0.6554796536952591, 'step': 545500}
INFO:transformers.trainer:{'loss': 2.578026520252228, 'learning_rate': 3.906532566704518e-05, 'epoch': 0.6560804599772895, 'step': 546000}
INFO:transformers.trainer:{'loss': 2.626532551646233, 'learning_rate': 3.9055312229011334e-05, 'epoch': 0.65668126625932, 'step': 546500}
INFO:transformers.trainer:{'loss': 2.562524371147156, 'learning_rate': 3.904529879097749e-05, 'epoch': 0.6572820725413505, 'step': 547000}
INFO:transformers.trainer:{'loss': 2.6086895503401757, 'learning_rate': 3.9035285352943655e-05, 'epoch': 0.6578828788233809, 'step': 547500}
INFO:transformers.trainer:{'loss': 2.570197903037071, 'learning_rate': 3.902527191490981e-05, 'epoch': 0.6584836851054114, 'step': 548000}
INFO:transformers.trainer:{'loss': 2.6044719042778017, 'learning_rate': 3.901525847687597e-05, 'epoch': 0.659084491387442, 'step': 548500}
INFO:transformers.trainer:{'loss': 2.583714283108711, 'learning_rate': 3.9005245038842126e-05, 'epoch': 0.6596852976694725, 'step': 549000}
INFO:transformers.trainer:{'loss': 2.5670491572618483, 'learning_rate': 3.899523160080828e-05, 'epoch': 0.6602861039515029, 'step': 549500}
INFO:transformers.trainer:{'loss': 2.607024170279503, 'learning_rate': 3.898521816277445e-05, 'epoch': 0.6608869102335334, 'step': 550000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-550000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-550000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-550000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-530000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6029136791229246, 'learning_rate': 3.8975204724740604e-05, 'epoch': 0.6614877165155639, 'step': 550500}
INFO:transformers.trainer:{'loss': 2.623205555677414, 'learning_rate': 3.896519128670677e-05, 'epoch': 0.6620885227975943, 'step': 551000}
INFO:transformers.trainer:{'loss': 2.6324858609437944, 'learning_rate': 3.895517784867292e-05, 'epoch': 0.6626893290796249, 'step': 551500}
INFO:transformers.trainer:{'loss': 2.519700812458992, 'learning_rate': 3.894516441063908e-05, 'epoch': 0.6632901353616554, 'step': 552000}
INFO:transformers.trainer:{'loss': 2.600926560640335, 'learning_rate': 3.893515097260524e-05, 'epoch': 0.6638909416436858, 'step': 552500}
INFO:transformers.trainer:{'loss': 2.5872147624492645, 'learning_rate': 3.8925137534571396e-05, 'epoch': 0.6644917479257163, 'step': 553000}
INFO:transformers.trainer:{'loss': 2.637120825767517, 'learning_rate': 3.891512409653755e-05, 'epoch': 0.6650925542077468, 'step': 553500}
INFO:transformers.trainer:{'loss': 2.59394548201561, 'learning_rate': 3.890511065850371e-05, 'epoch': 0.6656933604897772, 'step': 554000}
INFO:transformers.trainer:{'loss': 2.5994133867025377, 'learning_rate': 3.8895097220469874e-05, 'epoch': 0.6662941667718077, 'step': 554500}
INFO:transformers.trainer:{'loss': 2.6016732720136644, 'learning_rate': 3.888508378243603e-05, 'epoch': 0.6668949730538383, 'step': 555000}
INFO:transformers.trainer:{'loss': 2.5816352064609527, 'learning_rate': 3.887507034440219e-05, 'epoch': 0.6674957793358688, 'step': 555500}
INFO:transformers.trainer:{'loss': 2.6066272304058073, 'learning_rate': 3.8865056906368345e-05, 'epoch': 0.6680965856178992, 'step': 556000}
INFO:transformers.trainer:{'loss': 2.6418563470840453, 'learning_rate': 3.885504346833451e-05, 'epoch': 0.6686973918999297, 'step': 556500}
INFO:transformers.trainer:{'loss': 2.5982047688961027, 'learning_rate': 3.8845030030300666e-05, 'epoch': 0.6692981981819602, 'step': 557000}
INFO:transformers.trainer:{'loss': 2.6475897438526155, 'learning_rate': 3.883501659226682e-05, 'epoch': 0.6698990044639906, 'step': 557500}
INFO:transformers.trainer:{'loss': 2.543881376862526, 'learning_rate': 3.882500315423298e-05, 'epoch': 0.6704998107460212, 'step': 558000}
INFO:transformers.trainer:{'loss': 2.6160982086658477, 'learning_rate': 3.881498971619914e-05, 'epoch': 0.6711006170280517, 'step': 558500}
INFO:transformers.trainer:{'loss': 2.5508404022455213, 'learning_rate': 3.88049762781653e-05, 'epoch': 0.6717014233100821, 'step': 559000}
INFO:transformers.trainer:{'loss': 2.614520292639732, 'learning_rate': 3.879496284013146e-05, 'epoch': 0.6723022295921126, 'step': 559500}
INFO:transformers.trainer:{'loss': 2.573704940080643, 'learning_rate': 3.8784949402097615e-05, 'epoch': 0.6729030358741431, 'step': 560000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-560000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-560000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-560000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-540000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.594461527466774, 'learning_rate': 3.877493596406377e-05, 'epoch': 0.6735038421561735, 'step': 560500}
INFO:transformers.trainer:{'loss': 2.5729553544521333, 'learning_rate': 3.8764922526029936e-05, 'epoch': 0.6741046484382041, 'step': 561000}
INFO:transformers.trainer:{'loss': 2.555234929919243, 'learning_rate': 3.875490908799609e-05, 'epoch': 0.6747054547202346, 'step': 561500}
INFO:transformers.trainer:{'loss': 2.6003619351387024, 'learning_rate': 3.874489564996226e-05, 'epoch': 0.675306261002265, 'step': 562000}
INFO:transformers.trainer:{'loss': 2.631780859708786, 'learning_rate': 3.873488221192841e-05, 'epoch': 0.6759070672842955, 'step': 562500}
INFO:transformers.trainer:{'loss': 2.565631413936615, 'learning_rate': 3.872486877389457e-05, 'epoch': 0.676507873566326, 'step': 563000}
INFO:transformers.trainer:{'loss': 2.598222239613533, 'learning_rate': 3.871485533586073e-05, 'epoch': 0.6771086798483565, 'step': 563500}
INFO:transformers.trainer:{'loss': 2.6103472284078597, 'learning_rate': 3.8704841897826885e-05, 'epoch': 0.677709486130387, 'step': 564000}
INFO:transformers.trainer:{'loss': 2.6013636791706087, 'learning_rate': 3.869482845979304e-05, 'epoch': 0.6783102924124175, 'step': 564500}
INFO:transformers.trainer:{'loss': 2.6133332604169848, 'learning_rate': 3.86848150217592e-05, 'epoch': 0.678911098694448, 'step': 565000}
INFO:transformers.trainer:{'loss': 2.61312218773365, 'learning_rate': 3.867480158372536e-05, 'epoch': 0.6795119049764784, 'step': 565500}
INFO:transformers.trainer:{'loss': 2.5790449875593184, 'learning_rate': 3.866478814569152e-05, 'epoch': 0.6801127112585089, 'step': 566000}
INFO:transformers.trainer:{'loss': 2.5716436849832536, 'learning_rate': 3.865477470765768e-05, 'epoch': 0.6807135175405394, 'step': 566500}
INFO:transformers.trainer:{'loss': 2.595027010679245, 'learning_rate': 3.8644761269623834e-05, 'epoch': 0.6813143238225698, 'step': 567000}
INFO:transformers.trainer:{'loss': 2.5481717190742494, 'learning_rate': 3.863474783159e-05, 'epoch': 0.6819151301046004, 'step': 567500}
INFO:transformers.trainer:{'loss': 2.616759628653526, 'learning_rate': 3.8624734393556155e-05, 'epoch': 0.6825159363866309, 'step': 568000}
INFO:transformers.trainer:{'loss': 2.6289813549518586, 'learning_rate': 3.861472095552231e-05, 'epoch': 0.6831167426686613, 'step': 568500}
INFO:transformers.trainer:{'loss': 2.5365054961442945, 'learning_rate': 3.860470751748847e-05, 'epoch': 0.6837175489506918, 'step': 569000}
INFO:transformers.trainer:{'loss': 2.6306646488308907, 'learning_rate': 3.8594694079454626e-05, 'epoch': 0.6843183552327223, 'step': 569500}
INFO:transformers.trainer:{'loss': 2.603067782521248, 'learning_rate': 3.858468064142079e-05, 'epoch': 0.6849191615147529, 'step': 570000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-570000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-570000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-570000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-550000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5853875426054, 'learning_rate': 3.857466720338695e-05, 'epoch': 0.6855199677967833, 'step': 570500}
INFO:transformers.trainer:{'loss': 2.6252293236255646, 'learning_rate': 3.8564653765353104e-05, 'epoch': 0.6861207740788138, 'step': 571000}
INFO:transformers.trainer:{'loss': 2.529459972500801, 'learning_rate': 3.855464032731926e-05, 'epoch': 0.6867215803608443, 'step': 571500}
INFO:transformers.trainer:{'loss': 2.577322460770607, 'learning_rate': 3.8544626889285425e-05, 'epoch': 0.6873223866428747, 'step': 572000}
INFO:transformers.trainer:{'loss': 2.6112527294158934, 'learning_rate': 3.853461345125158e-05, 'epoch': 0.6879231929249052, 'step': 572500}
INFO:transformers.trainer:{'loss': 2.631817836403847, 'learning_rate': 3.852460001321774e-05, 'epoch': 0.6885239992069357, 'step': 573000}
INFO:transformers.trainer:{'loss': 2.5601964037418368, 'learning_rate': 3.8514586575183897e-05, 'epoch': 0.6891248054889662, 'step': 573500}
INFO:transformers.trainer:{'loss': 2.5983910326957704, 'learning_rate': 3.850457313715006e-05, 'epoch': 0.6897256117709967, 'step': 574000}
INFO:transformers.trainer:{'loss': 2.628598260998726, 'learning_rate': 3.849455969911622e-05, 'epoch': 0.6903264180530272, 'step': 574500}
INFO:transformers.trainer:{'loss': 2.619878026843071, 'learning_rate': 3.8484546261082375e-05, 'epoch': 0.6909272243350576, 'step': 575000}
INFO:transformers.trainer:{'loss': 2.632600129008293, 'learning_rate': 3.847453282304853e-05, 'epoch': 0.6915280306170881, 'step': 575500}
INFO:transformers.trainer:{'loss': 2.5824401705265045, 'learning_rate': 3.846451938501469e-05, 'epoch': 0.6921288368991186, 'step': 576000}
INFO:transformers.trainer:{'loss': 2.56946210372448, 'learning_rate': 3.845450594698085e-05, 'epoch': 0.6927296431811492, 'step': 576500}
INFO:transformers.trainer:{'loss': 2.657415734887123, 'learning_rate': 3.844449250894701e-05, 'epoch': 0.6933304494631796, 'step': 577000}
INFO:transformers.trainer:{'loss': 2.6182377129793166, 'learning_rate': 3.843447907091317e-05, 'epoch': 0.6939312557452101, 'step': 577500}
INFO:transformers.trainer:{'loss': 2.5618964072465897, 'learning_rate': 3.8424465632879324e-05, 'epoch': 0.6945320620272406, 'step': 578000}
INFO:transformers.trainer:{'loss': 2.6085038666725158, 'learning_rate': 3.841445219484549e-05, 'epoch': 0.695132868309271, 'step': 578500}
INFO:transformers.trainer:{'loss': 2.5739009845256806, 'learning_rate': 3.8404438756811645e-05, 'epoch': 0.6957336745913015, 'step': 579000}
INFO:transformers.trainer:{'loss': 2.616542249917984, 'learning_rate': 3.83944253187778e-05, 'epoch': 0.696334480873332, 'step': 579500}
INFO:transformers.trainer:{'loss': 2.5646412336826323, 'learning_rate': 3.838441188074396e-05, 'epoch': 0.6969352871553625, 'step': 580000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-580000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-580000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-580000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-560000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.571402471423149, 'learning_rate': 3.8374398442710116e-05, 'epoch': 0.697536093437393, 'step': 580500}
INFO:transformers.trainer:{'loss': 2.5719380759000776, 'learning_rate': 3.836438500467628e-05, 'epoch': 0.6981368997194235, 'step': 581000}
INFO:transformers.trainer:{'loss': 2.5256309036016464, 'learning_rate': 3.835437156664244e-05, 'epoch': 0.6987377060014539, 'step': 581500}
INFO:transformers.trainer:{'loss': 2.61995626103878, 'learning_rate': 3.8344358128608594e-05, 'epoch': 0.6993385122834844, 'step': 582000}
INFO:transformers.trainer:{'loss': 2.605120104312897, 'learning_rate': 3.833434469057475e-05, 'epoch': 0.699939318565515, 'step': 582500}
INFO:transformers.trainer:{'loss': 2.6595176329612733, 'learning_rate': 3.8324331252540915e-05, 'epoch': 0.7005401248475454, 'step': 583000}
INFO:transformers.trainer:{'loss': 2.626892100453377, 'learning_rate': 3.831431781450707e-05, 'epoch': 0.7011409311295759, 'step': 583500}
INFO:transformers.trainer:{'loss': 2.5734516218900683, 'learning_rate': 3.830430437647323e-05, 'epoch': 0.7017417374116064, 'step': 584000}
INFO:transformers.trainer:{'loss': 2.6504845509529114, 'learning_rate': 3.8294290938439386e-05, 'epoch': 0.7023425436936369, 'step': 584500}
INFO:transformers.trainer:{'loss': 2.6068846505880354, 'learning_rate': 3.828427750040555e-05, 'epoch': 0.7029433499756673, 'step': 585000}
INFO:transformers.trainer:{'loss': 2.5923408917188646, 'learning_rate': 3.827426406237171e-05, 'epoch': 0.7035441562576978, 'step': 585500}
INFO:transformers.trainer:{'loss': 2.616865515947342, 'learning_rate': 3.8264250624337864e-05, 'epoch': 0.7041449625397284, 'step': 586000}
INFO:transformers.trainer:{'loss': 2.566264778852463, 'learning_rate': 3.825423718630402e-05, 'epoch': 0.7047457688217588, 'step': 586500}
INFO:transformers.trainer:{'loss': 2.591627427458763, 'learning_rate': 3.824422374827018e-05, 'epoch': 0.7053465751037893, 'step': 587000}
INFO:transformers.trainer:{'loss': 2.591670646071434, 'learning_rate': 3.823421031023634e-05, 'epoch': 0.7059473813858198, 'step': 587500}
INFO:transformers.trainer:{'loss': 2.5878447144031527, 'learning_rate': 3.82241968722025e-05, 'epoch': 0.7065481876678502, 'step': 588000}
INFO:transformers.trainer:{'loss': 2.5654216752052306, 'learning_rate': 3.8214183434168656e-05, 'epoch': 0.7071489939498807, 'step': 588500}
INFO:transformers.trainer:{'loss': 2.5867275280952455, 'learning_rate': 3.820416999613481e-05, 'epoch': 0.7077498002319113, 'step': 589000}
INFO:transformers.trainer:{'loss': 2.572860235452652, 'learning_rate': 3.819415655810098e-05, 'epoch': 0.7083506065139417, 'step': 589500}
INFO:transformers.trainer:{'loss': 2.6295997895002365, 'learning_rate': 3.8184143120067134e-05, 'epoch': 0.7089514127959722, 'step': 590000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-590000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-590000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-590000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-570000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5849558583498, 'learning_rate': 3.817412968203329e-05, 'epoch': 0.7095522190780027, 'step': 590500}
INFO:transformers.trainer:{'loss': 2.5624533364772795, 'learning_rate': 3.816411624399945e-05, 'epoch': 0.7101530253600332, 'step': 591000}
INFO:transformers.trainer:{'loss': 2.5950410343408583, 'learning_rate': 3.8154102805965605e-05, 'epoch': 0.7107538316420636, 'step': 591500}
INFO:transformers.trainer:{'loss': 2.5866793214082717, 'learning_rate': 3.814408936793177e-05, 'epoch': 0.7113546379240941, 'step': 592000}
INFO:transformers.trainer:{'loss': 2.5740260722637176, 'learning_rate': 3.813407592989792e-05, 'epoch': 0.7119554442061247, 'step': 592500}
INFO:transformers.trainer:{'loss': 2.601416225194931, 'learning_rate': 3.812406249186408e-05, 'epoch': 0.7125562504881551, 'step': 593000}
INFO:transformers.trainer:{'loss': 2.605693517923355, 'learning_rate': 3.811404905383024e-05, 'epoch': 0.7131570567701856, 'step': 593500}
INFO:transformers.trainer:{'loss': 2.581904179930687, 'learning_rate': 3.8104035615796404e-05, 'epoch': 0.7137578630522161, 'step': 594000}
INFO:transformers.trainer:{'loss': 2.5830151438713074, 'learning_rate': 3.809402217776256e-05, 'epoch': 0.7143586693342465, 'step': 594500}
INFO:transformers.trainer:{'loss': 2.630292200446129, 'learning_rate': 3.808400873972872e-05, 'epoch': 0.714959475616277, 'step': 595000}
INFO:transformers.trainer:{'loss': 2.5645012356042862, 'learning_rate': 3.8073995301694875e-05, 'epoch': 0.7155602818983076, 'step': 595500}
INFO:transformers.trainer:{'loss': 2.5452489869594572, 'learning_rate': 3.806398186366104e-05, 'epoch': 0.716161088180338, 'step': 596000}
INFO:transformers.trainer:{'loss': 2.5527738503217696, 'learning_rate': 3.8053968425627196e-05, 'epoch': 0.7167618944623685, 'step': 596500}
INFO:transformers.trainer:{'loss': 2.5884743317365646, 'learning_rate': 3.804395498759335e-05, 'epoch': 0.717362700744399, 'step': 597000}
INFO:transformers.trainer:{'loss': 2.5298777922391893, 'learning_rate': 3.803394154955951e-05, 'epoch': 0.7179635070264294, 'step': 597500}
INFO:transformers.trainer:{'loss': 2.561864624261856, 'learning_rate': 3.802392811152567e-05, 'epoch': 0.7185643133084599, 'step': 598000}
INFO:transformers.trainer:{'loss': 2.577720839500427, 'learning_rate': 3.801391467349183e-05, 'epoch': 0.7191651195904905, 'step': 598500}
INFO:transformers.trainer:{'loss': 2.580921368718147, 'learning_rate': 3.800390123545798e-05, 'epoch': 0.719765925872521, 'step': 599000}
INFO:transformers.trainer:{'loss': 2.5635400651693345, 'learning_rate': 3.7993887797424145e-05, 'epoch': 0.7203667321545514, 'step': 599500}
INFO:transformers.trainer:{'loss': 2.6239128425121305, 'learning_rate': 3.79838743593903e-05, 'epoch': 0.7209675384365819, 'step': 600000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-600000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-600000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-600000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-580000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.627613292694092, 'learning_rate': 3.7973860921356466e-05, 'epoch': 0.7215683447186124, 'step': 600500}
INFO:transformers.trainer:{'loss': 2.602509154319763, 'learning_rate': 3.796384748332262e-05, 'epoch': 0.7221691510006428, 'step': 601000}
INFO:transformers.trainer:{'loss': 2.5701800150871277, 'learning_rate': 3.795383404528878e-05, 'epoch': 0.7227699572826733, 'step': 601500}
INFO:transformers.trainer:{'loss': 2.6036132134199144, 'learning_rate': 3.794382060725494e-05, 'epoch': 0.7233707635647039, 'step': 602000}
INFO:transformers.trainer:{'loss': 2.6288865337371825, 'learning_rate': 3.7933807169221094e-05, 'epoch': 0.7239715698467343, 'step': 602500}
INFO:transformers.trainer:{'loss': 2.586368972659111, 'learning_rate': 3.792379373118726e-05, 'epoch': 0.7245723761287648, 'step': 603000}
INFO:transformers.trainer:{'loss': 2.6095035791397097, 'learning_rate': 3.791378029315341e-05, 'epoch': 0.7251731824107953, 'step': 603500}
INFO:transformers.trainer:{'loss': 2.5933454179763795, 'learning_rate': 3.790376685511957e-05, 'epoch': 0.7257739886928257, 'step': 604000}
INFO:transformers.trainer:{'loss': 2.607783162355423, 'learning_rate': 3.789375341708573e-05, 'epoch': 0.7263747949748562, 'step': 604500}
INFO:transformers.trainer:{'loss': 2.6446272258758543, 'learning_rate': 3.788373997905189e-05, 'epoch': 0.7269756012568868, 'step': 605000}
INFO:transformers.trainer:{'loss': 2.5781196793317793, 'learning_rate': 3.7873726541018044e-05, 'epoch': 0.7275764075389173, 'step': 605500}
INFO:transformers.trainer:{'loss': 2.6105142899751663, 'learning_rate': 3.786371310298421e-05, 'epoch': 0.7281772138209477, 'step': 606000}
INFO:transformers.trainer:{'loss': 2.589471817970276, 'learning_rate': 3.7853699664950364e-05, 'epoch': 0.7287780201029782, 'step': 606500}
INFO:transformers.trainer:{'loss': 2.5789141501188277, 'learning_rate': 3.784368622691652e-05, 'epoch': 0.7293788263850087, 'step': 607000}
INFO:transformers.trainer:{'loss': 2.5978920842409132, 'learning_rate': 3.7833672788882685e-05, 'epoch': 0.7299796326670391, 'step': 607500}
INFO:transformers.trainer:{'loss': 2.635148437023163, 'learning_rate': 3.782365935084884e-05, 'epoch': 0.7305804389490697, 'step': 608000}
INFO:transformers.trainer:{'loss': 2.5771083867549898, 'learning_rate': 3.7813645912815e-05, 'epoch': 0.7311812452311002, 'step': 608500}
INFO:transformers.trainer:{'loss': 2.5755729258060454, 'learning_rate': 3.7803632474781157e-05, 'epoch': 0.7317820515131306, 'step': 609000}
INFO:transformers.trainer:{'loss': 2.561147828221321, 'learning_rate': 3.779361903674732e-05, 'epoch': 0.7323828577951611, 'step': 609500}
INFO:transformers.trainer:{'loss': 2.5549138901233674, 'learning_rate': 3.778360559871347e-05, 'epoch': 0.7329836640771916, 'step': 610000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-610000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-610000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-610000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-590000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.573866205453873, 'learning_rate': 3.7773592160679635e-05, 'epoch': 0.733584470359222, 'step': 610500}
INFO:transformers.trainer:{'loss': 2.5971280915737154, 'learning_rate': 3.776357872264579e-05, 'epoch': 0.7341852766412525, 'step': 611000}
INFO:transformers.trainer:{'loss': 2.616441079735756, 'learning_rate': 3.7753565284611955e-05, 'epoch': 0.7347860829232831, 'step': 611500}
INFO:transformers.trainer:{'loss': 2.573519825100899, 'learning_rate': 3.774355184657811e-05, 'epoch': 0.7353868892053135, 'step': 612000}
INFO:transformers.trainer:{'loss': 2.6591416236162186, 'learning_rate': 3.773353840854427e-05, 'epoch': 0.735987695487344, 'step': 612500}
INFO:transformers.trainer:{'loss': 2.6102879400253296, 'learning_rate': 3.772352497051043e-05, 'epoch': 0.7365885017693745, 'step': 613000}
INFO:transformers.trainer:{'loss': 2.5503932687044144, 'learning_rate': 3.7713511532476584e-05, 'epoch': 0.737189308051405, 'step': 613500}
INFO:transformers.trainer:{'loss': 2.648512423992157, 'learning_rate': 3.770349809444275e-05, 'epoch': 0.7377901143334354, 'step': 614000}
INFO:transformers.trainer:{'loss': 2.558895367741585, 'learning_rate': 3.76934846564089e-05, 'epoch': 0.738390920615466, 'step': 614500}
INFO:transformers.trainer:{'loss': 2.563120026707649, 'learning_rate': 3.768347121837506e-05, 'epoch': 0.7389917268974965, 'step': 615000}
INFO:transformers.trainer:{'loss': 2.619664556026459, 'learning_rate': 3.767345778034122e-05, 'epoch': 0.7395925331795269, 'step': 615500}
INFO:transformers.trainer:{'loss': 2.6203049409389494, 'learning_rate': 3.766344434230738e-05, 'epoch': 0.7401933394615574, 'step': 616000}
INFO:transformers.trainer:{'loss': 2.5695916092395783, 'learning_rate': 3.765343090427353e-05, 'epoch': 0.7407941457435879, 'step': 616500}
INFO:transformers.trainer:{'loss': 2.628508779525757, 'learning_rate': 3.76434174662397e-05, 'epoch': 0.7413949520256183, 'step': 617000}
INFO:transformers.trainer:{'loss': 2.600793430566788, 'learning_rate': 3.7633404028205854e-05, 'epoch': 0.7419957583076489, 'step': 617500}
INFO:transformers.trainer:{'loss': 2.5832321672439575, 'learning_rate': 3.762339059017201e-05, 'epoch': 0.7425965645896794, 'step': 618000}
INFO:transformers.trainer:{'loss': 2.5914453971385956, 'learning_rate': 3.7613377152138175e-05, 'epoch': 0.7431973708717098, 'step': 618500}
INFO:transformers.trainer:{'loss': 2.5810463486909865, 'learning_rate': 3.7603363714104325e-05, 'epoch': 0.7437981771537403, 'step': 619000}
INFO:transformers.trainer:{'loss': 2.568501600980759, 'learning_rate': 3.759335027607049e-05, 'epoch': 0.7443989834357708, 'step': 619500}
INFO:transformers.trainer:{'loss': 2.590990342259407, 'learning_rate': 3.7583336838036646e-05, 'epoch': 0.7449997897178013, 'step': 620000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-620000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-620000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-620000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-600000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5663383959531783, 'learning_rate': 3.757332340000281e-05, 'epoch': 0.7456005959998318, 'step': 620500}
INFO:transformers.trainer:{'loss': 2.5298507660627365, 'learning_rate': 3.756330996196896e-05, 'epoch': 0.7462014022818623, 'step': 621000}
INFO:transformers.trainer:{'loss': 2.5996999342739584, 'learning_rate': 3.7553296523935124e-05, 'epoch': 0.7468022085638928, 'step': 621500}
INFO:transformers.trainer:{'loss': 2.5894206919670104, 'learning_rate': 3.754328308590128e-05, 'epoch': 0.7474030148459232, 'step': 622000}
INFO:transformers.trainer:{'loss': 2.5922955485582353, 'learning_rate': 3.7533269647867445e-05, 'epoch': 0.7480038211279537, 'step': 622500}
INFO:transformers.trainer:{'loss': 2.56339387267828, 'learning_rate': 3.7523256209833595e-05, 'epoch': 0.7486046274099842, 'step': 623000}
INFO:transformers.trainer:{'loss': 2.556989176869392, 'learning_rate': 3.751324277179976e-05, 'epoch': 0.7492054336920146, 'step': 623500}
INFO:transformers.trainer:{'loss': 2.6145298063755034, 'learning_rate': 3.7503229333765916e-05, 'epoch': 0.7498062399740452, 'step': 624000}
INFO:transformers.trainer:{'loss': 2.571330519199371, 'learning_rate': 3.749321589573207e-05, 'epoch': 0.7504070462560757, 'step': 624500}
INFO:transformers.trainer:{'loss': 2.587031095147133, 'learning_rate': 3.748320245769824e-05, 'epoch': 0.7510078525381061, 'step': 625000}
INFO:transformers.trainer:{'loss': 2.5656836799383163, 'learning_rate': 3.747318901966439e-05, 'epoch': 0.7516086588201366, 'step': 625500}
INFO:transformers.trainer:{'loss': 2.577454716920853, 'learning_rate': 3.746317558163055e-05, 'epoch': 0.7522094651021671, 'step': 626000}
INFO:transformers.trainer:{'loss': 2.5606657111644746, 'learning_rate': 3.745316214359671e-05, 'epoch': 0.7528102713841975, 'step': 626500}
INFO:transformers.trainer:{'loss': 2.577600988149643, 'learning_rate': 3.744314870556287e-05, 'epoch': 0.7534110776662281, 'step': 627000}
INFO:transformers.trainer:{'loss': 2.562095495939255, 'learning_rate': 3.743313526752902e-05, 'epoch': 0.7540118839482586, 'step': 627500}
INFO:transformers.trainer:{'loss': 2.609274871587753, 'learning_rate': 3.7423121829495186e-05, 'epoch': 0.7546126902302891, 'step': 628000}
INFO:transformers.trainer:{'loss': 2.6351746923923494, 'learning_rate': 3.741310839146134e-05, 'epoch': 0.7552134965123195, 'step': 628500}
INFO:transformers.trainer:{'loss': 2.6433607980012894, 'learning_rate': 3.74030949534275e-05, 'epoch': 0.75581430279435, 'step': 629000}
INFO:transformers.trainer:{'loss': 2.61986985039711, 'learning_rate': 3.739308151539366e-05, 'epoch': 0.7564151090763805, 'step': 629500}
INFO:transformers.trainer:{'loss': 2.5861643040180207, 'learning_rate': 3.7383068077359814e-05, 'epoch': 0.757015915358411, 'step': 630000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-630000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-630000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-630000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-610000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5891794043779375, 'learning_rate': 3.737305463932598e-05, 'epoch': 0.7576167216404415, 'step': 630500}
INFO:transformers.trainer:{'loss': 2.5995679887533187, 'learning_rate': 3.7363041201292135e-05, 'epoch': 0.758217527922472, 'step': 631000}
INFO:transformers.trainer:{'loss': 2.6241762821674346, 'learning_rate': 3.73530277632583e-05, 'epoch': 0.7588183342045024, 'step': 631500}
INFO:transformers.trainer:{'loss': 2.5923136126995088, 'learning_rate': 3.734301432522445e-05, 'epoch': 0.7594191404865329, 'step': 632000}
INFO:transformers.trainer:{'loss': 2.6311706013679506, 'learning_rate': 3.733300088719061e-05, 'epoch': 0.7600199467685634, 'step': 632500}
INFO:transformers.trainer:{'loss': 2.5947305715084075, 'learning_rate': 3.732298744915677e-05, 'epoch': 0.7606207530505938, 'step': 633000}
INFO:transformers.trainer:{'loss': 2.6002300897836683, 'learning_rate': 3.7312974011122934e-05, 'epoch': 0.7612215593326244, 'step': 633500}
INFO:transformers.trainer:{'loss': 2.617974682211876, 'learning_rate': 3.7302960573089084e-05, 'epoch': 0.7618223656146549, 'step': 634000}
INFO:transformers.trainer:{'loss': 2.5826831887960435, 'learning_rate': 3.729294713505525e-05, 'epoch': 0.7624231718966854, 'step': 634500}
INFO:transformers.trainer:{'loss': 2.6035946748256684, 'learning_rate': 3.7282933697021405e-05, 'epoch': 0.7630239781787158, 'step': 635000}
INFO:transformers.trainer:{'loss': 2.5855212049484253, 'learning_rate': 3.727292025898756e-05, 'epoch': 0.7636247844607463, 'step': 635500}
INFO:transformers.trainer:{'loss': 2.5579650499820707, 'learning_rate': 3.726290682095372e-05, 'epoch': 0.7642255907427769, 'step': 636000}
INFO:transformers.trainer:{'loss': 2.5831494069099428, 'learning_rate': 3.7252893382919876e-05, 'epoch': 0.7648263970248073, 'step': 636500}
INFO:transformers.trainer:{'loss': 2.6117865533828737, 'learning_rate': 3.724287994488604e-05, 'epoch': 0.7654272033068378, 'step': 637000}
INFO:transformers.trainer:{'loss': 2.5428461936712266, 'learning_rate': 3.72328665068522e-05, 'epoch': 0.7660280095888683, 'step': 637500}
INFO:transformers.trainer:{'loss': 2.538460335969925, 'learning_rate': 3.722285306881836e-05, 'epoch': 0.7666288158708987, 'step': 638000}
INFO:transformers.trainer:{'loss': 2.604157842516899, 'learning_rate': 3.721283963078451e-05, 'epoch': 0.7672296221529292, 'step': 638500}
INFO:transformers.trainer:{'loss': 2.59743302488327, 'learning_rate': 3.7202826192750675e-05, 'epoch': 0.7678304284349597, 'step': 639000}
INFO:transformers.trainer:{'loss': 2.591900038719177, 'learning_rate': 3.719281275471683e-05, 'epoch': 0.7684312347169902, 'step': 639500}
INFO:transformers.trainer:{'loss': 2.5556370698213575, 'learning_rate': 3.718279931668299e-05, 'epoch': 0.7690320409990207, 'step': 640000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-640000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-640000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-640000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-620000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.561499533176422, 'learning_rate': 3.7172785878649146e-05, 'epoch': 0.7696328472810512, 'step': 640500}
INFO:transformers.trainer:{'loss': 2.611083989262581, 'learning_rate': 3.7162772440615304e-05, 'epoch': 0.7702336535630817, 'step': 641000}
INFO:transformers.trainer:{'loss': 2.596268842220306, 'learning_rate': 3.715275900258147e-05, 'epoch': 0.7708344598451121, 'step': 641500}
INFO:transformers.trainer:{'loss': 2.608423453330994, 'learning_rate': 3.7142745564547624e-05, 'epoch': 0.7714352661271426, 'step': 642000}
INFO:transformers.trainer:{'loss': 2.621047684431076, 'learning_rate': 3.713273212651378e-05, 'epoch': 0.7720360724091732, 'step': 642500}
INFO:transformers.trainer:{'loss': 2.5663051273822783, 'learning_rate': 3.712271868847994e-05, 'epoch': 0.7726368786912036, 'step': 643000}
INFO:transformers.trainer:{'loss': 2.5964838889837263, 'learning_rate': 3.71127052504461e-05, 'epoch': 0.7732376849732341, 'step': 643500}
INFO:transformers.trainer:{'loss': 2.584807055473328, 'learning_rate': 3.710269181241226e-05, 'epoch': 0.7738384912552646, 'step': 644000}
INFO:transformers.trainer:{'loss': 2.5586686680316926, 'learning_rate': 3.709267837437842e-05, 'epoch': 0.774439297537295, 'step': 644500}
INFO:transformers.trainer:{'loss': 2.6323503005504607, 'learning_rate': 3.7082664936344574e-05, 'epoch': 0.7750401038193255, 'step': 645000}
INFO:transformers.trainer:{'loss': 2.555554825544357, 'learning_rate': 3.707265149831074e-05, 'epoch': 0.775640910101356, 'step': 645500}
INFO:transformers.trainer:{'loss': 2.630920160293579, 'learning_rate': 3.7062638060276894e-05, 'epoch': 0.7762417163833865, 'step': 646000}
INFO:transformers.trainer:{'loss': 2.524926278948784, 'learning_rate': 3.705262462224305e-05, 'epoch': 0.776842522665417, 'step': 646500}
INFO:transformers.trainer:{'loss': 2.572524199008942, 'learning_rate': 3.704261118420921e-05, 'epoch': 0.7774433289474475, 'step': 647000}
INFO:transformers.trainer:{'loss': 2.6290107429623606, 'learning_rate': 3.7032597746175366e-05, 'epoch': 0.7780441352294779, 'step': 647500}
INFO:transformers.trainer:{'loss': 2.550205613017082, 'learning_rate': 3.702258430814153e-05, 'epoch': 0.7786449415115084, 'step': 648000}
INFO:transformers.trainer:{'loss': 2.5558345423936846, 'learning_rate': 3.7012570870107687e-05, 'epoch': 0.779245747793539, 'step': 648500}
INFO:transformers.trainer:{'loss': 2.5440747060775757, 'learning_rate': 3.7002557432073844e-05, 'epoch': 0.7798465540755695, 'step': 649000}
INFO:transformers.trainer:{'loss': 2.587524970531464, 'learning_rate': 3.699254399404e-05, 'epoch': 0.7804473603575999, 'step': 649500}
INFO:transformers.trainer:{'loss': 2.589208471298218, 'learning_rate': 3.6982530556006165e-05, 'epoch': 0.7810481666396304, 'step': 650000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-650000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-650000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-650000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-630000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5637320263385774, 'learning_rate': 3.697251711797232e-05, 'epoch': 0.7816489729216609, 'step': 650500}
INFO:transformers.trainer:{'loss': 2.56193781709671, 'learning_rate': 3.696250367993848e-05, 'epoch': 0.7822497792036913, 'step': 651000}
INFO:transformers.trainer:{'loss': 2.5517028889656066, 'learning_rate': 3.6952490241904636e-05, 'epoch': 0.7828505854857218, 'step': 651500}
INFO:transformers.trainer:{'loss': 2.601509355187416, 'learning_rate': 3.694247680387079e-05, 'epoch': 0.7834513917677524, 'step': 652000}
INFO:transformers.trainer:{'loss': 2.5714500702619554, 'learning_rate': 3.693246336583696e-05, 'epoch': 0.7840521980497828, 'step': 652500}
INFO:transformers.trainer:{'loss': 2.5955284873247146, 'learning_rate': 3.6922449927803114e-05, 'epoch': 0.7846530043318133, 'step': 653000}
INFO:transformers.trainer:{'loss': 2.5959394710063934, 'learning_rate': 3.691243648976927e-05, 'epoch': 0.7852538106138438, 'step': 653500}
INFO:transformers.trainer:{'loss': 2.591906967759132, 'learning_rate': 3.690242305173543e-05, 'epoch': 0.7858546168958742, 'step': 654000}
INFO:transformers.trainer:{'loss': 2.5396429929733277, 'learning_rate': 3.689240961370159e-05, 'epoch': 0.7864554231779047, 'step': 654500}
INFO:transformers.trainer:{'loss': 2.587982249259949, 'learning_rate': 3.688239617566775e-05, 'epoch': 0.7870562294599353, 'step': 655000}
INFO:transformers.trainer:{'loss': 2.6224237339496614, 'learning_rate': 3.6872382737633906e-05, 'epoch': 0.7876570357419658, 'step': 655500}
INFO:transformers.trainer:{'loss': 2.5718433719277383, 'learning_rate': 3.686236929960006e-05, 'epoch': 0.7882578420239962, 'step': 656000}
INFO:transformers.trainer:{'loss': 2.609420306324959, 'learning_rate': 3.685235586156623e-05, 'epoch': 0.7888586483060267, 'step': 656500}
INFO:transformers.trainer:{'loss': 2.5804274110794068, 'learning_rate': 3.6842342423532384e-05, 'epoch': 0.7894594545880572, 'step': 657000}
INFO:transformers.trainer:{'loss': 2.5768340858221053, 'learning_rate': 3.683232898549854e-05, 'epoch': 0.7900602608700876, 'step': 657500}
INFO:transformers.trainer:{'loss': 2.606340106010437, 'learning_rate': 3.68223155474647e-05, 'epoch': 0.7906610671521181, 'step': 658000}
INFO:transformers.trainer:{'loss': 2.611785620689392, 'learning_rate': 3.6812302109430855e-05, 'epoch': 0.7912618734341487, 'step': 658500}
INFO:transformers.trainer:{'loss': 2.5890608972311018, 'learning_rate': 3.680228867139702e-05, 'epoch': 0.7918626797161791, 'step': 659000}
INFO:transformers.trainer:{'loss': 2.5792348057031633, 'learning_rate': 3.6792275233363176e-05, 'epoch': 0.7924634859982096, 'step': 659500}
INFO:transformers.trainer:{'loss': 2.5974353976249693, 'learning_rate': 3.678226179532933e-05, 'epoch': 0.7930642922802401, 'step': 660000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-660000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-660000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-660000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-640000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.574409218668938, 'learning_rate': 3.677224835729549e-05, 'epoch': 0.7936650985622705, 'step': 660500}
INFO:transformers.trainer:{'loss': 2.581791607141495, 'learning_rate': 3.6762234919261654e-05, 'epoch': 0.794265904844301, 'step': 661000}
INFO:transformers.trainer:{'loss': 2.5370252244472504, 'learning_rate': 3.675222148122781e-05, 'epoch': 0.7948667111263316, 'step': 661500}
INFO:transformers.trainer:{'loss': 2.6169115200042725, 'learning_rate': 3.674220804319397e-05, 'epoch': 0.795467517408362, 'step': 662000}
INFO:transformers.trainer:{'loss': 2.537453267335892, 'learning_rate': 3.6732194605160125e-05, 'epoch': 0.7960683236903925, 'step': 662500}
INFO:transformers.trainer:{'loss': 2.602527720093727, 'learning_rate': 3.672218116712628e-05, 'epoch': 0.796669129972423, 'step': 663000}
INFO:transformers.trainer:{'loss': 2.574740339756012, 'learning_rate': 3.6712167729092446e-05, 'epoch': 0.7972699362544535, 'step': 663500}
INFO:transformers.trainer:{'loss': 2.566725136578083, 'learning_rate': 3.67021542910586e-05, 'epoch': 0.7978707425364839, 'step': 664000}
INFO:transformers.trainer:{'loss': 2.5639908496141435, 'learning_rate': 3.669214085302476e-05, 'epoch': 0.7984715488185145, 'step': 664500}
INFO:transformers.trainer:{'loss': 2.5909661009311677, 'learning_rate': 3.668212741499092e-05, 'epoch': 0.799072355100545, 'step': 665000}
INFO:transformers.trainer:{'loss': 2.6104574517011643, 'learning_rate': 3.667211397695708e-05, 'epoch': 0.7996731613825754, 'step': 665500}
INFO:transformers.trainer:{'loss': 2.612695545911789, 'learning_rate': 3.666210053892324e-05, 'epoch': 0.8002739676646059, 'step': 666000}
INFO:transformers.trainer:{'loss': 2.6071071306467055, 'learning_rate': 3.6652087100889395e-05, 'epoch': 0.8008747739466364, 'step': 666500}
INFO:transformers.trainer:{'loss': 2.6223783893585204, 'learning_rate': 3.664207366285555e-05, 'epoch': 0.8014755802286668, 'step': 667000}
INFO:transformers.trainer:{'loss': 2.5861781480312347, 'learning_rate': 3.663206022482171e-05, 'epoch': 0.8020763865106973, 'step': 667500}
INFO:transformers.trainer:{'loss': 2.588278369307518, 'learning_rate': 3.662204678678787e-05, 'epoch': 0.8026771927927279, 'step': 668000}
INFO:transformers.trainer:{'loss': 2.577591450214386, 'learning_rate': 3.661203334875403e-05, 'epoch': 0.8032779990747583, 'step': 668500}
INFO:transformers.trainer:{'loss': 2.5739493190050124, 'learning_rate': 3.660201991072019e-05, 'epoch': 0.8038788053567888, 'step': 669000}
INFO:transformers.trainer:{'loss': 2.5491736624240877, 'learning_rate': 3.6592006472686344e-05, 'epoch': 0.8044796116388193, 'step': 669500}
INFO:transformers.trainer:{'loss': 2.5301418974399565, 'learning_rate': 3.658199303465251e-05, 'epoch': 0.8050804179208498, 'step': 670000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-670000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-670000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-670000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-650000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5635981101989747, 'learning_rate': 3.6571979596618665e-05, 'epoch': 0.8056812242028802, 'step': 670500}
INFO:transformers.trainer:{'loss': 2.5826271653175352, 'learning_rate': 3.656196615858482e-05, 'epoch': 0.8062820304849108, 'step': 671000}
INFO:transformers.trainer:{'loss': 2.5911864409446714, 'learning_rate': 3.655195272055098e-05, 'epoch': 0.8068828367669413, 'step': 671500}
INFO:transformers.trainer:{'loss': 2.5683523136377335, 'learning_rate': 3.654193928251714e-05, 'epoch': 0.8074836430489717, 'step': 672000}
INFO:transformers.trainer:{'loss': 2.5676546186208724, 'learning_rate': 3.65319258444833e-05, 'epoch': 0.8080844493310022, 'step': 672500}
INFO:transformers.trainer:{'loss': 2.610747414469719, 'learning_rate': 3.652191240644946e-05, 'epoch': 0.8086852556130327, 'step': 673000}
INFO:transformers.trainer:{'loss': 2.5659564678668976, 'learning_rate': 3.6511898968415614e-05, 'epoch': 0.8092860618950631, 'step': 673500}
INFO:transformers.trainer:{'loss': 2.60375588619709, 'learning_rate': 3.650188553038177e-05, 'epoch': 0.8098868681770937, 'step': 674000}
INFO:transformers.trainer:{'loss': 2.610428874492645, 'learning_rate': 3.6491872092347935e-05, 'epoch': 0.8104876744591242, 'step': 674500}
INFO:transformers.trainer:{'loss': 2.543764132618904, 'learning_rate': 3.648185865431409e-05, 'epoch': 0.8110884807411546, 'step': 675000}
INFO:transformers.trainer:{'loss': 2.572577073931694, 'learning_rate': 3.647184521628025e-05, 'epoch': 0.8116892870231851, 'step': 675500}
INFO:transformers.trainer:{'loss': 2.6298628348112105, 'learning_rate': 3.6461831778246406e-05, 'epoch': 0.8122900933052156, 'step': 676000}
INFO:transformers.trainer:{'loss': 2.619736592769623, 'learning_rate': 3.645181834021257e-05, 'epoch': 0.812890899587246, 'step': 676500}
INFO:transformers.trainer:{'loss': 2.56702627825737, 'learning_rate': 3.644180490217873e-05, 'epoch': 0.8134917058692765, 'step': 677000}
INFO:transformers.trainer:{'loss': 2.5676241837739946, 'learning_rate': 3.6431791464144884e-05, 'epoch': 0.8140925121513071, 'step': 677500}
INFO:transformers.trainer:{'loss': 2.6090093166828154, 'learning_rate': 3.642177802611104e-05, 'epoch': 0.8146933184333376, 'step': 678000}
INFO:transformers.trainer:{'loss': 2.5766097922325133, 'learning_rate': 3.64117645880772e-05, 'epoch': 0.815294124715368, 'step': 678500}
INFO:transformers.trainer:{'loss': 2.6094265031814574, 'learning_rate': 3.640175115004336e-05, 'epoch': 0.8158949309973985, 'step': 679000}
INFO:transformers.trainer:{'loss': 2.612827983260155, 'learning_rate': 3.639173771200951e-05, 'epoch': 0.816495737279429, 'step': 679500}
INFO:transformers.trainer:{'loss': 2.589603873729706, 'learning_rate': 3.6381724273975677e-05, 'epoch': 0.8170965435614594, 'step': 680000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-680000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-680000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-680000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-660000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5625799190998078, 'learning_rate': 3.6371710835941834e-05, 'epoch': 0.81769734984349, 'step': 680500}
INFO:transformers.trainer:{'loss': 2.596727520465851, 'learning_rate': 3.6361697397908e-05, 'epoch': 0.8182981561255205, 'step': 681000}
INFO:transformers.trainer:{'loss': 2.576245091795921, 'learning_rate': 3.6351683959874154e-05, 'epoch': 0.8188989624075509, 'step': 681500}
INFO:transformers.trainer:{'loss': 2.5599913696050645, 'learning_rate': 3.634167052184031e-05, 'epoch': 0.8194997686895814, 'step': 682000}
INFO:transformers.trainer:{'loss': 2.544823693871498, 'learning_rate': 3.633165708380647e-05, 'epoch': 0.8201005749716119, 'step': 682500}
INFO:transformers.trainer:{'loss': 2.6067903881073, 'learning_rate': 3.632164364577263e-05, 'epoch': 0.8207013812536423, 'step': 683000}
INFO:transformers.trainer:{'loss': 2.6105687139034273, 'learning_rate': 3.631163020773879e-05, 'epoch': 0.8213021875356729, 'step': 683500}
INFO:transformers.trainer:{'loss': 2.597642817378044, 'learning_rate': 3.6301616769704947e-05, 'epoch': 0.8219029938177034, 'step': 684000}
INFO:transformers.trainer:{'loss': 2.5853535776138306, 'learning_rate': 3.6291603331671104e-05, 'epoch': 0.8225038000997339, 'step': 684500}
INFO:transformers.trainer:{'loss': 2.5927857097387315, 'learning_rate': 3.628158989363726e-05, 'epoch': 0.8231046063817643, 'step': 685000}
INFO:transformers.trainer:{'loss': 2.6242956429719926, 'learning_rate': 3.6271576455603425e-05, 'epoch': 0.8237054126637948, 'step': 685500}
INFO:transformers.trainer:{'loss': 2.54680526804924, 'learning_rate': 3.6261563017569575e-05, 'epoch': 0.8243062189458253, 'step': 686000}
INFO:transformers.trainer:{'loss': 2.5105492084026335, 'learning_rate': 3.625154957953574e-05, 'epoch': 0.8249070252278558, 'step': 686500}
INFO:transformers.trainer:{'loss': 2.5563634116649627, 'learning_rate': 3.6241536141501896e-05, 'epoch': 0.8255078315098863, 'step': 687000}
INFO:transformers.trainer:{'loss': 2.523581239104271, 'learning_rate': 3.623152270346806e-05, 'epoch': 0.8261086377919168, 'step': 687500}
INFO:transformers.trainer:{'loss': 2.558254212260246, 'learning_rate': 3.622150926543422e-05, 'epoch': 0.8267094440739472, 'step': 688000}
INFO:transformers.trainer:{'loss': 2.5657170922756194, 'learning_rate': 3.6211495827400374e-05, 'epoch': 0.8273102503559777, 'step': 688500}
INFO:transformers.trainer:{'loss': 2.5533632884025574, 'learning_rate': 3.620148238936653e-05, 'epoch': 0.8279110566380082, 'step': 689000}
INFO:transformers.trainer:{'loss': 2.598513654589653, 'learning_rate': 3.619146895133269e-05, 'epoch': 0.8285118629200386, 'step': 689500}
INFO:transformers.trainer:{'loss': 2.614591625213623, 'learning_rate': 3.618145551329885e-05, 'epoch': 0.8291126692020692, 'step': 690000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-690000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-690000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-690000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-670000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.567343023777008, 'learning_rate': 3.6171442075265e-05, 'epoch': 0.8297134754840997, 'step': 690500}
INFO:transformers.trainer:{'loss': 2.5873518592119216, 'learning_rate': 3.6161428637231166e-05, 'epoch': 0.8303142817661301, 'step': 691000}
INFO:transformers.trainer:{'loss': 2.585756368160248, 'learning_rate': 3.615141519919732e-05, 'epoch': 0.8309150880481606, 'step': 691500}
INFO:transformers.trainer:{'loss': 2.639484415650368, 'learning_rate': 3.614140176116349e-05, 'epoch': 0.8315158943301911, 'step': 692000}
INFO:transformers.trainer:{'loss': 2.6059767789840698, 'learning_rate': 3.613138832312964e-05, 'epoch': 0.8321167006122216, 'step': 692500}
INFO:transformers.trainer:{'loss': 2.6520102194547652, 'learning_rate': 3.61213748850958e-05, 'epoch': 0.8327175068942521, 'step': 693000}
INFO:transformers.trainer:{'loss': 2.6068315612077715, 'learning_rate': 3.611136144706196e-05, 'epoch': 0.8333183131762826, 'step': 693500}
INFO:transformers.trainer:{'loss': 2.6171526374816896, 'learning_rate': 3.610134800902812e-05, 'epoch': 0.8339191194583131, 'step': 694000}
INFO:transformers.trainer:{'loss': 2.5414525060653688, 'learning_rate': 3.609133457099428e-05, 'epoch': 0.8345199257403435, 'step': 694500}
INFO:transformers.trainer:{'loss': 2.5926297949552537, 'learning_rate': 3.6081321132960436e-05, 'epoch': 0.835120732022374, 'step': 695000}
INFO:transformers.trainer:{'loss': 2.596996531486511, 'learning_rate': 3.607130769492659e-05, 'epoch': 0.8357215383044045, 'step': 695500}
INFO:transformers.trainer:{'loss': 2.6071387096643446, 'learning_rate': 3.606129425689275e-05, 'epoch': 0.836322344586435, 'step': 696000}
INFO:transformers.trainer:{'loss': 2.5576655972003937, 'learning_rate': 3.6051280818858914e-05, 'epoch': 0.8369231508684655, 'step': 696500}
INFO:transformers.trainer:{'loss': 2.5836492776870728, 'learning_rate': 3.6041267380825064e-05, 'epoch': 0.837523957150496, 'step': 697000}
INFO:transformers.trainer:{'loss': 2.5957857583761217, 'learning_rate': 3.603125394279123e-05, 'epoch': 0.8381247634325264, 'step': 697500}
INFO:transformers.trainer:{'loss': 2.5700267407894133, 'learning_rate': 3.6021240504757385e-05, 'epoch': 0.8387255697145569, 'step': 698000}
INFO:transformers.trainer:{'loss': 2.521349422097206, 'learning_rate': 3.601122706672355e-05, 'epoch': 0.8393263759965874, 'step': 698500}
INFO:transformers.trainer:{'loss': 2.5489189187288286, 'learning_rate': 3.60012136286897e-05, 'epoch': 0.839927182278618, 'step': 699000}
INFO:transformers.trainer:{'loss': 2.5747579290866853, 'learning_rate': 3.599120019065586e-05, 'epoch': 0.8405279885606484, 'step': 699500}
INFO:transformers.trainer:{'loss': 2.5606446367502214, 'learning_rate': 3.598118675262202e-05, 'epoch': 0.8411287948426789, 'step': 700000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-700000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-700000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-700000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-680000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.589052765130997, 'learning_rate': 3.597117331458818e-05, 'epoch': 0.8417296011247094, 'step': 700500}
INFO:transformers.trainer:{'loss': 2.5840901424884795, 'learning_rate': 3.596115987655434e-05, 'epoch': 0.8423304074067398, 'step': 701000}
INFO:transformers.trainer:{'loss': 2.57998871922493, 'learning_rate': 3.595114643852049e-05, 'epoch': 0.8429312136887703, 'step': 701500}
INFO:transformers.trainer:{'loss': 2.538004885554314, 'learning_rate': 3.5941133000486655e-05, 'epoch': 0.8435320199708009, 'step': 702000}
INFO:transformers.trainer:{'loss': 2.594377959012985, 'learning_rate': 3.593111956245281e-05, 'epoch': 0.8441328262528313, 'step': 702500}
INFO:transformers.trainer:{'loss': 2.59152097928524, 'learning_rate': 3.5921106124418976e-05, 'epoch': 0.8447336325348618, 'step': 703000}
INFO:transformers.trainer:{'loss': 2.614366912126541, 'learning_rate': 3.5911092686385126e-05, 'epoch': 0.8453344388168923, 'step': 703500}
INFO:transformers.trainer:{'loss': 2.6116215074062348, 'learning_rate': 3.590107924835129e-05, 'epoch': 0.8459352450989227, 'step': 704000}
INFO:transformers.trainer:{'loss': 2.591540277123451, 'learning_rate': 3.589106581031745e-05, 'epoch': 0.8465360513809532, 'step': 704500}
INFO:transformers.trainer:{'loss': 2.633079684138298, 'learning_rate': 3.588105237228361e-05, 'epoch': 0.8471368576629837, 'step': 705000}
INFO:transformers.trainer:{'loss': 2.570826460123062, 'learning_rate': 3.587103893424976e-05, 'epoch': 0.8477376639450143, 'step': 705500}
INFO:transformers.trainer:{'loss': 2.568072945356369, 'learning_rate': 3.5861025496215925e-05, 'epoch': 0.8483384702270447, 'step': 706000}
INFO:transformers.trainer:{'loss': 2.5499989256858826, 'learning_rate': 3.585101205818208e-05, 'epoch': 0.8489392765090752, 'step': 706500}
INFO:transformers.trainer:{'loss': 2.5460237139463424, 'learning_rate': 3.584099862014824e-05, 'epoch': 0.8495400827911057, 'step': 707000}
INFO:transformers.trainer:{'loss': 2.5716441378593444, 'learning_rate': 3.58309851821144e-05, 'epoch': 0.8501408890731361, 'step': 707500}
INFO:transformers.trainer:{'loss': 2.5841530224084854, 'learning_rate': 3.5820971744080553e-05, 'epoch': 0.8507416953551666, 'step': 708000}
INFO:transformers.trainer:{'loss': 2.558806048870087, 'learning_rate': 3.581095830604672e-05, 'epoch': 0.8513425016371972, 'step': 708500}
INFO:transformers.trainer:{'loss': 2.534769911289215, 'learning_rate': 3.5800944868012874e-05, 'epoch': 0.8519433079192276, 'step': 709000}
INFO:transformers.trainer:{'loss': 2.531351732611656, 'learning_rate': 3.579093142997904e-05, 'epoch': 0.8525441142012581, 'step': 709500}
INFO:transformers.trainer:{'loss': 2.6060145102739334, 'learning_rate': 3.578091799194519e-05, 'epoch': 0.8531449204832886, 'step': 710000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-710000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-710000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-710000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-690000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5064693896770476, 'learning_rate': 3.577090455391135e-05, 'epoch': 0.853745726765319, 'step': 710500}
INFO:transformers.trainer:{'loss': 2.603512902736664, 'learning_rate': 3.576089111587751e-05, 'epoch': 0.8543465330473495, 'step': 711000}
INFO:transformers.trainer:{'loss': 2.5449429055452346, 'learning_rate': 3.5750877677843666e-05, 'epoch': 0.85494733932938, 'step': 711500}
INFO:transformers.trainer:{'loss': 2.611927545785904, 'learning_rate': 3.574086423980983e-05, 'epoch': 0.8555481456114105, 'step': 712000}
INFO:transformers.trainer:{'loss': 2.5352166023254394, 'learning_rate': 3.573085080177598e-05, 'epoch': 0.856148951893441, 'step': 712500}
INFO:transformers.trainer:{'loss': 2.5427030626535414, 'learning_rate': 3.5720837363742144e-05, 'epoch': 0.8567497581754715, 'step': 713000}
INFO:transformers.trainer:{'loss': 2.569243135750294, 'learning_rate': 3.57108239257083e-05, 'epoch': 0.857350564457502, 'step': 713500}
INFO:transformers.trainer:{'loss': 2.619420855998993, 'learning_rate': 3.5700810487674465e-05, 'epoch': 0.8579513707395324, 'step': 714000}
INFO:transformers.trainer:{'loss': 2.601712601184845, 'learning_rate': 3.5690797049640616e-05, 'epoch': 0.858552177021563, 'step': 714500}
INFO:transformers.trainer:{'loss': 2.572121569275856, 'learning_rate': 3.568078361160678e-05, 'epoch': 0.8591529833035935, 'step': 715000}
INFO:transformers.trainer:{'loss': 2.5675404362678527, 'learning_rate': 3.5670770173572936e-05, 'epoch': 0.8597537895856239, 'step': 715500}
INFO:transformers.trainer:{'loss': 2.5764020985364913, 'learning_rate': 3.56607567355391e-05, 'epoch': 0.8603545958676544, 'step': 716000}
INFO:transformers.trainer:{'loss': 2.5311288626194, 'learning_rate': 3.565074329750525e-05, 'epoch': 0.8609554021496849, 'step': 716500}
INFO:transformers.trainer:{'loss': 2.565466819882393, 'learning_rate': 3.5640729859471414e-05, 'epoch': 0.8615562084317153, 'step': 717000}
INFO:transformers.trainer:{'loss': 2.5656516933441162, 'learning_rate': 3.563071642143757e-05, 'epoch': 0.8621570147137458, 'step': 717500}
INFO:transformers.trainer:{'loss': 2.5180504636764525, 'learning_rate': 3.562070298340373e-05, 'epoch': 0.8627578209957764, 'step': 718000}
INFO:transformers.trainer:{'loss': 2.553351930618286, 'learning_rate': 3.561068954536989e-05, 'epoch': 0.8633586272778068, 'step': 718500}
INFO:transformers.trainer:{'loss': 2.5675370229482652, 'learning_rate': 3.560067610733604e-05, 'epoch': 0.8639594335598373, 'step': 719000}
INFO:transformers.trainer:{'loss': 2.5410312885046005, 'learning_rate': 3.5590662669302207e-05, 'epoch': 0.8645602398418678, 'step': 719500}
INFO:transformers.trainer:{'loss': 2.5171155638694764, 'learning_rate': 3.5580649231268364e-05, 'epoch': 0.8651610461238983, 'step': 720000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-720000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-720000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-720000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-700000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.6283181043863295, 'learning_rate': 3.557063579323453e-05, 'epoch': 0.8657618524059287, 'step': 720500}
INFO:transformers.trainer:{'loss': 2.580490510702133, 'learning_rate': 3.556062235520068e-05, 'epoch': 0.8663626586879593, 'step': 721000}
INFO:transformers.trainer:{'loss': 2.5562188271284105, 'learning_rate': 3.555060891716684e-05, 'epoch': 0.8669634649699898, 'step': 721500}
INFO:transformers.trainer:{'loss': 2.57594511783123, 'learning_rate': 3.5540595479133e-05, 'epoch': 0.8675642712520202, 'step': 722000}
INFO:transformers.trainer:{'loss': 2.5943040878772736, 'learning_rate': 3.5530582041099156e-05, 'epoch': 0.8681650775340507, 'step': 722500}
INFO:transformers.trainer:{'loss': 2.5466555720567703, 'learning_rate': 3.552056860306531e-05, 'epoch': 0.8687658838160812, 'step': 723000}
INFO:transformers.trainer:{'loss': 2.6065495414733886, 'learning_rate': 3.551055516503147e-05, 'epoch': 0.8693666900981116, 'step': 723500}
INFO:transformers.trainer:{'loss': 2.5793559650182725, 'learning_rate': 3.5500541726997634e-05, 'epoch': 0.8699674963801421, 'step': 724000}
INFO:transformers.trainer:{'loss': 2.540555857539177, 'learning_rate': 3.549052828896379e-05, 'epoch': 0.8705683026621727, 'step': 724500}
INFO:transformers.trainer:{'loss': 2.553462774038315, 'learning_rate': 3.5480514850929955e-05, 'epoch': 0.8711691089442031, 'step': 725000}
INFO:transformers.trainer:{'loss': 2.6214952083826066, 'learning_rate': 3.5470501412896105e-05, 'epoch': 0.8717699152262336, 'step': 725500}
INFO:transformers.trainer:{'loss': 2.534063962697983, 'learning_rate': 3.546048797486227e-05, 'epoch': 0.8723707215082641, 'step': 726000}
INFO:transformers.trainer:{'loss': 2.612621521472931, 'learning_rate': 3.5450474536828426e-05, 'epoch': 0.8729715277902945, 'step': 726500}
INFO:transformers.trainer:{'loss': 2.570910661458969, 'learning_rate': 3.544046109879458e-05, 'epoch': 0.873572334072325, 'step': 727000}
INFO:transformers.trainer:{'loss': 2.543604198217392, 'learning_rate': 3.543044766076074e-05, 'epoch': 0.8741731403543556, 'step': 727500}
INFO:transformers.trainer:{'loss': 2.5879577833414076, 'learning_rate': 3.5420434222726904e-05, 'epoch': 0.8747739466363861, 'step': 728000}
INFO:transformers.trainer:{'loss': 2.567291482448578, 'learning_rate': 3.541042078469306e-05, 'epoch': 0.8753747529184165, 'step': 728500}
INFO:transformers.trainer:{'loss': 2.582131085515022, 'learning_rate': 3.540040734665922e-05, 'epoch': 0.875975559200447, 'step': 729000}
INFO:transformers.trainer:{'loss': 2.607931814432144, 'learning_rate': 3.5390393908625375e-05, 'epoch': 0.8765763654824775, 'step': 729500}
INFO:transformers.trainer:{'loss': 2.5560187046527862, 'learning_rate': 3.538038047059153e-05, 'epoch': 0.8771771717645079, 'step': 730000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-730000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-730000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-730000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-710000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.523492172241211, 'learning_rate': 3.5370367032557696e-05, 'epoch': 0.8777779780465385, 'step': 730500}
INFO:transformers.trainer:{'loss': 2.530008217930794, 'learning_rate': 3.536035359452385e-05, 'epoch': 0.878378784328569, 'step': 731000}
INFO:transformers.trainer:{'loss': 2.544342823147774, 'learning_rate': 3.535034015649002e-05, 'epoch': 0.8789795906105994, 'step': 731500}
INFO:transformers.trainer:{'loss': 2.5208800848722457, 'learning_rate': 3.534032671845617e-05, 'epoch': 0.8795803968926299, 'step': 732000}
INFO:transformers.trainer:{'loss': 2.5762772011756896, 'learning_rate': 3.533031328042233e-05, 'epoch': 0.8801812031746604, 'step': 732500}
INFO:transformers.trainer:{'loss': 2.6140788156986234, 'learning_rate': 3.532029984238849e-05, 'epoch': 0.8807820094566908, 'step': 733000}
INFO:transformers.trainer:{'loss': 2.53446244597435, 'learning_rate': 3.5310286404354645e-05, 'epoch': 0.8813828157387213, 'step': 733500}
INFO:transformers.trainer:{'loss': 2.6020059196949004, 'learning_rate': 3.53002729663208e-05, 'epoch': 0.8819836220207519, 'step': 734000}
INFO:transformers.trainer:{'loss': 2.5818872373104096, 'learning_rate': 3.529025952828696e-05, 'epoch': 0.8825844283027824, 'step': 734500}
INFO:transformers.trainer:{'loss': 2.562016368865967, 'learning_rate': 3.528024609025312e-05, 'epoch': 0.8831852345848128, 'step': 735000}
INFO:transformers.trainer:{'loss': 2.5703612520694734, 'learning_rate': 3.527023265221928e-05, 'epoch': 0.8837860408668433, 'step': 735500}
INFO:transformers.trainer:{'loss': 2.6092361633777617, 'learning_rate': 3.526021921418544e-05, 'epoch': 0.8843868471488738, 'step': 736000}
INFO:transformers.trainer:{'loss': 2.552230876803398, 'learning_rate': 3.5250205776151594e-05, 'epoch': 0.8849876534309042, 'step': 736500}
INFO:transformers.trainer:{'loss': 2.576265026807785, 'learning_rate': 3.524019233811776e-05, 'epoch': 0.8855884597129348, 'step': 737000}
INFO:transformers.trainer:{'loss': 2.5632607116699218, 'learning_rate': 3.5230178900083915e-05, 'epoch': 0.8861892659949653, 'step': 737500}
INFO:transformers.trainer:{'loss': 2.562951614499092, 'learning_rate': 3.522016546205007e-05, 'epoch': 0.8867900722769957, 'step': 738000}
INFO:transformers.trainer:{'loss': 2.5172340482473374, 'learning_rate': 3.521015202401623e-05, 'epoch': 0.8873908785590262, 'step': 738500}
INFO:transformers.trainer:{'loss': 2.5477140588760374, 'learning_rate': 3.5200138585982386e-05, 'epoch': 0.8879916848410567, 'step': 739000}
INFO:transformers.trainer:{'loss': 2.522533926010132, 'learning_rate': 3.519012514794855e-05, 'epoch': 0.8885924911230871, 'step': 739500}
INFO:transformers.trainer:{'loss': 2.6067478951215746, 'learning_rate': 3.518011170991471e-05, 'epoch': 0.8891932974051177, 'step': 740000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-740000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-740000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-740000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-720000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.554769527196884, 'learning_rate': 3.5170098271880864e-05, 'epoch': 0.8897941036871482, 'step': 740500}
INFO:transformers.trainer:{'loss': 2.577286253809929, 'learning_rate': 3.516008483384702e-05, 'epoch': 0.8903949099691786, 'step': 741000}
INFO:transformers.trainer:{'loss': 2.568482878565788, 'learning_rate': 3.5150071395813185e-05, 'epoch': 0.8909957162512091, 'step': 741500}
INFO:transformers.trainer:{'loss': 2.5667688841819762, 'learning_rate': 3.514005795777934e-05, 'epoch': 0.8915965225332396, 'step': 742000}
INFO:transformers.trainer:{'loss': 2.6032978525161745, 'learning_rate': 3.51300445197455e-05, 'epoch': 0.8921973288152701, 'step': 742500}
INFO:transformers.trainer:{'loss': 2.543323821306229, 'learning_rate': 3.5120031081711656e-05, 'epoch': 0.8927981350973005, 'step': 743000}
INFO:transformers.trainer:{'loss': 2.587976731002331, 'learning_rate': 3.511001764367782e-05, 'epoch': 0.8933989413793311, 'step': 743500}
INFO:transformers.trainer:{'loss': 2.5264721031188966, 'learning_rate': 3.510000420564398e-05, 'epoch': 0.8939997476613616, 'step': 744000}
INFO:transformers.trainer:{'loss': 2.5569318054914474, 'learning_rate': 3.5089990767610134e-05, 'epoch': 0.894600553943392, 'step': 744500}
INFO:transformers.trainer:{'loss': 2.531701351046562, 'learning_rate': 3.507997732957629e-05, 'epoch': 0.8952013602254225, 'step': 745000}
INFO:transformers.trainer:{'loss': 2.570747043967247, 'learning_rate': 3.506996389154245e-05, 'epoch': 0.895802166507453, 'step': 745500}
INFO:transformers.trainer:{'loss': 2.591503792047501, 'learning_rate': 3.505995045350861e-05, 'epoch': 0.8964029727894834, 'step': 746000}
INFO:transformers.trainer:{'loss': 2.5761228182315827, 'learning_rate': 3.504993701547477e-05, 'epoch': 0.897003779071514, 'step': 746500}
INFO:transformers.trainer:{'loss': 2.544545882344246, 'learning_rate': 3.5039923577440926e-05, 'epoch': 0.8976045853535445, 'step': 747000}
INFO:transformers.trainer:{'loss': 2.5914932515621185, 'learning_rate': 3.5029910139407083e-05, 'epoch': 0.8982053916355749, 'step': 747500}
INFO:transformers.trainer:{'loss': 2.565274506211281, 'learning_rate': 3.501989670137325e-05, 'epoch': 0.8988061979176054, 'step': 748000}
INFO:transformers.trainer:{'loss': 2.6288141885995864, 'learning_rate': 3.5009883263339404e-05, 'epoch': 0.8994070041996359, 'step': 748500}
INFO:transformers.trainer:{'loss': 2.5569768917560576, 'learning_rate': 3.499986982530556e-05, 'epoch': 0.9000078104816664, 'step': 749000}
INFO:transformers.trainer:{'loss': 2.5759724566936493, 'learning_rate': 3.498985638727172e-05, 'epoch': 0.9006086167636969, 'step': 749500}
INFO:transformers.trainer:{'loss': 2.6097025032043457, 'learning_rate': 3.4979842949237876e-05, 'epoch': 0.9012094230457274, 'step': 750000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-750000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-750000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-750000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-730000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.587343899130821, 'learning_rate': 3.496982951120404e-05, 'epoch': 0.9018102293277579, 'step': 750500}
INFO:transformers.trainer:{'loss': 2.5180298092365265, 'learning_rate': 3.4959816073170196e-05, 'epoch': 0.9024110356097883, 'step': 751000}
INFO:transformers.trainer:{'loss': 2.5584065091609953, 'learning_rate': 3.4949802635136354e-05, 'epoch': 0.9030118418918188, 'step': 751500}
INFO:transformers.trainer:{'loss': 2.5516392186284067, 'learning_rate': 3.493978919710251e-05, 'epoch': 0.9036126481738493, 'step': 752000}
INFO:transformers.trainer:{'loss': 2.552192741036415, 'learning_rate': 3.4929775759068674e-05, 'epoch': 0.9042134544558798, 'step': 752500}
INFO:transformers.trainer:{'loss': 2.571284204363823, 'learning_rate': 3.491976232103483e-05, 'epoch': 0.9048142607379103, 'step': 753000}
INFO:transformers.trainer:{'loss': 2.592481996655464, 'learning_rate': 3.490974888300099e-05, 'epoch': 0.9054150670199408, 'step': 753500}
INFO:transformers.trainer:{'loss': 2.528771869659424, 'learning_rate': 3.4899735444967146e-05, 'epoch': 0.9060158733019712, 'step': 754000}
INFO:transformers.trainer:{'loss': 2.6052811207771303, 'learning_rate': 3.488972200693331e-05, 'epoch': 0.9066166795840017, 'step': 754500}
INFO:transformers.trainer:{'loss': 2.553765425205231, 'learning_rate': 3.4879708568899467e-05, 'epoch': 0.9072174858660322, 'step': 755000}
INFO:transformers.trainer:{'loss': 2.522620902657509, 'learning_rate': 3.4869695130865624e-05, 'epoch': 0.9078182921480628, 'step': 755500}
INFO:transformers.trainer:{'loss': 2.492978175520897, 'learning_rate': 3.485968169283178e-05, 'epoch': 0.9084190984300932, 'step': 756000}
INFO:transformers.trainer:{'loss': 2.5583420741558074, 'learning_rate': 3.484966825479794e-05, 'epoch': 0.9090199047121237, 'step': 756500}
INFO:transformers.trainer:{'loss': 2.5286607745885847, 'learning_rate': 3.48396548167641e-05, 'epoch': 0.9096207109941542, 'step': 757000}
INFO:transformers.trainer:{'loss': 2.5916887197494507, 'learning_rate': 3.482964137873026e-05, 'epoch': 0.9102215172761846, 'step': 757500}
INFO:transformers.trainer:{'loss': 2.5373847638368607, 'learning_rate': 3.4819627940696416e-05, 'epoch': 0.9108223235582151, 'step': 758000}
INFO:transformers.trainer:{'loss': 2.537323352575302, 'learning_rate': 3.480961450266257e-05, 'epoch': 0.9114231298402456, 'step': 758500}
INFO:transformers.trainer:{'loss': 2.517620168209076, 'learning_rate': 3.4799601064628737e-05, 'epoch': 0.9120239361222761, 'step': 759000}
INFO:transformers.trainer:{'loss': 2.508503397703171, 'learning_rate': 3.4789587626594894e-05, 'epoch': 0.9126247424043066, 'step': 759500}
INFO:transformers.trainer:{'loss': 2.5504709194898605, 'learning_rate': 3.477957418856105e-05, 'epoch': 0.9132255486863371, 'step': 760000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-760000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-760000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-760000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-740000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.595029168009758, 'learning_rate': 3.476956075052721e-05, 'epoch': 0.9138263549683675, 'step': 760500}
INFO:transformers.trainer:{'loss': 2.5887024092674253, 'learning_rate': 3.4759547312493365e-05, 'epoch': 0.914427161250398, 'step': 761000}
INFO:transformers.trainer:{'loss': 2.576948914170265, 'learning_rate': 3.474953387445953e-05, 'epoch': 0.9150279675324285, 'step': 761500}
INFO:transformers.trainer:{'loss': 2.5915303316116334, 'learning_rate': 3.4739520436425686e-05, 'epoch': 0.915628773814459, 'step': 762000}
INFO:transformers.trainer:{'loss': 2.5465801063776015, 'learning_rate': 3.472950699839184e-05, 'epoch': 0.9162295800964895, 'step': 762500}
INFO:transformers.trainer:{'loss': 2.5597282506227494, 'learning_rate': 3.4719493560358e-05, 'epoch': 0.91683038637852, 'step': 763000}
INFO:transformers.trainer:{'loss': 2.5688746190071106, 'learning_rate': 3.4709480122324164e-05, 'epoch': 0.9174311926605505, 'step': 763500}
INFO:transformers.trainer:{'loss': 2.5155642603635786, 'learning_rate': 3.469946668429032e-05, 'epoch': 0.9180319989425809, 'step': 764000}
INFO:transformers.trainer:{'loss': 2.586260376930237, 'learning_rate': 3.468945324625648e-05, 'epoch': 0.9186328052246114, 'step': 764500}
INFO:transformers.trainer:{'loss': 2.5341630029678344, 'learning_rate': 3.4679439808222635e-05, 'epoch': 0.919233611506642, 'step': 765000}
INFO:transformers.trainer:{'loss': 2.5099363822937013, 'learning_rate': 3.46694263701888e-05, 'epoch': 0.9198344177886724, 'step': 765500}
INFO:transformers.trainer:{'loss': 2.5841401209831236, 'learning_rate': 3.4659412932154956e-05, 'epoch': 0.9204352240707029, 'step': 766000}
INFO:transformers.trainer:{'loss': 2.5755449769496916, 'learning_rate': 3.464939949412111e-05, 'epoch': 0.9210360303527334, 'step': 766500}
INFO:transformers.trainer:{'loss': 2.5551797134876253, 'learning_rate': 3.463938605608727e-05, 'epoch': 0.9216368366347638, 'step': 767000}
INFO:transformers.trainer:{'loss': 2.560489898443222, 'learning_rate': 3.462937261805343e-05, 'epoch': 0.9222376429167943, 'step': 767500}
INFO:transformers.trainer:{'loss': 2.5549541821479798, 'learning_rate': 3.461935918001959e-05, 'epoch': 0.9228384491988249, 'step': 768000}
INFO:transformers.trainer:{'loss': 2.5825678627490998, 'learning_rate': 3.460934574198575e-05, 'epoch': 0.9234392554808553, 'step': 768500}
INFO:transformers.trainer:{'loss': 2.5446664251089097, 'learning_rate': 3.4599332303951905e-05, 'epoch': 0.9240400617628858, 'step': 769000}
INFO:transformers.trainer:{'loss': 2.562826693058014, 'learning_rate': 3.458931886591806e-05, 'epoch': 0.9246408680449163, 'step': 769500}
INFO:transformers.trainer:{'loss': 2.5144260349273684, 'learning_rate': 3.4579305427884226e-05, 'epoch': 0.9252416743269468, 'step': 770000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-770000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-770000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-770000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-750000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5559902588129044, 'learning_rate': 3.456929198985038e-05, 'epoch': 0.9258424806089772, 'step': 770500}
INFO:transformers.trainer:{'loss': 2.5857105705738066, 'learning_rate': 3.455927855181654e-05, 'epoch': 0.9264432868910077, 'step': 771000}
INFO:transformers.trainer:{'loss': 2.5707693031430243, 'learning_rate': 3.45492651137827e-05, 'epoch': 0.9270440931730383, 'step': 771500}
INFO:transformers.trainer:{'loss': 2.5478292779922485, 'learning_rate': 3.4539251675748854e-05, 'epoch': 0.9276448994550687, 'step': 772000}
INFO:transformers.trainer:{'loss': 2.5219511343240737, 'learning_rate': 3.452923823771502e-05, 'epoch': 0.9282457057370992, 'step': 772500}
INFO:transformers.trainer:{'loss': 2.5705767022371293, 'learning_rate': 3.451922479968117e-05, 'epoch': 0.9288465120191297, 'step': 773000}
INFO:transformers.trainer:{'loss': 2.5784384567737577, 'learning_rate': 3.450921136164733e-05, 'epoch': 0.9294473183011601, 'step': 773500}
INFO:transformers.trainer:{'loss': 2.6040445660352707, 'learning_rate': 3.449919792361349e-05, 'epoch': 0.9300481245831906, 'step': 774000}
INFO:transformers.trainer:{'loss': 2.5345946888923647, 'learning_rate': 3.448918448557965e-05, 'epoch': 0.9306489308652212, 'step': 774500}
INFO:transformers.trainer:{'loss': 2.580464363694191, 'learning_rate': 3.447917104754581e-05, 'epoch': 0.9312497371472516, 'step': 775000}
INFO:transformers.trainer:{'loss': 2.5504185816049576, 'learning_rate': 3.446915760951197e-05, 'epoch': 0.9318505434292821, 'step': 775500}
INFO:transformers.trainer:{'loss': 2.5430944983959196, 'learning_rate': 3.4459144171478124e-05, 'epoch': 0.9324513497113126, 'step': 776000}
INFO:transformers.trainer:{'loss': 2.570880253791809, 'learning_rate': 3.444913073344429e-05, 'epoch': 0.933052155993343, 'step': 776500}
INFO:transformers.trainer:{'loss': 2.5746770963668824, 'learning_rate': 3.4439117295410445e-05, 'epoch': 0.9336529622753735, 'step': 777000}
INFO:transformers.trainer:{'loss': 2.5191944793462753, 'learning_rate': 3.44291038573766e-05, 'epoch': 0.934253768557404, 'step': 777500}
INFO:transformers.trainer:{'loss': 2.565863450884819, 'learning_rate': 3.441909041934276e-05, 'epoch': 0.9348545748394346, 'step': 778000}
INFO:transformers.trainer:{'loss': 2.5250224834680557, 'learning_rate': 3.4409076981308916e-05, 'epoch': 0.935455381121465, 'step': 778500}
INFO:transformers.trainer:{'loss': 2.5806371309757234, 'learning_rate': 3.439906354327508e-05, 'epoch': 0.9360561874034955, 'step': 779000}
INFO:transformers.trainer:{'loss': 2.552032419204712, 'learning_rate': 3.438905010524123e-05, 'epoch': 0.936656993685526, 'step': 779500}
INFO:transformers.trainer:{'loss': 2.608303689002991, 'learning_rate': 3.4379036667207394e-05, 'epoch': 0.9372577999675564, 'step': 780000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-780000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-780000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-780000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-760000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5484714180231096, 'learning_rate': 3.436902322917355e-05, 'epoch': 0.937858606249587, 'step': 780500}
INFO:transformers.trainer:{'loss': 2.5355619546175, 'learning_rate': 3.4359009791139715e-05, 'epoch': 0.9384594125316175, 'step': 781000}
INFO:transformers.trainer:{'loss': 2.586987862229347, 'learning_rate': 3.434899635310587e-05, 'epoch': 0.9390602188136479, 'step': 781500}
INFO:transformers.trainer:{'loss': 2.5926183669567107, 'learning_rate': 3.433898291507203e-05, 'epoch': 0.9396610250956784, 'step': 782000}
INFO:transformers.trainer:{'loss': 2.5243349010944365, 'learning_rate': 3.4328969477038186e-05, 'epoch': 0.9402618313777089, 'step': 782500}
INFO:transformers.trainer:{'loss': 2.5859080226421356, 'learning_rate': 3.4318956039004343e-05, 'epoch': 0.9408626376597393, 'step': 783000}
INFO:transformers.trainer:{'loss': 2.567288360476494, 'learning_rate': 3.430894260097051e-05, 'epoch': 0.9414634439417698, 'step': 783500}
INFO:transformers.trainer:{'loss': 2.545673682689667, 'learning_rate': 3.429892916293666e-05, 'epoch': 0.9420642502238004, 'step': 784000}
INFO:transformers.trainer:{'loss': 2.529520598530769, 'learning_rate': 3.428891572490282e-05, 'epoch': 0.9426650565058309, 'step': 784500}
INFO:transformers.trainer:{'loss': 2.6073639013767242, 'learning_rate': 3.427890228686898e-05, 'epoch': 0.9432658627878613, 'step': 785000}
INFO:transformers.trainer:{'loss': 2.544504307508469, 'learning_rate': 3.426888884883514e-05, 'epoch': 0.9438666690698918, 'step': 785500}
INFO:transformers.trainer:{'loss': 2.5428893140554427, 'learning_rate': 3.425887541080129e-05, 'epoch': 0.9444674753519223, 'step': 786000}
INFO:transformers.trainer:{'loss': 2.586576649069786, 'learning_rate': 3.4248861972767456e-05, 'epoch': 0.9450682816339527, 'step': 786500}
INFO:transformers.trainer:{'loss': 2.5944250885248183, 'learning_rate': 3.4238848534733614e-05, 'epoch': 0.9456690879159833, 'step': 787000}
INFO:transformers.trainer:{'loss': 2.491439091026783, 'learning_rate': 3.422883509669977e-05, 'epoch': 0.9462698941980138, 'step': 787500}
INFO:transformers.trainer:{'loss': 2.576420655965805, 'learning_rate': 3.4218821658665934e-05, 'epoch': 0.9468707004800442, 'step': 788000}
INFO:transformers.trainer:{'loss': 2.5394848648309707, 'learning_rate': 3.420880822063209e-05, 'epoch': 0.9474715067620747, 'step': 788500}
INFO:transformers.trainer:{'loss': 2.4838988572359084, 'learning_rate': 3.419879478259825e-05, 'epoch': 0.9480723130441052, 'step': 789000}
INFO:transformers.trainer:{'loss': 2.55447148001194, 'learning_rate': 3.4188781344564406e-05, 'epoch': 0.9486731193261356, 'step': 789500}
INFO:transformers.trainer:{'loss': 2.5287865451574327, 'learning_rate': 3.417876790653057e-05, 'epoch': 0.9492739256081661, 'step': 790000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-790000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-790000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-790000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-770000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5189973266124723, 'learning_rate': 3.416875446849672e-05, 'epoch': 0.9498747318901967, 'step': 790500}
INFO:transformers.trainer:{'loss': 2.521353768825531, 'learning_rate': 3.4158741030462884e-05, 'epoch': 0.9504755381722271, 'step': 791000}
INFO:transformers.trainer:{'loss': 2.5366278536319733, 'learning_rate': 3.414872759242904e-05, 'epoch': 0.9510763444542576, 'step': 791500}
INFO:transformers.trainer:{'loss': 2.598638187646866, 'learning_rate': 3.4138714154395204e-05, 'epoch': 0.9516771507362881, 'step': 792000}
INFO:transformers.trainer:{'loss': 2.5379152693748472, 'learning_rate': 3.4128700716361355e-05, 'epoch': 0.9522779570183186, 'step': 792500}
INFO:transformers.trainer:{'loss': 2.558255154132843, 'learning_rate': 3.411868727832752e-05, 'epoch': 0.952878763300349, 'step': 793000}
INFO:transformers.trainer:{'loss': 2.486440944433212, 'learning_rate': 3.4108673840293676e-05, 'epoch': 0.9534795695823796, 'step': 793500}
INFO:transformers.trainer:{'loss': 2.585064070224762, 'learning_rate': 3.409866040225983e-05, 'epoch': 0.9540803758644101, 'step': 794000}
INFO:transformers.trainer:{'loss': 2.5683014314174653, 'learning_rate': 3.4088646964225997e-05, 'epoch': 0.9546811821464405, 'step': 794500}
INFO:transformers.trainer:{'loss': 2.544121430516243, 'learning_rate': 3.407863352619215e-05, 'epoch': 0.955281988428471, 'step': 795000}
INFO:transformers.trainer:{'loss': 2.5575329674482346, 'learning_rate': 3.406862008815831e-05, 'epoch': 0.9558827947105015, 'step': 795500}
INFO:transformers.trainer:{'loss': 2.5721202315092087, 'learning_rate': 3.405860665012447e-05, 'epoch': 0.9564836009925319, 'step': 796000}
INFO:transformers.trainer:{'loss': 2.5393718957901, 'learning_rate': 3.404859321209063e-05, 'epoch': 0.9570844072745625, 'step': 796500}
INFO:transformers.trainer:{'loss': 2.5724779068231585, 'learning_rate': 3.403857977405678e-05, 'epoch': 0.957685213556593, 'step': 797000}
INFO:transformers.trainer:{'loss': 2.54013092315197, 'learning_rate': 3.4028566336022946e-05, 'epoch': 0.9582860198386234, 'step': 797500}
INFO:transformers.trainer:{'loss': 2.518334226965904, 'learning_rate': 3.40185528979891e-05, 'epoch': 0.9588868261206539, 'step': 798000}
INFO:transformers.trainer:{'loss': 2.5068167692422865, 'learning_rate': 3.400853945995526e-05, 'epoch': 0.9594876324026844, 'step': 798500}
INFO:transformers.trainer:{'loss': 2.539979378223419, 'learning_rate': 3.399852602192142e-05, 'epoch': 0.9600884386847149, 'step': 799000}
INFO:transformers.trainer:{'loss': 2.5155378952026366, 'learning_rate': 3.3988512583887574e-05, 'epoch': 0.9606892449667453, 'step': 799500}
INFO:transformers.trainer:{'loss': 2.5114665131568907, 'learning_rate': 3.397849914585374e-05, 'epoch': 0.9612900512487759, 'step': 800000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-800000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-800000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-800000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-780000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5508604892492293, 'learning_rate': 3.3968485707819895e-05, 'epoch': 0.9618908575308064, 'step': 800500}
INFO:transformers.trainer:{'loss': 2.5213779867887496, 'learning_rate': 3.395847226978606e-05, 'epoch': 0.9624916638128368, 'step': 801000}
INFO:transformers.trainer:{'loss': 2.5443650963306426, 'learning_rate': 3.394845883175221e-05, 'epoch': 0.9630924700948673, 'step': 801500}
INFO:transformers.trainer:{'loss': 2.5669610360860826, 'learning_rate': 3.393844539371837e-05, 'epoch': 0.9636932763768978, 'step': 802000}
INFO:transformers.trainer:{'loss': 2.5652984416484834, 'learning_rate': 3.392843195568453e-05, 'epoch': 0.9642940826589282, 'step': 802500}
INFO:transformers.trainer:{'loss': 2.5400048431158067, 'learning_rate': 3.3918418517650694e-05, 'epoch': 0.9648948889409588, 'step': 803000}
INFO:transformers.trainer:{'loss': 2.5443943904042245, 'learning_rate': 3.3908405079616844e-05, 'epoch': 0.9654956952229893, 'step': 803500}
INFO:transformers.trainer:{'loss': 2.5684780728816987, 'learning_rate': 3.389839164158301e-05, 'epoch': 0.9660965015050197, 'step': 804000}
INFO:transformers.trainer:{'loss': 2.5369888877868654, 'learning_rate': 3.3888378203549165e-05, 'epoch': 0.9666973077870502, 'step': 804500}
INFO:transformers.trainer:{'loss': 2.4845646367073058, 'learning_rate': 3.387836476551532e-05, 'epoch': 0.9672981140690807, 'step': 805000}
INFO:transformers.trainer:{'loss': 2.57519037437439, 'learning_rate': 3.3868351327481486e-05, 'epoch': 0.9678989203511111, 'step': 805500}
INFO:transformers.trainer:{'loss': 2.524093470454216, 'learning_rate': 3.3858337889447636e-05, 'epoch': 0.9684997266331417, 'step': 806000}
INFO:transformers.trainer:{'loss': 2.5129103459715845, 'learning_rate': 3.38483244514138e-05, 'epoch': 0.9691005329151722, 'step': 806500}
INFO:transformers.trainer:{'loss': 2.519344610452652, 'learning_rate': 3.383831101337996e-05, 'epoch': 0.9697013391972027, 'step': 807000}
INFO:transformers.trainer:{'loss': 2.528532221674919, 'learning_rate': 3.382829757534612e-05, 'epoch': 0.9703021454792331, 'step': 807500}
INFO:transformers.trainer:{'loss': 2.5269560976028442, 'learning_rate': 3.381828413731227e-05, 'epoch': 0.9709029517612636, 'step': 808000}
INFO:transformers.trainer:{'loss': 2.5657031630277634, 'learning_rate': 3.3808270699278435e-05, 'epoch': 0.9715037580432941, 'step': 808500}
INFO:transformers.trainer:{'loss': 2.464581232070923, 'learning_rate': 3.379825726124459e-05, 'epoch': 0.9721045643253245, 'step': 809000}
INFO:transformers.trainer:{'loss': 2.575859552025795, 'learning_rate': 3.378824382321075e-05, 'epoch': 0.9727053706073551, 'step': 809500}
INFO:transformers.trainer:{'loss': 2.537635441839695, 'learning_rate': 3.3778230385176906e-05, 'epoch': 0.9733061768893856, 'step': 810000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-810000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-810000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-810000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-790000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5009403587579726, 'learning_rate': 3.376821694714306e-05, 'epoch': 0.973906983171416, 'step': 810500}
INFO:transformers.trainer:{'loss': 2.456883839726448, 'learning_rate': 3.375820350910923e-05, 'epoch': 0.9745077894534465, 'step': 811000}
INFO:transformers.trainer:{'loss': 2.5006758496761323, 'learning_rate': 3.3748190071075384e-05, 'epoch': 0.975108595735477, 'step': 811500}
INFO:transformers.trainer:{'loss': 2.5873782616853713, 'learning_rate': 3.373817663304155e-05, 'epoch': 0.9757094020175074, 'step': 812000}
INFO:transformers.trainer:{'loss': 2.4914258685112, 'learning_rate': 3.37281631950077e-05, 'epoch': 0.976310208299538, 'step': 812500}
INFO:transformers.trainer:{'loss': 2.5280943994522094, 'learning_rate': 3.371814975697386e-05, 'epoch': 0.9769110145815685, 'step': 813000}
INFO:transformers.trainer:{'loss': 2.5009161250591276, 'learning_rate': 3.370813631894002e-05, 'epoch': 0.977511820863599, 'step': 813500}
INFO:transformers.trainer:{'loss': 2.560349803209305, 'learning_rate': 3.369812288090618e-05, 'epoch': 0.9781126271456294, 'step': 814000}
INFO:transformers.trainer:{'loss': 2.517894204854965, 'learning_rate': 3.368810944287233e-05, 'epoch': 0.9787134334276599, 'step': 814500}
INFO:transformers.trainer:{'loss': 2.498057789683342, 'learning_rate': 3.36780960048385e-05, 'epoch': 0.9793142397096904, 'step': 815000}
INFO:transformers.trainer:{'loss': 2.54962654030323, 'learning_rate': 3.3668082566804654e-05, 'epoch': 0.9799150459917209, 'step': 815500}
INFO:transformers.trainer:{'loss': 2.576194845557213, 'learning_rate': 3.365806912877081e-05, 'epoch': 0.9805158522737514, 'step': 816000}
INFO:transformers.trainer:{'loss': 2.520569257378578, 'learning_rate': 3.364805569073697e-05, 'epoch': 0.9811166585557819, 'step': 816500}
INFO:transformers.trainer:{'loss': 2.5335762289762496, 'learning_rate': 3.3638042252703125e-05, 'epoch': 0.9817174648378123, 'step': 817000}
INFO:transformers.trainer:{'loss': 2.5616589282751083, 'learning_rate': 3.362802881466929e-05, 'epoch': 0.9823182711198428, 'step': 817500}
INFO:transformers.trainer:{'loss': 2.553961241006851, 'learning_rate': 3.3618015376635446e-05, 'epoch': 0.9829190774018733, 'step': 818000}
INFO:transformers.trainer:{'loss': 2.5117744352817537, 'learning_rate': 3.360800193860161e-05, 'epoch': 0.9835198836839038, 'step': 818500}
INFO:transformers.trainer:{'loss': 2.571115778923035, 'learning_rate': 3.359798850056776e-05, 'epoch': 0.9841206899659343, 'step': 819000}
INFO:transformers.trainer:{'loss': 2.5575658294558523, 'learning_rate': 3.3587975062533924e-05, 'epoch': 0.9847214962479648, 'step': 819500}
INFO:transformers.trainer:{'loss': 2.5529742488861085, 'learning_rate': 3.357796162450008e-05, 'epoch': 0.9853223025299953, 'step': 820000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-820000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-820000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-820000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-800000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.482676945209503, 'learning_rate': 3.356794818646624e-05, 'epoch': 0.9859231088120257, 'step': 820500}
INFO:transformers.trainer:{'loss': 2.535807916998863, 'learning_rate': 3.3557934748432396e-05, 'epoch': 0.9865239150940562, 'step': 821000}
INFO:transformers.trainer:{'loss': 2.5707373123168944, 'learning_rate': 3.354792131039855e-05, 'epoch': 0.9871247213760868, 'step': 821500}
INFO:transformers.trainer:{'loss': 2.510722206234932, 'learning_rate': 3.3537907872364716e-05, 'epoch': 0.9877255276581172, 'step': 822000}
INFO:transformers.trainer:{'loss': 2.512548658251762, 'learning_rate': 3.3527894434330873e-05, 'epoch': 0.9883263339401477, 'step': 822500}
INFO:transformers.trainer:{'loss': 2.5678302570581435, 'learning_rate': 3.351788099629703e-05, 'epoch': 0.9889271402221782, 'step': 823000}
INFO:transformers.trainer:{'loss': 2.505370492100716, 'learning_rate': 3.350786755826319e-05, 'epoch': 0.9895279465042086, 'step': 823500}
INFO:transformers.trainer:{'loss': 2.535991788983345, 'learning_rate': 3.349785412022935e-05, 'epoch': 0.9901287527862391, 'step': 824000}
INFO:transformers.trainer:{'loss': 2.4578522411584856, 'learning_rate': 3.348784068219551e-05, 'epoch': 0.9907295590682696, 'step': 824500}
INFO:transformers.trainer:{'loss': 2.522752502322197, 'learning_rate': 3.347782724416167e-05, 'epoch': 0.9913303653503001, 'step': 825000}
INFO:transformers.trainer:{'loss': 2.5843493056297304, 'learning_rate': 3.346781380612782e-05, 'epoch': 0.9919311716323306, 'step': 825500}
INFO:transformers.trainer:{'loss': 2.5096706795692443, 'learning_rate': 3.3457800368093986e-05, 'epoch': 0.9925319779143611, 'step': 826000}
INFO:transformers.trainer:{'loss': 2.524108647584915, 'learning_rate': 3.3447786930060144e-05, 'epoch': 0.9931327841963915, 'step': 826500}
INFO:transformers.trainer:{'loss': 2.527120419025421, 'learning_rate': 3.34377734920263e-05, 'epoch': 0.993733590478422, 'step': 827000}
INFO:transformers.trainer:{'loss': 2.543037459373474, 'learning_rate': 3.342776005399246e-05, 'epoch': 0.9943343967604525, 'step': 827500}
INFO:transformers.trainer:{'loss': 2.5100399926900865, 'learning_rate': 3.3417746615958615e-05, 'epoch': 0.9949352030424831, 'step': 828000}
INFO:transformers.trainer:{'loss': 2.5415830585956574, 'learning_rate': 3.340773317792478e-05, 'epoch': 0.9955360093245135, 'step': 828500}
INFO:transformers.trainer:{'loss': 2.4878334302902223, 'learning_rate': 3.3397719739890936e-05, 'epoch': 0.996136815606544, 'step': 829000}
INFO:transformers.trainer:{'loss': 2.5084968402385712, 'learning_rate': 3.338770630185709e-05, 'epoch': 0.9967376218885745, 'step': 829500}
INFO:transformers.trainer:{'loss': 2.553564151406288, 'learning_rate': 3.337769286382325e-05, 'epoch': 0.9973384281706049, 'step': 830000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-830000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-830000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-830000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-810000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.544292031764984, 'learning_rate': 3.3367679425789414e-05, 'epoch': 0.9979392344526354, 'step': 830500}
INFO:transformers.trainer:{'loss': 2.535539501786232, 'learning_rate': 3.335766598775557e-05, 'epoch': 0.998540040734666, 'step': 831000}
INFO:transformers.trainer:{'loss': 2.558002649664879, 'learning_rate': 3.334765254972173e-05, 'epoch': 0.9991408470166964, 'step': 831500}
INFO:transformers.trainer:{'loss': 2.496298859000206, 'learning_rate': 3.3337639111687885e-05, 'epoch': 0.9997416532987269, 'step': 832000}
INFO:transformers.trainer:{'loss': 2.506716352581978, 'learning_rate': 3.332762567365404e-05, 'epoch': 1.0003424595807573, 'step': 832500}
INFO:transformers.trainer:{'loss': 2.5014368669986724, 'learning_rate': 3.3317612235620206e-05, 'epoch': 1.000943265862788, 'step': 833000}
INFO:transformers.trainer:{'loss': 2.517776579260826, 'learning_rate': 3.330759879758636e-05, 'epoch': 1.0015440721448183, 'step': 833500}
INFO:transformers.trainer:{'loss': 2.5399473882317545, 'learning_rate': 3.329758535955252e-05, 'epoch': 1.0021448784268487, 'step': 834000}
INFO:transformers.trainer:{'loss': 2.478875335097313, 'learning_rate': 3.328757192151868e-05, 'epoch': 1.0027456847088794, 'step': 834500}
INFO:transformers.trainer:{'loss': 2.5406403617858886, 'learning_rate': 3.327755848348484e-05, 'epoch': 1.0033464909909098, 'step': 835000}
INFO:transformers.trainer:{'loss': 2.5427433902025225, 'learning_rate': 3.3267545045451e-05, 'epoch': 1.0039472972729402, 'step': 835500}
INFO:transformers.trainer:{'loss': 2.5229172856807707, 'learning_rate': 3.3257531607417155e-05, 'epoch': 1.0045481035549708, 'step': 836000}
INFO:transformers.trainer:{'loss': 2.545990159869194, 'learning_rate': 3.324751816938331e-05, 'epoch': 1.0051489098370012, 'step': 836500}
INFO:transformers.trainer:{'loss': 2.5826314134597776, 'learning_rate': 3.3237504731349476e-05, 'epoch': 1.0057497161190316, 'step': 837000}
INFO:transformers.trainer:{'loss': 2.542744683265686, 'learning_rate': 3.322749129331563e-05, 'epoch': 1.0063505224010623, 'step': 837500}
INFO:transformers.trainer:{'loss': 2.5386885781288147, 'learning_rate': 3.321747785528179e-05, 'epoch': 1.0069513286830927, 'step': 838000}
INFO:transformers.trainer:{'loss': 2.5101025332212448, 'learning_rate': 3.320746441724795e-05, 'epoch': 1.0075521349651233, 'step': 838500}
INFO:transformers.trainer:{'loss': 2.5810355607271194, 'learning_rate': 3.3197450979214104e-05, 'epoch': 1.0081529412471537, 'step': 839000}
INFO:transformers.trainer:{'loss': 2.5326673752069473, 'learning_rate': 3.318743754118027e-05, 'epoch': 1.0087537475291841, 'step': 839500}
INFO:transformers.trainer:{'loss': 2.535191179871559, 'learning_rate': 3.3177424103146425e-05, 'epoch': 1.0093545538112147, 'step': 840000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-840000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-840000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-840000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-820000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5271958198547364, 'learning_rate': 3.316741066511258e-05, 'epoch': 1.0099553600932452, 'step': 840500}
INFO:transformers.trainer:{'loss': 2.4921330986022947, 'learning_rate': 3.315739722707874e-05, 'epoch': 1.0105561663752756, 'step': 841000}
INFO:transformers.trainer:{'loss': 2.5714674780368805, 'learning_rate': 3.31473837890449e-05, 'epoch': 1.0111569726573062, 'step': 841500}
INFO:transformers.trainer:{'loss': 2.533309729933739, 'learning_rate': 3.313737035101106e-05, 'epoch': 1.0117577789393366, 'step': 842000}
INFO:transformers.trainer:{'loss': 2.5047915440797808, 'learning_rate': 3.312735691297722e-05, 'epoch': 1.012358585221367, 'step': 842500}
INFO:transformers.trainer:{'loss': 2.4999701709747315, 'learning_rate': 3.3117343474943374e-05, 'epoch': 1.0129593915033976, 'step': 843000}
INFO:transformers.trainer:{'loss': 2.523605027794838, 'learning_rate': 3.310733003690953e-05, 'epoch': 1.013560197785428, 'step': 843500}
INFO:transformers.trainer:{'loss': 2.545709897637367, 'learning_rate': 3.3097316598875695e-05, 'epoch': 1.0141610040674585, 'step': 844000}
INFO:transformers.trainer:{'loss': 2.522860703229904, 'learning_rate': 3.308730316084185e-05, 'epoch': 1.014761810349489, 'step': 844500}
INFO:transformers.trainer:{'loss': 2.5145329192876815, 'learning_rate': 3.307728972280801e-05, 'epoch': 1.0153626166315195, 'step': 845000}
INFO:transformers.trainer:{'loss': 2.5572097592949867, 'learning_rate': 3.3067276284774166e-05, 'epoch': 1.01596342291355, 'step': 845500}
INFO:transformers.trainer:{'loss': 2.5483806583881377, 'learning_rate': 3.305726284674033e-05, 'epoch': 1.0165642291955805, 'step': 846000}
INFO:transformers.trainer:{'loss': 2.5221524720191955, 'learning_rate': 3.304724940870649e-05, 'epoch': 1.017165035477611, 'step': 846500}
INFO:transformers.trainer:{'loss': 2.4975670988559724, 'learning_rate': 3.3037235970672644e-05, 'epoch': 1.0177658417596414, 'step': 847000}
INFO:transformers.trainer:{'loss': 2.483925518155098, 'learning_rate': 3.30272225326388e-05, 'epoch': 1.018366648041672, 'step': 847500}
INFO:transformers.trainer:{'loss': 2.534662039399147, 'learning_rate': 3.3017209094604965e-05, 'epoch': 1.0189674543237024, 'step': 848000}
INFO:transformers.trainer:{'loss': 2.538630750656128, 'learning_rate': 3.300719565657112e-05, 'epoch': 1.0195682606057328, 'step': 848500}
INFO:transformers.trainer:{'loss': 2.5040043679475783, 'learning_rate': 3.299718221853728e-05, 'epoch': 1.0201690668877634, 'step': 849000}
INFO:transformers.trainer:{'loss': 2.5089714199304582, 'learning_rate': 3.2987168780503436e-05, 'epoch': 1.0207698731697938, 'step': 849500}
INFO:transformers.trainer:{'loss': 2.473214903831482, 'learning_rate': 3.297715534246959e-05, 'epoch': 1.0213706794518242, 'step': 850000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-850000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-850000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-850000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-830000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.513394021034241, 'learning_rate': 3.296714190443576e-05, 'epoch': 1.0219714857338549, 'step': 850500}
INFO:transformers.trainer:{'loss': 2.560288728713989, 'learning_rate': 3.2957128466401914e-05, 'epoch': 1.0225722920158853, 'step': 851000}
INFO:transformers.trainer:{'loss': 2.5512677582502366, 'learning_rate': 3.294711502836807e-05, 'epoch': 1.0231730982979157, 'step': 851500}
INFO:transformers.trainer:{'loss': 2.531808893084526, 'learning_rate': 3.293710159033423e-05, 'epoch': 1.0237739045799463, 'step': 852000}
INFO:transformers.trainer:{'loss': 2.519169968724251, 'learning_rate': 3.292708815230039e-05, 'epoch': 1.0243747108619767, 'step': 852500}
INFO:transformers.trainer:{'loss': 2.4526121581792832, 'learning_rate': 3.291707471426655e-05, 'epoch': 1.0249755171440074, 'step': 853000}
INFO:transformers.trainer:{'loss': 2.5520456969738006, 'learning_rate': 3.2907061276232706e-05, 'epoch': 1.0255763234260378, 'step': 853500}
INFO:transformers.trainer:{'loss': 2.480602781534195, 'learning_rate': 3.2897047838198863e-05, 'epoch': 1.0261771297080682, 'step': 854000}
INFO:transformers.trainer:{'loss': 2.554113470196724, 'learning_rate': 3.288703440016502e-05, 'epoch': 1.0267779359900988, 'step': 854500}
INFO:transformers.trainer:{'loss': 2.5282701888084413, 'learning_rate': 3.2877020962131184e-05, 'epoch': 1.0273787422721292, 'step': 855000}
INFO:transformers.trainer:{'loss': 2.5244670363664627, 'learning_rate': 3.286700752409734e-05, 'epoch': 1.0279795485541596, 'step': 855500}
INFO:transformers.trainer:{'loss': 2.5236381333470344, 'learning_rate': 3.28569940860635e-05, 'epoch': 1.0285803548361903, 'step': 856000}
INFO:transformers.trainer:{'loss': 2.4866934648752212, 'learning_rate': 3.2846980648029656e-05, 'epoch': 1.0291811611182207, 'step': 856500}
INFO:transformers.trainer:{'loss': 2.529337984442711, 'learning_rate': 3.283696720999582e-05, 'epoch': 1.029781967400251, 'step': 857000}
INFO:transformers.trainer:{'loss': 2.53735763156414, 'learning_rate': 3.2826953771961976e-05, 'epoch': 1.0303827736822817, 'step': 857500}
INFO:transformers.trainer:{'loss': 2.548334274530411, 'learning_rate': 3.2816940333928133e-05, 'epoch': 1.0309835799643121, 'step': 858000}
INFO:transformers.trainer:{'loss': 2.539970515370369, 'learning_rate': 3.280692689589429e-05, 'epoch': 1.0315843862463425, 'step': 858500}
INFO:transformers.trainer:{'loss': 2.5544394599199296, 'learning_rate': 3.279691345786045e-05, 'epoch': 1.0321851925283732, 'step': 859000}
INFO:transformers.trainer:{'loss': 2.516601573944092, 'learning_rate': 3.278690001982661e-05, 'epoch': 1.0327859988104036, 'step': 859500}
INFO:transformers.trainer:{'loss': 2.572324681997299, 'learning_rate': 3.277688658179276e-05, 'epoch': 1.033386805092434, 'step': 860000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-860000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-860000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-860000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-840000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.516934727191925, 'learning_rate': 3.2766873143758926e-05, 'epoch': 1.0339876113744646, 'step': 860500}
INFO:transformers.trainer:{'loss': 2.556118640065193, 'learning_rate': 3.275685970572508e-05, 'epoch': 1.034588417656495, 'step': 861000}
INFO:transformers.trainer:{'loss': 2.496616751432419, 'learning_rate': 3.2746846267691246e-05, 'epoch': 1.0351892239385254, 'step': 861500}
INFO:transformers.trainer:{'loss': 2.541841384768486, 'learning_rate': 3.2736832829657404e-05, 'epoch': 1.035790030220556, 'step': 862000}
INFO:transformers.trainer:{'loss': 2.528337680220604, 'learning_rate': 3.272681939162356e-05, 'epoch': 1.0363908365025865, 'step': 862500}
INFO:transformers.trainer:{'loss': 2.5209486243724823, 'learning_rate': 3.271680595358972e-05, 'epoch': 1.0369916427846169, 'step': 863000}
INFO:transformers.trainer:{'loss': 2.467805122375488, 'learning_rate': 3.270679251555588e-05, 'epoch': 1.0375924490666475, 'step': 863500}
INFO:transformers.trainer:{'loss': 2.5237380464076997, 'learning_rate': 3.269677907752204e-05, 'epoch': 1.038193255348678, 'step': 864000}
INFO:transformers.trainer:{'loss': 2.626662531733513, 'learning_rate': 3.2686765639488196e-05, 'epoch': 1.0387940616307083, 'step': 864500}
INFO:transformers.trainer:{'loss': 2.550319899082184, 'learning_rate': 3.267675220145435e-05, 'epoch': 1.039394867912739, 'step': 865000}
INFO:transformers.trainer:{'loss': 2.5294352136850358, 'learning_rate': 3.266673876342051e-05, 'epoch': 1.0399956741947693, 'step': 865500}
INFO:transformers.trainer:{'loss': 2.49827817094326, 'learning_rate': 3.2656725325386674e-05, 'epoch': 1.0405964804768, 'step': 866000}
INFO:transformers.trainer:{'loss': 2.520660987854004, 'learning_rate': 3.2646711887352824e-05, 'epoch': 1.0411972867588304, 'step': 866500}
INFO:transformers.trainer:{'loss': 2.553628022015095, 'learning_rate': 3.263669844931899e-05, 'epoch': 1.0417980930408608, 'step': 867000}
INFO:transformers.trainer:{'loss': 2.461156203746796, 'learning_rate': 3.2626685011285145e-05, 'epoch': 1.0423988993228914, 'step': 867500}
INFO:transformers.trainer:{'loss': 2.5272196102142335, 'learning_rate': 3.261667157325131e-05, 'epoch': 1.0429997056049218, 'step': 868000}
INFO:transformers.trainer:{'loss': 2.4534889953136445, 'learning_rate': 3.2606658135217466e-05, 'epoch': 1.0436005118869522, 'step': 868500}
INFO:transformers.trainer:{'loss': 2.5497718188762666, 'learning_rate': 3.259664469718362e-05, 'epoch': 1.0442013181689829, 'step': 869000}
INFO:transformers.trainer:{'loss': 2.5356888781785965, 'learning_rate': 3.258663125914978e-05, 'epoch': 1.0448021244510133, 'step': 869500}
INFO:transformers.trainer:{'loss': 2.478556240200996, 'learning_rate': 3.257661782111594e-05, 'epoch': 1.0454029307330437, 'step': 870000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-870000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-870000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-870000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-850000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4921452457904816, 'learning_rate': 3.25666043830821e-05, 'epoch': 1.0460037370150743, 'step': 870500}
INFO:transformers.trainer:{'loss': 2.510277286171913, 'learning_rate': 3.255659094504825e-05, 'epoch': 1.0466045432971047, 'step': 871000}
INFO:transformers.trainer:{'loss': 2.5442537573575974, 'learning_rate': 3.2546577507014415e-05, 'epoch': 1.0472053495791351, 'step': 871500}
INFO:transformers.trainer:{'loss': 2.5115676420927047, 'learning_rate': 3.253656406898057e-05, 'epoch': 1.0478061558611658, 'step': 872000}
INFO:transformers.trainer:{'loss': 2.535921028494835, 'learning_rate': 3.2526550630946736e-05, 'epoch': 1.0484069621431962, 'step': 872500}
INFO:transformers.trainer:{'loss': 2.4938800649642943, 'learning_rate': 3.2516537192912886e-05, 'epoch': 1.0490077684252266, 'step': 873000}
INFO:transformers.trainer:{'loss': 2.489529155373573, 'learning_rate': 3.250652375487905e-05, 'epoch': 1.0496085747072572, 'step': 873500}
INFO:transformers.trainer:{'loss': 2.529683519244194, 'learning_rate': 3.249651031684521e-05, 'epoch': 1.0502093809892876, 'step': 874000}
INFO:transformers.trainer:{'loss': 2.555356984257698, 'learning_rate': 3.248649687881137e-05, 'epoch': 1.050810187271318, 'step': 874500}
INFO:transformers.trainer:{'loss': 2.522086042523384, 'learning_rate': 3.247648344077753e-05, 'epoch': 1.0514109935533487, 'step': 875000}
INFO:transformers.trainer:{'loss': 2.483379886507988, 'learning_rate': 3.2466470002743685e-05, 'epoch': 1.052011799835379, 'step': 875500}
INFO:transformers.trainer:{'loss': 2.503557817220688, 'learning_rate': 3.245645656470984e-05, 'epoch': 1.0526126061174095, 'step': 876000}
INFO:transformers.trainer:{'loss': 2.5559913314580918, 'learning_rate': 3.2446443126676e-05, 'epoch': 1.05321341239944, 'step': 876500}
INFO:transformers.trainer:{'loss': 2.516575001001358, 'learning_rate': 3.243642968864216e-05, 'epoch': 1.0538142186814705, 'step': 877000}
INFO:transformers.trainer:{'loss': 2.5330971893072127, 'learning_rate': 3.242641625060831e-05, 'epoch': 1.054415024963501, 'step': 877500}
INFO:transformers.trainer:{'loss': 2.54153367125988, 'learning_rate': 3.241640281257448e-05, 'epoch': 1.0550158312455316, 'step': 878000}
INFO:transformers.trainer:{'loss': 2.538500866532326, 'learning_rate': 3.2406389374540634e-05, 'epoch': 1.055616637527562, 'step': 878500}
INFO:transformers.trainer:{'loss': 2.533690506219864, 'learning_rate': 3.23963759365068e-05, 'epoch': 1.0562174438095924, 'step': 879000}
INFO:transformers.trainer:{'loss': 2.538709837317467, 'learning_rate': 3.238636249847295e-05, 'epoch': 1.056818250091623, 'step': 879500}
INFO:transformers.trainer:{'loss': 2.544384973883629, 'learning_rate': 3.237634906043911e-05, 'epoch': 1.0574190563736534, 'step': 880000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-880000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-880000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-880000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-860000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.47769282579422, 'learning_rate': 3.236633562240527e-05, 'epoch': 1.058019862655684, 'step': 880500}
INFO:transformers.trainer:{'loss': 2.5221130701303482, 'learning_rate': 3.2356322184371426e-05, 'epoch': 1.0586206689377144, 'step': 881000}
INFO:transformers.trainer:{'loss': 2.5208601168394087, 'learning_rate': 3.234630874633759e-05, 'epoch': 1.0592214752197449, 'step': 881500}
INFO:transformers.trainer:{'loss': 2.5267968045473097, 'learning_rate': 3.233629530830374e-05, 'epoch': 1.0598222815017755, 'step': 882000}
INFO:transformers.trainer:{'loss': 2.495519046187401, 'learning_rate': 3.2326281870269904e-05, 'epoch': 1.060423087783806, 'step': 882500}
INFO:transformers.trainer:{'loss': 2.5108756214380263, 'learning_rate': 3.231626843223606e-05, 'epoch': 1.0610238940658363, 'step': 883000}
INFO:transformers.trainer:{'loss': 2.4881319366693497, 'learning_rate': 3.2306254994202225e-05, 'epoch': 1.061624700347867, 'step': 883500}
INFO:transformers.trainer:{'loss': 2.5041023116111756, 'learning_rate': 3.2296241556168375e-05, 'epoch': 1.0622255066298973, 'step': 884000}
INFO:transformers.trainer:{'loss': 2.526794508099556, 'learning_rate': 3.228622811813454e-05, 'epoch': 1.0628263129119278, 'step': 884500}
INFO:transformers.trainer:{'loss': 2.508077383518219, 'learning_rate': 3.2276214680100696e-05, 'epoch': 1.0634271191939584, 'step': 885000}
INFO:transformers.trainer:{'loss': 2.49171879863739, 'learning_rate': 3.226620124206686e-05, 'epoch': 1.0640279254759888, 'step': 885500}
INFO:transformers.trainer:{'loss': 2.486136207461357, 'learning_rate': 3.225618780403301e-05, 'epoch': 1.0646287317580192, 'step': 886000}
INFO:transformers.trainer:{'loss': 2.5551780978441236, 'learning_rate': 3.2246174365999174e-05, 'epoch': 1.0652295380400498, 'step': 886500}
INFO:transformers.trainer:{'loss': 2.577398467421532, 'learning_rate': 3.223616092796533e-05, 'epoch': 1.0658303443220802, 'step': 887000}
INFO:transformers.trainer:{'loss': 2.555199044704437, 'learning_rate': 3.222614748993149e-05, 'epoch': 1.0664311506041106, 'step': 887500}
INFO:transformers.trainer:{'loss': 2.4787283236980437, 'learning_rate': 3.221613405189765e-05, 'epoch': 1.0670319568861413, 'step': 888000}
INFO:transformers.trainer:{'loss': 2.508686251401901, 'learning_rate': 3.22061206138638e-05, 'epoch': 1.0676327631681717, 'step': 888500}
INFO:transformers.trainer:{'loss': 2.562164850473404, 'learning_rate': 3.2196107175829966e-05, 'epoch': 1.068233569450202, 'step': 889000}
INFO:transformers.trainer:{'loss': 2.5040396723747254, 'learning_rate': 3.218609373779612e-05, 'epoch': 1.0688343757322327, 'step': 889500}
INFO:transformers.trainer:{'loss': 2.5494507619142532, 'learning_rate': 3.217608029976229e-05, 'epoch': 1.0694351820142631, 'step': 890000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-890000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-890000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-890000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-870000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.507127491950989, 'learning_rate': 3.216606686172844e-05, 'epoch': 1.0700359882962935, 'step': 890500}
INFO:transformers.trainer:{'loss': 2.5195776200294495, 'learning_rate': 3.21560534236946e-05, 'epoch': 1.0706367945783242, 'step': 891000}
INFO:transformers.trainer:{'loss': 2.564531956911087, 'learning_rate': 3.214603998566076e-05, 'epoch': 1.0712376008603546, 'step': 891500}
INFO:transformers.trainer:{'loss': 2.4873210254907607, 'learning_rate': 3.2136026547626915e-05, 'epoch': 1.071838407142385, 'step': 892000}
INFO:transformers.trainer:{'loss': 2.4682658717632293, 'learning_rate': 3.212601310959307e-05, 'epoch': 1.0724392134244156, 'step': 892500}
INFO:transformers.trainer:{'loss': 2.5441681405305863, 'learning_rate': 3.211599967155923e-05, 'epoch': 1.073040019706446, 'step': 893000}
INFO:transformers.trainer:{'loss': 2.5232749445438385, 'learning_rate': 3.2105986233525393e-05, 'epoch': 1.0736408259884764, 'step': 893500}
INFO:transformers.trainer:{'loss': 2.487518052697182, 'learning_rate': 3.209597279549155e-05, 'epoch': 1.074241632270507, 'step': 894000}
INFO:transformers.trainer:{'loss': 2.517352131962776, 'learning_rate': 3.2085959357457714e-05, 'epoch': 1.0748424385525375, 'step': 894500}
INFO:transformers.trainer:{'loss': 2.4508943654298783, 'learning_rate': 3.2075945919423865e-05, 'epoch': 1.075443244834568, 'step': 895000}
INFO:transformers.trainer:{'loss': 2.4776004197597503, 'learning_rate': 3.206593248139003e-05, 'epoch': 1.0760440511165985, 'step': 895500}
INFO:transformers.trainer:{'loss': 2.5231482540369035, 'learning_rate': 3.2055919043356186e-05, 'epoch': 1.076644857398629, 'step': 896000}
INFO:transformers.trainer:{'loss': 2.5462909359931944, 'learning_rate': 3.204590560532235e-05, 'epoch': 1.0772456636806595, 'step': 896500}
INFO:transformers.trainer:{'loss': 2.5480925872325897, 'learning_rate': 3.20358921672885e-05, 'epoch': 1.07784646996269, 'step': 897000}
INFO:transformers.trainer:{'loss': 2.5410949667692186, 'learning_rate': 3.2025878729254664e-05, 'epoch': 1.0784472762447204, 'step': 897500}
INFO:transformers.trainer:{'loss': 2.533846931934357, 'learning_rate': 3.201586529122082e-05, 'epoch': 1.079048082526751, 'step': 898000}
INFO:transformers.trainer:{'loss': 2.531662286877632, 'learning_rate': 3.200585185318698e-05, 'epoch': 1.0796488888087814, 'step': 898500}
INFO:transformers.trainer:{'loss': 2.5631724009513857, 'learning_rate': 3.1995838415153135e-05, 'epoch': 1.0802496950908118, 'step': 899000}
INFO:transformers.trainer:{'loss': 2.5147864842414855, 'learning_rate': 3.198582497711929e-05, 'epoch': 1.0808505013728424, 'step': 899500}
INFO:transformers.trainer:{'loss': 2.477138676404953, 'learning_rate': 3.1975811539085456e-05, 'epoch': 1.0814513076548729, 'step': 900000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-900000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-900000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-900000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-880000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5300752359628675, 'learning_rate': 3.196579810105161e-05, 'epoch': 1.0820521139369033, 'step': 900500}
INFO:transformers.trainer:{'loss': 2.5181141763925554, 'learning_rate': 3.1955784663017777e-05, 'epoch': 1.082652920218934, 'step': 901000}
INFO:transformers.trainer:{'loss': 2.459453324794769, 'learning_rate': 3.194577122498393e-05, 'epoch': 1.0832537265009643, 'step': 901500}
INFO:transformers.trainer:{'loss': 2.514757218837738, 'learning_rate': 3.193575778695009e-05, 'epoch': 1.0838545327829947, 'step': 902000}
INFO:transformers.trainer:{'loss': 2.48275572681427, 'learning_rate': 3.192574434891625e-05, 'epoch': 1.0844553390650253, 'step': 902500}
INFO:transformers.trainer:{'loss': 2.5254092618227006, 'learning_rate': 3.1915730910882405e-05, 'epoch': 1.0850561453470557, 'step': 903000}
INFO:transformers.trainer:{'loss': 2.539101180434227, 'learning_rate': 3.190571747284856e-05, 'epoch': 1.0856569516290862, 'step': 903500}
INFO:transformers.trainer:{'loss': 2.5089124602079393, 'learning_rate': 3.189570403481472e-05, 'epoch': 1.0862577579111168, 'step': 904000}
INFO:transformers.trainer:{'loss': 2.5174110168218613, 'learning_rate': 3.188569059678088e-05, 'epoch': 1.0868585641931472, 'step': 904500}
INFO:transformers.trainer:{'loss': 2.441629598736763, 'learning_rate': 3.187567715874704e-05, 'epoch': 1.0874593704751776, 'step': 905000}
INFO:transformers.trainer:{'loss': 2.507933046102524, 'learning_rate': 3.1865663720713204e-05, 'epoch': 1.0880601767572082, 'step': 905500}
INFO:transformers.trainer:{'loss': 2.5293970255851748, 'learning_rate': 3.1855650282679354e-05, 'epoch': 1.0886609830392386, 'step': 906000}
INFO:transformers.trainer:{'loss': 2.490255662560463, 'learning_rate': 3.184563684464552e-05, 'epoch': 1.089261789321269, 'step': 906500}
INFO:transformers.trainer:{'loss': 2.4971815186738966, 'learning_rate': 3.1835623406611675e-05, 'epoch': 1.0898625956032997, 'step': 907000}
INFO:transformers.trainer:{'loss': 2.5080490744113924, 'learning_rate': 3.182560996857783e-05, 'epoch': 1.09046340188533, 'step': 907500}
INFO:transformers.trainer:{'loss': 2.524788643360138, 'learning_rate': 3.181559653054399e-05, 'epoch': 1.0910642081673605, 'step': 908000}
INFO:transformers.trainer:{'loss': 2.495824094295502, 'learning_rate': 3.180558309251015e-05, 'epoch': 1.0916650144493911, 'step': 908500}
INFO:transformers.trainer:{'loss': 2.529359988451004, 'learning_rate': 3.179556965447631e-05, 'epoch': 1.0922658207314215, 'step': 909000}
INFO:transformers.trainer:{'loss': 2.5336878426373004, 'learning_rate': 3.178555621644247e-05, 'epoch': 1.0928666270134522, 'step': 909500}
INFO:transformers.trainer:{'loss': 2.506806980371475, 'learning_rate': 3.1775542778408624e-05, 'epoch': 1.0934674332954826, 'step': 910000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-910000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-910000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-910000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-890000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4750484009981157, 'learning_rate': 3.176552934037478e-05, 'epoch': 1.094068239577513, 'step': 910500}
INFO:transformers.trainer:{'loss': 2.5275492926836014, 'learning_rate': 3.1755515902340945e-05, 'epoch': 1.0946690458595436, 'step': 911000}
INFO:transformers.trainer:{'loss': 2.4803488895297052, 'learning_rate': 3.17455024643071e-05, 'epoch': 1.095269852141574, 'step': 911500}
INFO:transformers.trainer:{'loss': 2.488180582165718, 'learning_rate': 3.1735489026273266e-05, 'epoch': 1.0958706584236044, 'step': 912000}
INFO:transformers.trainer:{'loss': 2.515666884183884, 'learning_rate': 3.1725475588239416e-05, 'epoch': 1.096471464705635, 'step': 912500}
INFO:transformers.trainer:{'loss': 2.5061202400922777, 'learning_rate': 3.171546215020558e-05, 'epoch': 1.0970722709876655, 'step': 913000}
INFO:transformers.trainer:{'loss': 2.4956043610572816, 'learning_rate': 3.170544871217174e-05, 'epoch': 1.0976730772696959, 'step': 913500}
INFO:transformers.trainer:{'loss': 2.5121760296821596, 'learning_rate': 3.1695435274137894e-05, 'epoch': 1.0982738835517265, 'step': 914000}
INFO:transformers.trainer:{'loss': 2.493400971531868, 'learning_rate': 3.168542183610405e-05, 'epoch': 1.098874689833757, 'step': 914500}
INFO:transformers.trainer:{'loss': 2.5782728741168977, 'learning_rate': 3.167540839807021e-05, 'epoch': 1.0994754961157873, 'step': 915000}
INFO:transformers.trainer:{'loss': 2.489435664534569, 'learning_rate': 3.166539496003637e-05, 'epoch': 1.100076302397818, 'step': 915500}
INFO:transformers.trainer:{'loss': 2.5544481682777405, 'learning_rate': 3.165538152200253e-05, 'epoch': 1.1006771086798484, 'step': 916000}
INFO:transformers.trainer:{'loss': 2.5045129935741426, 'learning_rate': 3.1645368083968686e-05, 'epoch': 1.1012779149618788, 'step': 916500}
INFO:transformers.trainer:{'loss': 2.4929164488315583, 'learning_rate': 3.163535464593484e-05, 'epoch': 1.1018787212439094, 'step': 917000}
INFO:transformers.trainer:{'loss': 2.5309950085878374, 'learning_rate': 3.162534120790101e-05, 'epoch': 1.1024795275259398, 'step': 917500}
INFO:transformers.trainer:{'loss': 2.4609098069667814, 'learning_rate': 3.1615327769867164e-05, 'epoch': 1.1030803338079702, 'step': 918000}
INFO:transformers.trainer:{'loss': 2.5004649500846865, 'learning_rate': 3.160531433183332e-05, 'epoch': 1.1036811400900008, 'step': 918500}
INFO:transformers.trainer:{'loss': 2.468606302380562, 'learning_rate': 3.159530089379948e-05, 'epoch': 1.1042819463720313, 'step': 919000}
INFO:transformers.trainer:{'loss': 2.453196588754654, 'learning_rate': 3.1585287455765635e-05, 'epoch': 1.1048827526540617, 'step': 919500}
INFO:transformers.trainer:{'loss': 2.5396636189222335, 'learning_rate': 3.15752740177318e-05, 'epoch': 1.1054835589360923, 'step': 920000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-920000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-920000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-920000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-900000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.516187796592712, 'learning_rate': 3.1565260579697956e-05, 'epoch': 1.1060843652181227, 'step': 920500}
INFO:transformers.trainer:{'loss': 2.4985059368610383, 'learning_rate': 3.155524714166411e-05, 'epoch': 1.106685171500153, 'step': 921000}
INFO:transformers.trainer:{'loss': 2.529445586323738, 'learning_rate': 3.154523370363027e-05, 'epoch': 1.1072859777821837, 'step': 921500}
INFO:transformers.trainer:{'loss': 2.4815864033699038, 'learning_rate': 3.1535220265596434e-05, 'epoch': 1.1078867840642141, 'step': 922000}
INFO:transformers.trainer:{'loss': 2.529158056139946, 'learning_rate': 3.152520682756259e-05, 'epoch': 1.1084875903462446, 'step': 922500}
INFO:transformers.trainer:{'loss': 2.5196161653995515, 'learning_rate': 3.151519338952875e-05, 'epoch': 1.1090883966282752, 'step': 923000}
INFO:transformers.trainer:{'loss': 2.5064065614938738, 'learning_rate': 3.1505179951494905e-05, 'epoch': 1.1096892029103056, 'step': 923500}
INFO:transformers.trainer:{'loss': 2.45619666659832, 'learning_rate': 3.149516651346107e-05, 'epoch': 1.1102900091923362, 'step': 924000}
INFO:transformers.trainer:{'loss': 2.5118070187568664, 'learning_rate': 3.1485153075427226e-05, 'epoch': 1.1108908154743666, 'step': 924500}
INFO:transformers.trainer:{'loss': 2.5201483511924745, 'learning_rate': 3.147513963739338e-05, 'epoch': 1.111491621756397, 'step': 925000}
INFO:transformers.trainer:{'loss': 2.4937234109640123, 'learning_rate': 3.146512619935954e-05, 'epoch': 1.1120924280384277, 'step': 925500}
INFO:transformers.trainer:{'loss': 2.542174191236496, 'learning_rate': 3.14551127613257e-05, 'epoch': 1.112693234320458, 'step': 926000}
INFO:transformers.trainer:{'loss': 2.53349094581604, 'learning_rate': 3.144509932329186e-05, 'epoch': 1.1132940406024885, 'step': 926500}
INFO:transformers.trainer:{'loss': 2.5511373637914656, 'learning_rate': 3.143508588525802e-05, 'epoch': 1.1138948468845191, 'step': 927000}
INFO:transformers.trainer:{'loss': 2.48384214758873, 'learning_rate': 3.1425072447224175e-05, 'epoch': 1.1144956531665495, 'step': 927500}
INFO:transformers.trainer:{'loss': 2.503879457950592, 'learning_rate': 3.141505900919033e-05, 'epoch': 1.11509645944858, 'step': 928000}
INFO:transformers.trainer:{'loss': 2.5413073260784147, 'learning_rate': 3.1405045571156496e-05, 'epoch': 1.1156972657306106, 'step': 928500}
INFO:transformers.trainer:{'loss': 2.4997002483606336, 'learning_rate': 3.1395032133122653e-05, 'epoch': 1.116298072012641, 'step': 929000}
INFO:transformers.trainer:{'loss': 2.5097371138334275, 'learning_rate': 3.138501869508881e-05, 'epoch': 1.1168988782946714, 'step': 929500}
INFO:transformers.trainer:{'loss': 2.5142059334516524, 'learning_rate': 3.137500525705497e-05, 'epoch': 1.117499684576702, 'step': 930000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-930000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-930000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-930000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-910000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.49894939661026, 'learning_rate': 3.1364991819021125e-05, 'epoch': 1.1181004908587324, 'step': 930500}
INFO:transformers.trainer:{'loss': 2.4663654268980024, 'learning_rate': 3.135497838098729e-05, 'epoch': 1.1187012971407628, 'step': 931000}
INFO:transformers.trainer:{'loss': 2.492801985681057, 'learning_rate': 3.1344964942953446e-05, 'epoch': 1.1193021034227935, 'step': 931500}
INFO:transformers.trainer:{'loss': 2.513709313750267, 'learning_rate': 3.13349515049196e-05, 'epoch': 1.1199029097048239, 'step': 932000}
INFO:transformers.trainer:{'loss': 2.47819067800045, 'learning_rate': 3.132493806688576e-05, 'epoch': 1.1205037159868543, 'step': 932500}
INFO:transformers.trainer:{'loss': 2.511110612154007, 'learning_rate': 3.1314924628851924e-05, 'epoch': 1.121104522268885, 'step': 933000}
INFO:transformers.trainer:{'loss': 2.437353285074234, 'learning_rate': 3.130491119081808e-05, 'epoch': 1.1217053285509153, 'step': 933500}
INFO:transformers.trainer:{'loss': 2.54564503800869, 'learning_rate': 3.129489775278424e-05, 'epoch': 1.1223061348329457, 'step': 934000}
INFO:transformers.trainer:{'loss': 2.487992346048355, 'learning_rate': 3.1284884314750395e-05, 'epoch': 1.1229069411149764, 'step': 934500}
INFO:transformers.trainer:{'loss': 2.4618861800432206, 'learning_rate': 3.127487087671656e-05, 'epoch': 1.1235077473970068, 'step': 935000}
INFO:transformers.trainer:{'loss': 2.506460453271866, 'learning_rate': 3.1264857438682716e-05, 'epoch': 1.1241085536790372, 'step': 935500}
INFO:transformers.trainer:{'loss': 2.5010051802396775, 'learning_rate': 3.125484400064887e-05, 'epoch': 1.1247093599610678, 'step': 936000}
INFO:transformers.trainer:{'loss': 2.4854729486107825, 'learning_rate': 3.124483056261503e-05, 'epoch': 1.1253101662430982, 'step': 936500}
INFO:transformers.trainer:{'loss': 2.4644089896678922, 'learning_rate': 3.123481712458119e-05, 'epoch': 1.1259109725251286, 'step': 937000}
INFO:transformers.trainer:{'loss': 2.508667129278183, 'learning_rate': 3.122480368654735e-05, 'epoch': 1.1265117788071592, 'step': 937500}
INFO:transformers.trainer:{'loss': 2.5393607985973357, 'learning_rate': 3.121479024851351e-05, 'epoch': 1.1271125850891897, 'step': 938000}
INFO:transformers.trainer:{'loss': 2.5006062352657317, 'learning_rate': 3.1204776810479665e-05, 'epoch': 1.1277133913712203, 'step': 938500}
INFO:transformers.trainer:{'loss': 2.484209059357643, 'learning_rate': 3.119476337244582e-05, 'epoch': 1.1283141976532507, 'step': 939000}
INFO:transformers.trainer:{'loss': 2.5135871307849884, 'learning_rate': 3.1184749934411986e-05, 'epoch': 1.128915003935281, 'step': 939500}
INFO:transformers.trainer:{'loss': 2.504113656640053, 'learning_rate': 3.117473649637814e-05, 'epoch': 1.1295158102173115, 'step': 940000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-940000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-940000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-940000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-920000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5083651258945463, 'learning_rate': 3.11647230583443e-05, 'epoch': 1.1301166164993421, 'step': 940500}
INFO:transformers.trainer:{'loss': 2.545873203992844, 'learning_rate': 3.115470962031046e-05, 'epoch': 1.1307174227813725, 'step': 941000}
INFO:transformers.trainer:{'loss': 2.517845160841942, 'learning_rate': 3.1144696182276614e-05, 'epoch': 1.1313182290634032, 'step': 941500}
INFO:transformers.trainer:{'loss': 2.5101596037745475, 'learning_rate': 3.113468274424278e-05, 'epoch': 1.1319190353454336, 'step': 942000}
INFO:transformers.trainer:{'loss': 2.506091915488243, 'learning_rate': 3.112466930620893e-05, 'epoch': 1.132519841627464, 'step': 942500}
INFO:transformers.trainer:{'loss': 2.560349100589752, 'learning_rate': 3.111465586817509e-05, 'epoch': 1.1331206479094946, 'step': 943000}
INFO:transformers.trainer:{'loss': 2.5456463590860365, 'learning_rate': 3.110464243014125e-05, 'epoch': 1.133721454191525, 'step': 943500}
INFO:transformers.trainer:{'loss': 2.4733135182857513, 'learning_rate': 3.109462899210741e-05, 'epoch': 1.1343222604735554, 'step': 944000}
INFO:transformers.trainer:{'loss': 2.5387923632860185, 'learning_rate': 3.108461555407357e-05, 'epoch': 1.134923066755586, 'step': 944500}
INFO:transformers.trainer:{'loss': 2.4750082159042357, 'learning_rate': 3.107460211603973e-05, 'epoch': 1.1355238730376165, 'step': 945000}
INFO:transformers.trainer:{'loss': 2.471509753346443, 'learning_rate': 3.1064588678005884e-05, 'epoch': 1.136124679319647, 'step': 945500}
INFO:transformers.trainer:{'loss': 2.491203508257866, 'learning_rate': 3.105457523997205e-05, 'epoch': 1.1367254856016775, 'step': 946000}
INFO:transformers.trainer:{'loss': 2.5005224437713625, 'learning_rate': 3.1044561801938205e-05, 'epoch': 1.137326291883708, 'step': 946500}
INFO:transformers.trainer:{'loss': 2.556183158159256, 'learning_rate': 3.103454836390436e-05, 'epoch': 1.1379270981657383, 'step': 947000}
INFO:transformers.trainer:{'loss': 2.4879504972696305, 'learning_rate': 3.102453492587052e-05, 'epoch': 1.138527904447769, 'step': 947500}
INFO:transformers.trainer:{'loss': 2.461153923511505, 'learning_rate': 3.1014521487836676e-05, 'epoch': 1.1391287107297994, 'step': 948000}
INFO:transformers.trainer:{'loss': 2.491382505774498, 'learning_rate': 3.100450804980284e-05, 'epoch': 1.1397295170118298, 'step': 948500}
INFO:transformers.trainer:{'loss': 2.50362091588974, 'learning_rate': 3.099449461176899e-05, 'epoch': 1.1403303232938604, 'step': 949000}
INFO:transformers.trainer:{'loss': 2.5119226064682008, 'learning_rate': 3.0984481173735154e-05, 'epoch': 1.1409311295758908, 'step': 949500}
INFO:transformers.trainer:{'loss': 2.4770871871709823, 'learning_rate': 3.097446773570131e-05, 'epoch': 1.1415319358579215, 'step': 950000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-950000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-950000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-950000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-930000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5560844141244887, 'learning_rate': 3.0964454297667475e-05, 'epoch': 1.1421327421399519, 'step': 950500}
INFO:transformers.trainer:{'loss': 2.5446088421344757, 'learning_rate': 3.095444085963363e-05, 'epoch': 1.1427335484219823, 'step': 951000}
INFO:transformers.trainer:{'loss': 2.5216449447870253, 'learning_rate': 3.094442742159979e-05, 'epoch': 1.1433343547040127, 'step': 951500}
INFO:transformers.trainer:{'loss': 2.5105719678401948, 'learning_rate': 3.0934413983565946e-05, 'epoch': 1.1439351609860433, 'step': 952000}
INFO:transformers.trainer:{'loss': 2.5178112386465075, 'learning_rate': 3.09244005455321e-05, 'epoch': 1.1445359672680737, 'step': 952500}
INFO:transformers.trainer:{'loss': 2.477315710544586, 'learning_rate': 3.091438710749827e-05, 'epoch': 1.1451367735501043, 'step': 953000}
INFO:transformers.trainer:{'loss': 2.5473468462228777, 'learning_rate': 3.090437366946442e-05, 'epoch': 1.1457375798321348, 'step': 953500}
INFO:transformers.trainer:{'loss': 2.51885680603981, 'learning_rate': 3.089436023143058e-05, 'epoch': 1.1463383861141652, 'step': 954000}
INFO:transformers.trainer:{'loss': 2.4866969888210297, 'learning_rate': 3.088434679339674e-05, 'epoch': 1.1469391923961956, 'step': 954500}
INFO:transformers.trainer:{'loss': 2.4531204425096513, 'learning_rate': 3.08743333553629e-05, 'epoch': 1.1475399986782262, 'step': 955000}
INFO:transformers.trainer:{'loss': 2.5201307475566863, 'learning_rate': 3.086431991732906e-05, 'epoch': 1.1481408049602566, 'step': 955500}
INFO:transformers.trainer:{'loss': 2.51417452442646, 'learning_rate': 3.0854306479295216e-05, 'epoch': 1.1487416112422872, 'step': 956000}
INFO:transformers.trainer:{'loss': 2.477117886066437, 'learning_rate': 3.084429304126137e-05, 'epoch': 1.1493424175243176, 'step': 956500}
INFO:transformers.trainer:{'loss': 2.50704622232914, 'learning_rate': 3.083427960322754e-05, 'epoch': 1.149943223806348, 'step': 957000}
INFO:transformers.trainer:{'loss': 2.464615172982216, 'learning_rate': 3.0824266165193694e-05, 'epoch': 1.1505440300883787, 'step': 957500}
INFO:transformers.trainer:{'loss': 2.4774004967212675, 'learning_rate': 3.081425272715985e-05, 'epoch': 1.151144836370409, 'step': 958000}
INFO:transformers.trainer:{'loss': 2.499427302837372, 'learning_rate': 3.080423928912601e-05, 'epoch': 1.1517456426524395, 'step': 958500}
INFO:transformers.trainer:{'loss': 2.4171343783140182, 'learning_rate': 3.0794225851092165e-05, 'epoch': 1.1523464489344701, 'step': 959000}
INFO:transformers.trainer:{'loss': 2.549595620036125, 'learning_rate': 3.078421241305833e-05, 'epoch': 1.1529472552165005, 'step': 959500}
INFO:transformers.trainer:{'loss': 2.487329383254051, 'learning_rate': 3.077419897502448e-05, 'epoch': 1.153548061498531, 'step': 960000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-960000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-960000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-960000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-940000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4916005626916884, 'learning_rate': 3.076418553699064e-05, 'epoch': 1.1541488677805616, 'step': 960500}
INFO:transformers.trainer:{'loss': 2.468963723421097, 'learning_rate': 3.07541720989568e-05, 'epoch': 1.154749674062592, 'step': 961000}
INFO:transformers.trainer:{'loss': 2.5340715911388396, 'learning_rate': 3.0744158660922964e-05, 'epoch': 1.1553504803446224, 'step': 961500}
INFO:transformers.trainer:{'loss': 2.5163533387184143, 'learning_rate': 3.073414522288912e-05, 'epoch': 1.155951286626653, 'step': 962000}
INFO:transformers.trainer:{'loss': 2.505059257388115, 'learning_rate': 3.072413178485528e-05, 'epoch': 1.1565520929086834, 'step': 962500}
INFO:transformers.trainer:{'loss': 2.5392621331214906, 'learning_rate': 3.0714118346821435e-05, 'epoch': 1.1571528991907138, 'step': 963000}
INFO:transformers.trainer:{'loss': 2.433522450566292, 'learning_rate': 3.070410490878759e-05, 'epoch': 1.1577537054727445, 'step': 963500}
INFO:transformers.trainer:{'loss': 2.470523962855339, 'learning_rate': 3.0694091470753756e-05, 'epoch': 1.1583545117547749, 'step': 964000}
INFO:transformers.trainer:{'loss': 2.5057382519245146, 'learning_rate': 3.068407803271991e-05, 'epoch': 1.1589553180368055, 'step': 964500}
INFO:transformers.trainer:{'loss': 2.5248220331668856, 'learning_rate': 3.067406459468607e-05, 'epoch': 1.159556124318836, 'step': 965000}
INFO:transformers.trainer:{'loss': 2.4955246807336806, 'learning_rate': 3.066405115665223e-05, 'epoch': 1.1601569306008663, 'step': 965500}
INFO:transformers.trainer:{'loss': 2.461384717822075, 'learning_rate': 3.065403771861839e-05, 'epoch': 1.1607577368828967, 'step': 966000}
INFO:transformers.trainer:{'loss': 2.514876200199127, 'learning_rate': 3.064402428058454e-05, 'epoch': 1.1613585431649274, 'step': 966500}
INFO:transformers.trainer:{'loss': 2.486737678170204, 'learning_rate': 3.0634010842550706e-05, 'epoch': 1.1619593494469578, 'step': 967000}
INFO:transformers.trainer:{'loss': 2.4953965842723846, 'learning_rate': 3.062399740451686e-05, 'epoch': 1.1625601557289884, 'step': 967500}
INFO:transformers.trainer:{'loss': 2.4670460754036903, 'learning_rate': 3.061398396648302e-05, 'epoch': 1.1631609620110188, 'step': 968000}
INFO:transformers.trainer:{'loss': 2.478907007813454, 'learning_rate': 3.0603970528449183e-05, 'epoch': 1.1637617682930492, 'step': 968500}
INFO:transformers.trainer:{'loss': 2.5026244764328003, 'learning_rate': 3.059395709041534e-05, 'epoch': 1.1643625745750796, 'step': 969000}
INFO:transformers.trainer:{'loss': 2.5471555814743043, 'learning_rate': 3.05839436523815e-05, 'epoch': 1.1649633808571103, 'step': 969500}
INFO:transformers.trainer:{'loss': 2.4834093354940414, 'learning_rate': 3.0573930214347655e-05, 'epoch': 1.1655641871391407, 'step': 970000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-970000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-970000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-970000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-950000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5281785517930984, 'learning_rate': 3.056391677631382e-05, 'epoch': 1.1661649934211713, 'step': 970500}
INFO:transformers.trainer:{'loss': 2.4933053900003435, 'learning_rate': 3.055390333827997e-05, 'epoch': 1.1667657997032017, 'step': 971000}
INFO:transformers.trainer:{'loss': 2.465836527943611, 'learning_rate': 3.054388990024613e-05, 'epoch': 1.1673666059852321, 'step': 971500}
INFO:transformers.trainer:{'loss': 2.490726879000664, 'learning_rate': 3.053387646221229e-05, 'epoch': 1.1679674122672628, 'step': 972000}
INFO:transformers.trainer:{'loss': 2.4589748723506926, 'learning_rate': 3.0523863024178454e-05, 'epoch': 1.1685682185492932, 'step': 972500}
INFO:transformers.trainer:{'loss': 2.492809894442558, 'learning_rate': 3.0513849586144604e-05, 'epoch': 1.1691690248313236, 'step': 973000}
INFO:transformers.trainer:{'loss': 2.4955593782663343, 'learning_rate': 3.0503836148110764e-05, 'epoch': 1.1697698311133542, 'step': 973500}
INFO:transformers.trainer:{'loss': 2.4789530005455016, 'learning_rate': 3.0493822710076925e-05, 'epoch': 1.1703706373953846, 'step': 974000}
INFO:transformers.trainer:{'loss': 2.5164297086000444, 'learning_rate': 3.0483809272043085e-05, 'epoch': 1.170971443677415, 'step': 974500}
INFO:transformers.trainer:{'loss': 2.457118915438652, 'learning_rate': 3.0473795834009246e-05, 'epoch': 1.1715722499594456, 'step': 975000}
INFO:transformers.trainer:{'loss': 2.472908475279808, 'learning_rate': 3.04637823959754e-05, 'epoch': 1.172173056241476, 'step': 975500}
INFO:transformers.trainer:{'loss': 2.503240363240242, 'learning_rate': 3.045376895794156e-05, 'epoch': 1.1727738625235065, 'step': 976000}
INFO:transformers.trainer:{'loss': 2.4884446263313293, 'learning_rate': 3.0443755519907717e-05, 'epoch': 1.173374668805537, 'step': 976500}
INFO:transformers.trainer:{'loss': 2.4921746916770937, 'learning_rate': 3.0433742081873877e-05, 'epoch': 1.1739754750875675, 'step': 977000}
INFO:transformers.trainer:{'loss': 2.4932462372779844, 'learning_rate': 3.042372864384003e-05, 'epoch': 1.174576281369598, 'step': 977500}
INFO:transformers.trainer:{'loss': 2.5373927073478697, 'learning_rate': 3.041371520580619e-05, 'epoch': 1.1751770876516285, 'step': 978000}
INFO:transformers.trainer:{'loss': 2.50890464425087, 'learning_rate': 3.0403701767772352e-05, 'epoch': 1.175777893933659, 'step': 978500}
INFO:transformers.trainer:{'loss': 2.5183279769420626, 'learning_rate': 3.0393688329738512e-05, 'epoch': 1.1763787002156896, 'step': 979000}
INFO:transformers.trainer:{'loss': 2.4923368701934816, 'learning_rate': 3.0383674891704666e-05, 'epoch': 1.17697950649772, 'step': 979500}
INFO:transformers.trainer:{'loss': 2.5071402666568754, 'learning_rate': 3.0373661453670826e-05, 'epoch': 1.1775803127797504, 'step': 980000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-980000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-980000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-980000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-960000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.485805804491043, 'learning_rate': 3.0363648015636987e-05, 'epoch': 1.1781811190617808, 'step': 980500}
INFO:transformers.trainer:{'loss': 2.4796857620477675, 'learning_rate': 3.0353634577603147e-05, 'epoch': 1.1787819253438114, 'step': 981000}
INFO:transformers.trainer:{'loss': 2.443392098724842, 'learning_rate': 3.0343621139569304e-05, 'epoch': 1.1793827316258418, 'step': 981500}
INFO:transformers.trainer:{'loss': 2.52263684463501, 'learning_rate': 3.033360770153546e-05, 'epoch': 1.1799835379078725, 'step': 982000}
INFO:transformers.trainer:{'loss': 2.492911882638931, 'learning_rate': 3.032359426350162e-05, 'epoch': 1.1805843441899029, 'step': 982500}
INFO:transformers.trainer:{'loss': 2.4592503752708437, 'learning_rate': 3.031358082546778e-05, 'epoch': 1.1811851504719333, 'step': 983000}
INFO:transformers.trainer:{'loss': 2.5115810405015946, 'learning_rate': 3.030356738743394e-05, 'epoch': 1.1817859567539637, 'step': 983500}
INFO:transformers.trainer:{'loss': 2.492551337361336, 'learning_rate': 3.0293553949400093e-05, 'epoch': 1.1823867630359943, 'step': 984000}
INFO:transformers.trainer:{'loss': 2.503582355618477, 'learning_rate': 3.0283540511366254e-05, 'epoch': 1.1829875693180247, 'step': 984500}
INFO:transformers.trainer:{'loss': 2.490871488928795, 'learning_rate': 3.0273527073332414e-05, 'epoch': 1.1835883756000554, 'step': 985000}
INFO:transformers.trainer:{'loss': 2.5021563198566437, 'learning_rate': 3.0263513635298574e-05, 'epoch': 1.1841891818820858, 'step': 985500}
INFO:transformers.trainer:{'loss': 2.447428892493248, 'learning_rate': 3.0253500197264728e-05, 'epoch': 1.1847899881641162, 'step': 986000}
INFO:transformers.trainer:{'loss': 2.5513547251224518, 'learning_rate': 3.024348675923089e-05, 'epoch': 1.1853907944461468, 'step': 986500}
INFO:transformers.trainer:{'loss': 2.5666536391973493, 'learning_rate': 3.023347332119705e-05, 'epoch': 1.1859916007281772, 'step': 987000}
INFO:transformers.trainer:{'loss': 2.4823610407114027, 'learning_rate': 3.0223459883163206e-05, 'epoch': 1.1865924070102076, 'step': 987500}
INFO:transformers.trainer:{'loss': 2.45817479801178, 'learning_rate': 3.0213446445129367e-05, 'epoch': 1.1871932132922383, 'step': 988000}
INFO:transformers.trainer:{'loss': 2.45647642660141, 'learning_rate': 3.020343300709552e-05, 'epoch': 1.1877940195742687, 'step': 988500}
INFO:transformers.trainer:{'loss': 2.4767474690675737, 'learning_rate': 3.019341956906168e-05, 'epoch': 1.188394825856299, 'step': 989000}
INFO:transformers.trainer:{'loss': 2.544751308441162, 'learning_rate': 3.018340613102784e-05, 'epoch': 1.1889956321383297, 'step': 989500}
INFO:transformers.trainer:{'loss': 2.5399106394052504, 'learning_rate': 3.0173392692994e-05, 'epoch': 1.1895964384203601, 'step': 990000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-990000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-990000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-990000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-970000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5264163603782652, 'learning_rate': 3.0163379254960155e-05, 'epoch': 1.1901972447023905, 'step': 990500}
INFO:transformers.trainer:{'loss': 2.514674924135208, 'learning_rate': 3.0153365816926316e-05, 'epoch': 1.1907980509844212, 'step': 991000}
INFO:transformers.trainer:{'loss': 2.525364769816399, 'learning_rate': 3.0143352378892476e-05, 'epoch': 1.1913988572664516, 'step': 991500}
INFO:transformers.trainer:{'loss': 2.500476722598076, 'learning_rate': 3.0133338940858637e-05, 'epoch': 1.191999663548482, 'step': 992000}
INFO:transformers.trainer:{'loss': 2.5068191949129104, 'learning_rate': 3.012332550282479e-05, 'epoch': 1.1926004698305126, 'step': 992500}
INFO:transformers.trainer:{'loss': 2.480645322561264, 'learning_rate': 3.011331206479095e-05, 'epoch': 1.193201276112543, 'step': 993000}
INFO:transformers.trainer:{'loss': 2.5179884966611863, 'learning_rate': 3.0103298626757108e-05, 'epoch': 1.1938020823945736, 'step': 993500}
INFO:transformers.trainer:{'loss': 2.5480016491413116, 'learning_rate': 3.0093285188723268e-05, 'epoch': 1.194402888676604, 'step': 994000}
INFO:transformers.trainer:{'loss': 2.48095339179039, 'learning_rate': 3.008327175068943e-05, 'epoch': 1.1950036949586345, 'step': 994500}
INFO:transformers.trainer:{'loss': 2.4786956971883773, 'learning_rate': 3.0073258312655582e-05, 'epoch': 1.1956045012406649, 'step': 995000}
INFO:transformers.trainer:{'loss': 2.5371373542547224, 'learning_rate': 3.0063244874621743e-05, 'epoch': 1.1962053075226955, 'step': 995500}
INFO:transformers.trainer:{'loss': 2.5521603919267655, 'learning_rate': 3.0053231436587903e-05, 'epoch': 1.196806113804726, 'step': 996000}
INFO:transformers.trainer:{'loss': 2.5202578696012496, 'learning_rate': 3.0043217998554064e-05, 'epoch': 1.1974069200867565, 'step': 996500}
INFO:transformers.trainer:{'loss': 2.5091432014107706, 'learning_rate': 3.0033204560520217e-05, 'epoch': 1.198007726368787, 'step': 997000}
INFO:transformers.trainer:{'loss': 2.52746799492836, 'learning_rate': 3.0023191122486378e-05, 'epoch': 1.1986085326508173, 'step': 997500}
INFO:transformers.trainer:{'loss': 2.4900999393463135, 'learning_rate': 3.001317768445254e-05, 'epoch': 1.1992093389328478, 'step': 998000}
INFO:transformers.trainer:{'loss': 2.524932240366936, 'learning_rate': 3.0003164246418695e-05, 'epoch': 1.1998101452148784, 'step': 998500}
INFO:transformers.trainer:{'loss': 2.516244215607643, 'learning_rate': 2.9993150808384852e-05, 'epoch': 1.2004109514969088, 'step': 999000}
INFO:transformers.trainer:{'loss': 2.550610550403595, 'learning_rate': 2.998313737035101e-05, 'epoch': 1.2010117577789394, 'step': 999500}
INFO:transformers.trainer:{'loss': 2.5281842839717865, 'learning_rate': 2.997312393231717e-05, 'epoch': 1.2016125640609698, 'step': 1000000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1000000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1000000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1000000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-980000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.481610765337944, 'learning_rate': 2.996311049428333e-05, 'epoch': 1.2022133703430002, 'step': 1000500}
INFO:transformers.trainer:{'loss': 2.5013865666389465, 'learning_rate': 2.995309705624949e-05, 'epoch': 1.2028141766250309, 'step': 1001000}
INFO:transformers.trainer:{'loss': 2.468258189558983, 'learning_rate': 2.9943083618215645e-05, 'epoch': 1.2034149829070613, 'step': 1001500}
INFO:transformers.trainer:{'loss': 2.4878615069389345, 'learning_rate': 2.9933070180181805e-05, 'epoch': 1.2040157891890917, 'step': 1002000}
INFO:transformers.trainer:{'loss': 2.4618670181035998, 'learning_rate': 2.9923056742147965e-05, 'epoch': 1.2046165954711223, 'step': 1002500}
INFO:transformers.trainer:{'loss': 2.4659348105192183, 'learning_rate': 2.9913043304114126e-05, 'epoch': 1.2052174017531527, 'step': 1003000}
INFO:transformers.trainer:{'loss': 2.4904401918649675, 'learning_rate': 2.990302986608028e-05, 'epoch': 1.2058182080351831, 'step': 1003500}
INFO:transformers.trainer:{'loss': 2.510593659758568, 'learning_rate': 2.989301642804644e-05, 'epoch': 1.2064190143172138, 'step': 1004000}
INFO:transformers.trainer:{'loss': 2.509479517698288, 'learning_rate': 2.9883002990012597e-05, 'epoch': 1.2070198205992442, 'step': 1004500}
INFO:transformers.trainer:{'loss': 2.5013060448169706, 'learning_rate': 2.9872989551978758e-05, 'epoch': 1.2076206268812746, 'step': 1005000}
INFO:transformers.trainer:{'loss': 2.4583709463477135, 'learning_rate': 2.9862976113944918e-05, 'epoch': 1.2082214331633052, 'step': 1005500}
INFO:transformers.trainer:{'loss': 2.4392097259759904, 'learning_rate': 2.9852962675911072e-05, 'epoch': 1.2088222394453356, 'step': 1006000}
INFO:transformers.trainer:{'loss': 2.4901547507047654, 'learning_rate': 2.9842949237877232e-05, 'epoch': 1.209423045727366, 'step': 1006500}
INFO:transformers.trainer:{'loss': 2.441230060338974, 'learning_rate': 2.9832935799843393e-05, 'epoch': 1.2100238520093967, 'step': 1007000}
INFO:transformers.trainer:{'loss': 2.5116245929002763, 'learning_rate': 2.9822922361809553e-05, 'epoch': 1.210624658291427, 'step': 1007500}
INFO:transformers.trainer:{'loss': 2.5104864757061005, 'learning_rate': 2.9812908923775707e-05, 'epoch': 1.2112254645734577, 'step': 1008000}
INFO:transformers.trainer:{'loss': 2.4809797449111937, 'learning_rate': 2.9802895485741867e-05, 'epoch': 1.211826270855488, 'step': 1008500}
INFO:transformers.trainer:{'loss': 2.45831123149395, 'learning_rate': 2.9792882047708028e-05, 'epoch': 1.2124270771375185, 'step': 1009000}
INFO:transformers.trainer:{'loss': 2.466439963102341, 'learning_rate': 2.9782868609674185e-05, 'epoch': 1.213027883419549, 'step': 1009500}
INFO:transformers.trainer:{'loss': 2.4660227183103562, 'learning_rate': 2.9772855171640342e-05, 'epoch': 1.2136286897015796, 'step': 1010000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1010000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1010000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1010000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-990000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.450103953719139, 'learning_rate': 2.97628417336065e-05, 'epoch': 1.21422949598361, 'step': 1010500}
INFO:transformers.trainer:{'loss': 2.4665099861621855, 'learning_rate': 2.975282829557266e-05, 'epoch': 1.2148303022656406, 'step': 1011000}
INFO:transformers.trainer:{'loss': 2.5120262459516525, 'learning_rate': 2.974281485753882e-05, 'epoch': 1.215431108547671, 'step': 1011500}
INFO:transformers.trainer:{'loss': 2.4801263684034347, 'learning_rate': 2.973280141950498e-05, 'epoch': 1.2160319148297014, 'step': 1012000}
INFO:transformers.trainer:{'loss': 2.4711983363628387, 'learning_rate': 2.9722787981471134e-05, 'epoch': 1.216632721111732, 'step': 1012500}
INFO:transformers.trainer:{'loss': 2.5124458882808685, 'learning_rate': 2.9712774543437294e-05, 'epoch': 1.2172335273937624, 'step': 1013000}
INFO:transformers.trainer:{'loss': 2.4904411017894743, 'learning_rate': 2.9702761105403455e-05, 'epoch': 1.2178343336757929, 'step': 1013500}
INFO:transformers.trainer:{'loss': 2.459228076577187, 'learning_rate': 2.9692747667369615e-05, 'epoch': 1.2184351399578235, 'step': 1014000}
INFO:transformers.trainer:{'loss': 2.470037678480148, 'learning_rate': 2.968273422933577e-05, 'epoch': 1.219035946239854, 'step': 1014500}
INFO:transformers.trainer:{'loss': 2.4718595051169396, 'learning_rate': 2.967272079130193e-05, 'epoch': 1.2196367525218843, 'step': 1015000}
INFO:transformers.trainer:{'loss': 2.476169167757034, 'learning_rate': 2.9662707353268086e-05, 'epoch': 1.220237558803915, 'step': 1015500}
INFO:transformers.trainer:{'loss': 2.526729532599449, 'learning_rate': 2.9652693915234247e-05, 'epoch': 1.2208383650859453, 'step': 1016000}
INFO:transformers.trainer:{'loss': 2.3984861832857134, 'learning_rate': 2.96426804772004e-05, 'epoch': 1.2214391713679758, 'step': 1016500}
INFO:transformers.trainer:{'loss': 2.480232662677765, 'learning_rate': 2.963266703916656e-05, 'epoch': 1.2220399776500064, 'step': 1017000}
INFO:transformers.trainer:{'loss': 2.53864607912302, 'learning_rate': 2.962265360113272e-05, 'epoch': 1.2226407839320368, 'step': 1017500}
INFO:transformers.trainer:{'loss': 2.5010539506673815, 'learning_rate': 2.9612640163098882e-05, 'epoch': 1.2232415902140672, 'step': 1018000}
INFO:transformers.trainer:{'loss': 2.4293919194936753, 'learning_rate': 2.9602626725065042e-05, 'epoch': 1.2238423964960978, 'step': 1018500}
INFO:transformers.trainer:{'loss': 2.5065451679229738, 'learning_rate': 2.9592613287031196e-05, 'epoch': 1.2244432027781282, 'step': 1019000}
INFO:transformers.trainer:{'loss': 2.4924821029901505, 'learning_rate': 2.9582599848997356e-05, 'epoch': 1.2250440090601586, 'step': 1019500}
INFO:transformers.trainer:{'loss': 2.517320779919624, 'learning_rate': 2.9572586410963517e-05, 'epoch': 1.2256448153421893, 'step': 1020000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1020000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1020000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1020000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1000000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.521318392157555, 'learning_rate': 2.9562572972929674e-05, 'epoch': 1.2262456216242197, 'step': 1020500}
INFO:transformers.trainer:{'loss': 2.519256139099598, 'learning_rate': 2.955255953489583e-05, 'epoch': 1.22684642790625, 'step': 1021000}
INFO:transformers.trainer:{'loss': 2.470438670396805, 'learning_rate': 2.9542546096861988e-05, 'epoch': 1.2274472341882807, 'step': 1021500}
INFO:transformers.trainer:{'loss': 2.4717063574790954, 'learning_rate': 2.953253265882815e-05, 'epoch': 1.2280480404703111, 'step': 1022000}
INFO:transformers.trainer:{'loss': 2.4676059761047364, 'learning_rate': 2.952251922079431e-05, 'epoch': 1.2286488467523418, 'step': 1022500}
INFO:transformers.trainer:{'loss': 2.491413938999176, 'learning_rate': 2.9512505782760463e-05, 'epoch': 1.2292496530343722, 'step': 1023000}
INFO:transformers.trainer:{'loss': 2.5128460870981217, 'learning_rate': 2.9502492344726623e-05, 'epoch': 1.2298504593164026, 'step': 1023500}
INFO:transformers.trainer:{'loss': 2.5131049580574034, 'learning_rate': 2.9492478906692784e-05, 'epoch': 1.230451265598433, 'step': 1024000}
INFO:transformers.trainer:{'loss': 2.487597627878189, 'learning_rate': 2.9482465468658944e-05, 'epoch': 1.2310520718804636, 'step': 1024500}
INFO:transformers.trainer:{'loss': 2.461152332186699, 'learning_rate': 2.9472452030625105e-05, 'epoch': 1.231652878162494, 'step': 1025000}
INFO:transformers.trainer:{'loss': 2.4825646094083784, 'learning_rate': 2.9462438592591258e-05, 'epoch': 1.2322536844445247, 'step': 1025500}
INFO:transformers.trainer:{'loss': 2.521804034233093, 'learning_rate': 2.945242515455742e-05, 'epoch': 1.232854490726555, 'step': 1026000}
INFO:transformers.trainer:{'loss': 2.59067982172966, 'learning_rate': 2.9442411716523576e-05, 'epoch': 1.2334552970085855, 'step': 1026500}
INFO:transformers.trainer:{'loss': 2.476446163892746, 'learning_rate': 2.9432398278489736e-05, 'epoch': 1.234056103290616, 'step': 1027000}
INFO:transformers.trainer:{'loss': 2.499085144996643, 'learning_rate': 2.942238484045589e-05, 'epoch': 1.2346569095726465, 'step': 1027500}
INFO:transformers.trainer:{'loss': 2.498456198334694, 'learning_rate': 2.941237140242205e-05, 'epoch': 1.235257715854677, 'step': 1028000}
INFO:transformers.trainer:{'loss': 2.5164681020975115, 'learning_rate': 2.940235796438821e-05, 'epoch': 1.2358585221367075, 'step': 1028500}
INFO:transformers.trainer:{'loss': 2.534536827802658, 'learning_rate': 2.939234452635437e-05, 'epoch': 1.236459328418738, 'step': 1029000}
INFO:transformers.trainer:{'loss': 2.468350522041321, 'learning_rate': 2.9382331088320525e-05, 'epoch': 1.2370601347007684, 'step': 1029500}
INFO:transformers.trainer:{'loss': 2.467698846578598, 'learning_rate': 2.9372317650286685e-05, 'epoch': 1.237660940982799, 'step': 1030000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1030000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1030000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1030000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1010000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5211551319360734, 'learning_rate': 2.9362304212252846e-05, 'epoch': 1.2382617472648294, 'step': 1030500}
INFO:transformers.trainer:{'loss': 2.4899673047065733, 'learning_rate': 2.9352290774219003e-05, 'epoch': 1.2388625535468598, 'step': 1031000}
INFO:transformers.trainer:{'loss': 2.4794646461009977, 'learning_rate': 2.9342277336185163e-05, 'epoch': 1.2394633598288904, 'step': 1031500}
INFO:transformers.trainer:{'loss': 2.51569191634655, 'learning_rate': 2.933226389815132e-05, 'epoch': 1.2400641661109209, 'step': 1032000}
INFO:transformers.trainer:{'loss': 2.499681097626686, 'learning_rate': 2.9322250460117477e-05, 'epoch': 1.2406649723929513, 'step': 1032500}
INFO:transformers.trainer:{'loss': 2.4998476266860963, 'learning_rate': 2.9312237022083638e-05, 'epoch': 1.241265778674982, 'step': 1033000}
INFO:transformers.trainer:{'loss': 2.471558654308319, 'learning_rate': 2.93022235840498e-05, 'epoch': 1.2418665849570123, 'step': 1033500}
INFO:transformers.trainer:{'loss': 2.486897397875786, 'learning_rate': 2.9292210146015952e-05, 'epoch': 1.2424673912390427, 'step': 1034000}
INFO:transformers.trainer:{'loss': 2.468318924307823, 'learning_rate': 2.9282196707982112e-05, 'epoch': 1.2430681975210733, 'step': 1034500}
INFO:transformers.trainer:{'loss': 2.4962546381950377, 'learning_rate': 2.9272183269948273e-05, 'epoch': 1.2436690038031037, 'step': 1035000}
INFO:transformers.trainer:{'loss': 2.4678094470500946, 'learning_rate': 2.9262169831914433e-05, 'epoch': 1.2442698100851342, 'step': 1035500}
INFO:transformers.trainer:{'loss': 2.4437064369916914, 'learning_rate': 2.9252156393880587e-05, 'epoch': 1.2448706163671648, 'step': 1036000}
INFO:transformers.trainer:{'loss': 2.5353775417804716, 'learning_rate': 2.9242142955846748e-05, 'epoch': 1.2454714226491952, 'step': 1036500}
INFO:transformers.trainer:{'loss': 2.485451328277588, 'learning_rate': 2.9232129517812905e-05, 'epoch': 1.2460722289312258, 'step': 1037000}
INFO:transformers.trainer:{'loss': 2.4698559582829476, 'learning_rate': 2.9222116079779065e-05, 'epoch': 1.2466730352132562, 'step': 1037500}
INFO:transformers.trainer:{'loss': 2.4942978101968767, 'learning_rate': 2.9212102641745225e-05, 'epoch': 1.2472738414952866, 'step': 1038000}
INFO:transformers.trainer:{'loss': 2.446601687312126, 'learning_rate': 2.920208920371138e-05, 'epoch': 1.247874647777317, 'step': 1038500}
INFO:transformers.trainer:{'loss': 2.5020304839611054, 'learning_rate': 2.919207576567754e-05, 'epoch': 1.2484754540593477, 'step': 1039000}
INFO:transformers.trainer:{'loss': 2.51441483438015, 'learning_rate': 2.91820623276437e-05, 'epoch': 1.249076260341378, 'step': 1039500}
INFO:transformers.trainer:{'loss': 2.548183202266693, 'learning_rate': 2.917204888960986e-05, 'epoch': 1.2496770666234087, 'step': 1040000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1040000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1040000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1040000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1020000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.514783409237862, 'learning_rate': 2.9162035451576014e-05, 'epoch': 1.2502778729054391, 'step': 1040500}
INFO:transformers.trainer:{'loss': 2.4938519480228423, 'learning_rate': 2.9152022013542175e-05, 'epoch': 1.2508786791874695, 'step': 1041000}
INFO:transformers.trainer:{'loss': 2.4965329232215883, 'learning_rate': 2.9142008575508335e-05, 'epoch': 1.2514794854695, 'step': 1041500}
INFO:transformers.trainer:{'loss': 2.52091614818573, 'learning_rate': 2.9131995137474492e-05, 'epoch': 1.2520802917515306, 'step': 1042000}
INFO:transformers.trainer:{'loss': 2.489736449956894, 'learning_rate': 2.912198169944065e-05, 'epoch': 1.252681098033561, 'step': 1042500}
INFO:transformers.trainer:{'loss': 2.4648891569375992, 'learning_rate': 2.9111968261406806e-05, 'epoch': 1.2532819043155916, 'step': 1043000}
INFO:transformers.trainer:{'loss': 2.491418339729309, 'learning_rate': 2.9101954823372967e-05, 'epoch': 1.253882710597622, 'step': 1043500}
INFO:transformers.trainer:{'loss': 2.495415496468544, 'learning_rate': 2.9091941385339127e-05, 'epoch': 1.2544835168796524, 'step': 1044000}
INFO:transformers.trainer:{'loss': 2.480116574406624, 'learning_rate': 2.9081927947305288e-05, 'epoch': 1.255084323161683, 'step': 1044500}
INFO:transformers.trainer:{'loss': 2.484085707902908, 'learning_rate': 2.907191450927144e-05, 'epoch': 1.2556851294437135, 'step': 1045000}
INFO:transformers.trainer:{'loss': 2.4629612963199614, 'learning_rate': 2.9061901071237602e-05, 'epoch': 1.2562859357257439, 'step': 1045500}
INFO:transformers.trainer:{'loss': 2.481242633700371, 'learning_rate': 2.9051887633203762e-05, 'epoch': 1.2568867420077745, 'step': 1046000}
INFO:transformers.trainer:{'loss': 2.512576493740082, 'learning_rate': 2.9041874195169923e-05, 'epoch': 1.257487548289805, 'step': 1046500}
INFO:transformers.trainer:{'loss': 2.442441740632057, 'learning_rate': 2.9031860757136076e-05, 'epoch': 1.2580883545718353, 'step': 1047000}
INFO:transformers.trainer:{'loss': 2.4804323860406874, 'learning_rate': 2.9021847319102237e-05, 'epoch': 1.258689160853866, 'step': 1047500}
INFO:transformers.trainer:{'loss': 2.476145361304283, 'learning_rate': 2.9011833881068394e-05, 'epoch': 1.2592899671358964, 'step': 1048000}
INFO:transformers.trainer:{'loss': 2.4472169547080993, 'learning_rate': 2.9001820443034554e-05, 'epoch': 1.259890773417927, 'step': 1048500}
INFO:transformers.trainer:{'loss': 2.4898962444067, 'learning_rate': 2.8991807005000715e-05, 'epoch': 1.2604915796999574, 'step': 1049000}
INFO:transformers.trainer:{'loss': 2.4824037170410156, 'learning_rate': 2.898179356696687e-05, 'epoch': 1.2610923859819878, 'step': 1049500}
INFO:transformers.trainer:{'loss': 2.4832888857126236, 'learning_rate': 2.897178012893303e-05, 'epoch': 1.2616931922640182, 'step': 1050000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1050000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1050000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1050000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1030000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.465678292095661, 'learning_rate': 2.896176669089919e-05, 'epoch': 1.2622939985460488, 'step': 1050500}
INFO:transformers.trainer:{'loss': 2.5067198721170425, 'learning_rate': 2.895175325286535e-05, 'epoch': 1.2628948048280793, 'step': 1051000}
INFO:transformers.trainer:{'loss': 2.467817172288895, 'learning_rate': 2.8941739814831503e-05, 'epoch': 1.2634956111101099, 'step': 1051500}
INFO:transformers.trainer:{'loss': 2.499509781718254, 'learning_rate': 2.8931726376797664e-05, 'epoch': 1.2640964173921403, 'step': 1052000}
INFO:transformers.trainer:{'loss': 2.477398206591606, 'learning_rate': 2.8921712938763824e-05, 'epoch': 1.2646972236741707, 'step': 1052500}
INFO:transformers.trainer:{'loss': 2.4496159645318984, 'learning_rate': 2.891169950072998e-05, 'epoch': 1.265298029956201, 'step': 1053000}
INFO:transformers.trainer:{'loss': 2.4533663744926453, 'learning_rate': 2.890168606269614e-05, 'epoch': 1.2658988362382317, 'step': 1053500}
INFO:transformers.trainer:{'loss': 2.5080194088220598, 'learning_rate': 2.8891672624662296e-05, 'epoch': 1.2664996425202621, 'step': 1054000}
INFO:transformers.trainer:{'loss': 2.436388960003853, 'learning_rate': 2.8881659186628456e-05, 'epoch': 1.2671004488022928, 'step': 1054500}
INFO:transformers.trainer:{'loss': 2.4539037125110625, 'learning_rate': 2.8871645748594616e-05, 'epoch': 1.2677012550843232, 'step': 1055000}
INFO:transformers.trainer:{'loss': 2.5155235748291016, 'learning_rate': 2.8861632310560777e-05, 'epoch': 1.2683020613663536, 'step': 1055500}
INFO:transformers.trainer:{'loss': 2.4797632156610487, 'learning_rate': 2.885161887252693e-05, 'epoch': 1.268902867648384, 'step': 1056000}
INFO:transformers.trainer:{'loss': 2.523064699292183, 'learning_rate': 2.884160543449309e-05, 'epoch': 1.2695036739304146, 'step': 1056500}
INFO:transformers.trainer:{'loss': 2.510574646830559, 'learning_rate': 2.883159199645925e-05, 'epoch': 1.270104480212445, 'step': 1057000}
INFO:transformers.trainer:{'loss': 2.467660813808441, 'learning_rate': 2.8821578558425412e-05, 'epoch': 1.2707052864944757, 'step': 1057500}
INFO:transformers.trainer:{'loss': 2.4634536596536636, 'learning_rate': 2.8811565120391566e-05, 'epoch': 1.271306092776506, 'step': 1058000}
INFO:transformers.trainer:{'loss': 2.4911048622131347, 'learning_rate': 2.8801551682357726e-05, 'epoch': 1.2719068990585365, 'step': 1058500}
INFO:transformers.trainer:{'loss': 2.4951752651929855, 'learning_rate': 2.8791538244323883e-05, 'epoch': 1.2725077053405671, 'step': 1059000}
INFO:transformers.trainer:{'loss': 2.5003531522750855, 'learning_rate': 2.8781524806290044e-05, 'epoch': 1.2731085116225975, 'step': 1059500}
INFO:transformers.trainer:{'loss': 2.4682978107929228, 'learning_rate': 2.8771511368256197e-05, 'epoch': 1.273709317904628, 'step': 1060000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1060000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1060000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1060000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1040000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4654600702673197, 'learning_rate': 2.8761497930222358e-05, 'epoch': 1.2743101241866586, 'step': 1060500}
INFO:transformers.trainer:{'loss': 2.4963564282655715, 'learning_rate': 2.8751484492188518e-05, 'epoch': 1.274910930468689, 'step': 1061000}
INFO:transformers.trainer:{'loss': 2.446931975722313, 'learning_rate': 2.874147105415468e-05, 'epoch': 1.2755117367507194, 'step': 1061500}
INFO:transformers.trainer:{'loss': 2.453597240686417, 'learning_rate': 2.873145761612084e-05, 'epoch': 1.27611254303275, 'step': 1062000}
INFO:transformers.trainer:{'loss': 2.517107072353363, 'learning_rate': 2.8721444178086993e-05, 'epoch': 1.2767133493147804, 'step': 1062500}
INFO:transformers.trainer:{'loss': 2.494307433128357, 'learning_rate': 2.8711430740053153e-05, 'epoch': 1.277314155596811, 'step': 1063000}
INFO:transformers.trainer:{'loss': 2.5166928769499064, 'learning_rate': 2.8701417302019314e-05, 'epoch': 1.2779149618788415, 'step': 1063500}
INFO:transformers.trainer:{'loss': 2.487538442134857, 'learning_rate': 2.869140386398547e-05, 'epoch': 1.2785157681608719, 'step': 1064000}
INFO:transformers.trainer:{'loss': 2.490495590209961, 'learning_rate': 2.8681390425951628e-05, 'epoch': 1.2791165744429023, 'step': 1064500}
INFO:transformers.trainer:{'loss': 2.4886101635694504, 'learning_rate': 2.8671376987917785e-05, 'epoch': 1.279717380724933, 'step': 1065000}
INFO:transformers.trainer:{'loss': 2.45013843357563, 'learning_rate': 2.8661363549883945e-05, 'epoch': 1.2803181870069633, 'step': 1065500}
INFO:transformers.trainer:{'loss': 2.483232703208923, 'learning_rate': 2.8651350111850106e-05, 'epoch': 1.280918993288994, 'step': 1066000}
INFO:transformers.trainer:{'loss': 2.4595073430538177, 'learning_rate': 2.864133667381626e-05, 'epoch': 1.2815197995710244, 'step': 1066500}
INFO:transformers.trainer:{'loss': 2.449286008477211, 'learning_rate': 2.863132323578242e-05, 'epoch': 1.2821206058530548, 'step': 1067000}
INFO:transformers.trainer:{'loss': 2.4776266008615493, 'learning_rate': 2.862130979774858e-05, 'epoch': 1.2827214121350852, 'step': 1067500}
INFO:transformers.trainer:{'loss': 2.484717312455177, 'learning_rate': 2.861129635971474e-05, 'epoch': 1.2833222184171158, 'step': 1068000}
INFO:transformers.trainer:{'loss': 2.461031692504883, 'learning_rate': 2.86012829216809e-05, 'epoch': 1.2839230246991462, 'step': 1068500}
INFO:transformers.trainer:{'loss': 2.484206445336342, 'learning_rate': 2.8591269483647055e-05, 'epoch': 1.2845238309811768, 'step': 1069000}
INFO:transformers.trainer:{'loss': 2.483673693776131, 'learning_rate': 2.8581256045613215e-05, 'epoch': 1.2851246372632072, 'step': 1069500}
INFO:transformers.trainer:{'loss': 2.532179196715355, 'learning_rate': 2.8571242607579372e-05, 'epoch': 1.2857254435452377, 'step': 1070000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1070000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1070000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1070000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1050000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4924320291280746, 'learning_rate': 2.8561229169545533e-05, 'epoch': 1.286326249827268, 'step': 1070500}
INFO:transformers.trainer:{'loss': 2.5069832373857497, 'learning_rate': 2.8551215731511687e-05, 'epoch': 1.2869270561092987, 'step': 1071000}
INFO:transformers.trainer:{'loss': 2.5021622657775877, 'learning_rate': 2.8541202293477847e-05, 'epoch': 1.287527862391329, 'step': 1071500}
INFO:transformers.trainer:{'loss': 2.4552650763988493, 'learning_rate': 2.8531188855444007e-05, 'epoch': 1.2881286686733597, 'step': 1072000}
INFO:transformers.trainer:{'loss': 2.5079949905872345, 'learning_rate': 2.8521175417410168e-05, 'epoch': 1.2887294749553901, 'step': 1072500}
INFO:transformers.trainer:{'loss': 2.460531274318695, 'learning_rate': 2.851116197937632e-05, 'epoch': 1.2893302812374206, 'step': 1073000}
INFO:transformers.trainer:{'loss': 2.49228101503849, 'learning_rate': 2.8501148541342482e-05, 'epoch': 1.2899310875194512, 'step': 1073500}
INFO:transformers.trainer:{'loss': 2.508374799370766, 'learning_rate': 2.8491135103308643e-05, 'epoch': 1.2905318938014816, 'step': 1074000}
INFO:transformers.trainer:{'loss': 2.464556313276291, 'learning_rate': 2.8481121665274803e-05, 'epoch': 1.291132700083512, 'step': 1074500}
INFO:transformers.trainer:{'loss': 2.5031535360217094, 'learning_rate': 2.847110822724096e-05, 'epoch': 1.2917335063655426, 'step': 1075000}
INFO:transformers.trainer:{'loss': 2.4820332468748094, 'learning_rate': 2.8461094789207117e-05, 'epoch': 1.292334312647573, 'step': 1075500}
INFO:transformers.trainer:{'loss': 2.446869688510895, 'learning_rate': 2.8451081351173274e-05, 'epoch': 1.2929351189296034, 'step': 1076000}
INFO:transformers.trainer:{'loss': 2.4526971406936644, 'learning_rate': 2.8441067913139435e-05, 'epoch': 1.293535925211634, 'step': 1076500}
INFO:transformers.trainer:{'loss': 2.465183548092842, 'learning_rate': 2.8431054475105595e-05, 'epoch': 1.2941367314936645, 'step': 1077000}
INFO:transformers.trainer:{'loss': 2.4929200897216797, 'learning_rate': 2.842104103707175e-05, 'epoch': 1.2947375377756951, 'step': 1077500}
INFO:transformers.trainer:{'loss': 2.4806311250925064, 'learning_rate': 2.841102759903791e-05, 'epoch': 1.2953383440577255, 'step': 1078000}
INFO:transformers.trainer:{'loss': 2.4536016010046007, 'learning_rate': 2.840101416100407e-05, 'epoch': 1.295939150339756, 'step': 1078500}
INFO:transformers.trainer:{'loss': 2.494812884867191, 'learning_rate': 2.839100072297023e-05, 'epoch': 1.2965399566217863, 'step': 1079000}
INFO:transformers.trainer:{'loss': 2.473224123477936, 'learning_rate': 2.8380987284936384e-05, 'epoch': 1.297140762903817, 'step': 1079500}
INFO:transformers.trainer:{'loss': 2.4892427332401277, 'learning_rate': 2.8370973846902544e-05, 'epoch': 1.2977415691858474, 'step': 1080000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1080000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1080000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1080000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1060000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.444453793764114, 'learning_rate': 2.8360960408868705e-05, 'epoch': 1.298342375467878, 'step': 1080500}
INFO:transformers.trainer:{'loss': 2.475415409684181, 'learning_rate': 2.8350946970834862e-05, 'epoch': 1.2989431817499084, 'step': 1081000}
INFO:transformers.trainer:{'loss': 2.4872943910360337, 'learning_rate': 2.8340933532801022e-05, 'epoch': 1.2995439880319388, 'step': 1081500}
INFO:transformers.trainer:{'loss': 2.4804172163009643, 'learning_rate': 2.8330920094767176e-05, 'epoch': 1.3001447943139692, 'step': 1082000}
INFO:transformers.trainer:{'loss': 2.451074755907059, 'learning_rate': 2.8320906656733336e-05, 'epoch': 1.3007456005959999, 'step': 1082500}
INFO:transformers.trainer:{'loss': 2.4310960916280746, 'learning_rate': 2.8310893218699497e-05, 'epoch': 1.3013464068780303, 'step': 1083000}
INFO:transformers.trainer:{'loss': 2.4439107022285462, 'learning_rate': 2.8300879780665657e-05, 'epoch': 1.301947213160061, 'step': 1083500}
INFO:transformers.trainer:{'loss': 2.441568395614624, 'learning_rate': 2.829086634263181e-05, 'epoch': 1.3025480194420913, 'step': 1084000}
INFO:transformers.trainer:{'loss': 2.439239602804184, 'learning_rate': 2.828085290459797e-05, 'epoch': 1.3031488257241217, 'step': 1084500}
INFO:transformers.trainer:{'loss': 2.460901552915573, 'learning_rate': 2.8270839466564132e-05, 'epoch': 1.3037496320061521, 'step': 1085000}
INFO:transformers.trainer:{'loss': 2.4638434681892396, 'learning_rate': 2.8260826028530292e-05, 'epoch': 1.3043504382881828, 'step': 1085500}
INFO:transformers.trainer:{'loss': 2.4350235339403152, 'learning_rate': 2.8250812590496446e-05, 'epoch': 1.3049512445702132, 'step': 1086000}
INFO:transformers.trainer:{'loss': 2.5013976740837096, 'learning_rate': 2.8240799152462606e-05, 'epoch': 1.3055520508522438, 'step': 1086500}
INFO:transformers.trainer:{'loss': 2.487925080060959, 'learning_rate': 2.8230785714428763e-05, 'epoch': 1.3061528571342742, 'step': 1087000}
INFO:transformers.trainer:{'loss': 2.500984851002693, 'learning_rate': 2.8220772276394924e-05, 'epoch': 1.3067536634163046, 'step': 1087500}
INFO:transformers.trainer:{'loss': 2.471868809223175, 'learning_rate': 2.8210758838361084e-05, 'epoch': 1.3073544696983352, 'step': 1088000}
INFO:transformers.trainer:{'loss': 2.437664172887802, 'learning_rate': 2.8200745400327238e-05, 'epoch': 1.3079552759803657, 'step': 1088500}
INFO:transformers.trainer:{'loss': 2.4669085837602616, 'learning_rate': 2.81907319622934e-05, 'epoch': 1.308556082262396, 'step': 1089000}
INFO:transformers.trainer:{'loss': 2.47672727394104, 'learning_rate': 2.818071852425956e-05, 'epoch': 1.3091568885444267, 'step': 1089500}
INFO:transformers.trainer:{'loss': 2.4593109990358353, 'learning_rate': 2.817070508622572e-05, 'epoch': 1.309757694826457, 'step': 1090000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1090000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1090000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1090000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1070000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4067350343465805, 'learning_rate': 2.8160691648191873e-05, 'epoch': 1.3103585011084875, 'step': 1090500}
INFO:transformers.trainer:{'loss': 2.450414770960808, 'learning_rate': 2.8150678210158034e-05, 'epoch': 1.3109593073905181, 'step': 1091000}
INFO:transformers.trainer:{'loss': 2.462436122894287, 'learning_rate': 2.814066477212419e-05, 'epoch': 1.3115601136725485, 'step': 1091500}
INFO:transformers.trainer:{'loss': 2.4392085201740263, 'learning_rate': 2.813065133409035e-05, 'epoch': 1.3121609199545792, 'step': 1092000}
INFO:transformers.trainer:{'loss': 2.4528836842775346, 'learning_rate': 2.8120637896056508e-05, 'epoch': 1.3127617262366096, 'step': 1092500}
INFO:transformers.trainer:{'loss': 2.481249347925186, 'learning_rate': 2.8110624458022665e-05, 'epoch': 1.31336253251864, 'step': 1093000}
INFO:transformers.trainer:{'loss': 2.45618504345417, 'learning_rate': 2.8100611019988826e-05, 'epoch': 1.3139633388006704, 'step': 1093500}
INFO:transformers.trainer:{'loss': 2.4365118594169615, 'learning_rate': 2.8090597581954986e-05, 'epoch': 1.314564145082701, 'step': 1094000}
INFO:transformers.trainer:{'loss': 2.4492430458068846, 'learning_rate': 2.8080584143921147e-05, 'epoch': 1.3151649513647314, 'step': 1094500}
INFO:transformers.trainer:{'loss': 2.5090555192232133, 'learning_rate': 2.80705707058873e-05, 'epoch': 1.315765757646762, 'step': 1095000}
INFO:transformers.trainer:{'loss': 2.451044674515724, 'learning_rate': 2.806055726785346e-05, 'epoch': 1.3163665639287925, 'step': 1095500}
INFO:transformers.trainer:{'loss': 2.4673933907747267, 'learning_rate': 2.805054382981962e-05, 'epoch': 1.3169673702108229, 'step': 1096000}
INFO:transformers.trainer:{'loss': 2.4025381035208704, 'learning_rate': 2.8040530391785778e-05, 'epoch': 1.3175681764928533, 'step': 1096500}
INFO:transformers.trainer:{'loss': 2.4766246873140334, 'learning_rate': 2.8030516953751935e-05, 'epoch': 1.318168982774884, 'step': 1097000}
INFO:transformers.trainer:{'loss': 2.4448913683891296, 'learning_rate': 2.8020503515718092e-05, 'epoch': 1.3187697890569143, 'step': 1097500}
INFO:transformers.trainer:{'loss': 2.5274513696432113, 'learning_rate': 2.8010490077684253e-05, 'epoch': 1.319370595338945, 'step': 1098000}
INFO:transformers.trainer:{'loss': 2.4718034009933474, 'learning_rate': 2.8000476639650413e-05, 'epoch': 1.3199714016209754, 'step': 1098500}
INFO:transformers.trainer:{'loss': 2.451758506298065, 'learning_rate': 2.7990463201616574e-05, 'epoch': 1.3205722079030058, 'step': 1099000}
INFO:transformers.trainer:{'loss': 2.5021505852937698, 'learning_rate': 2.7980449763582727e-05, 'epoch': 1.3211730141850362, 'step': 1099500}
INFO:transformers.trainer:{'loss': 2.4989643963575365, 'learning_rate': 2.7970436325548888e-05, 'epoch': 1.3217738204670668, 'step': 1100000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1100000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1100000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1100000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1080000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4834199076890946, 'learning_rate': 2.7960422887515048e-05, 'epoch': 1.3223746267490972, 'step': 1100500}
INFO:transformers.trainer:{'loss': 2.469932291865349, 'learning_rate': 2.795040944948121e-05, 'epoch': 1.3229754330311279, 'step': 1101000}
INFO:transformers.trainer:{'loss': 2.475191421508789, 'learning_rate': 2.7940396011447362e-05, 'epoch': 1.3235762393131583, 'step': 1101500}
INFO:transformers.trainer:{'loss': 2.451601267337799, 'learning_rate': 2.7930382573413523e-05, 'epoch': 1.3241770455951887, 'step': 1102000}
INFO:transformers.trainer:{'loss': 2.487226284742355, 'learning_rate': 2.792036913537968e-05, 'epoch': 1.3247778518772193, 'step': 1102500}
INFO:transformers.trainer:{'loss': 2.4588800975084304, 'learning_rate': 2.791035569734584e-05, 'epoch': 1.3253786581592497, 'step': 1103000}
INFO:transformers.trainer:{'loss': 2.4935651091337205, 'learning_rate': 2.7900342259311994e-05, 'epoch': 1.3259794644412801, 'step': 1103500}
INFO:transformers.trainer:{'loss': 2.4429461872577667, 'learning_rate': 2.7890328821278154e-05, 'epoch': 1.3265802707233108, 'step': 1104000}
INFO:transformers.trainer:{'loss': 2.453720314383507, 'learning_rate': 2.7880315383244315e-05, 'epoch': 1.3271810770053412, 'step': 1104500}
INFO:transformers.trainer:{'loss': 2.4561243660449983, 'learning_rate': 2.7870301945210475e-05, 'epoch': 1.3277818832873716, 'step': 1105000}
INFO:transformers.trainer:{'loss': 2.489115489244461, 'learning_rate': 2.7860288507176636e-05, 'epoch': 1.3283826895694022, 'step': 1105500}
INFO:transformers.trainer:{'loss': 2.3945176829695702, 'learning_rate': 2.785027506914279e-05, 'epoch': 1.3289834958514326, 'step': 1106000}
INFO:transformers.trainer:{'loss': 2.4775163177251818, 'learning_rate': 2.784026163110895e-05, 'epoch': 1.3295843021334632, 'step': 1106500}
INFO:transformers.trainer:{'loss': 2.4816285898685457, 'learning_rate': 2.783024819307511e-05, 'epoch': 1.3301851084154936, 'step': 1107000}
INFO:transformers.trainer:{'loss': 2.50482896065712, 'learning_rate': 2.7820234755041267e-05, 'epoch': 1.330785914697524, 'step': 1107500}
INFO:transformers.trainer:{'loss': 2.486003630757332, 'learning_rate': 2.7810221317007425e-05, 'epoch': 1.3313867209795545, 'step': 1108000}
INFO:transformers.trainer:{'loss': 2.4881323491334917, 'learning_rate': 2.780020787897358e-05, 'epoch': 1.331987527261585, 'step': 1108500}
INFO:transformers.trainer:{'loss': 2.4726287080049514, 'learning_rate': 2.7790194440939742e-05, 'epoch': 1.3325883335436155, 'step': 1109000}
INFO:transformers.trainer:{'loss': 2.457533084332943, 'learning_rate': 2.7780181002905902e-05, 'epoch': 1.3331891398256461, 'step': 1109500}
INFO:transformers.trainer:{'loss': 2.4717382905483247, 'learning_rate': 2.7770167564872056e-05, 'epoch': 1.3337899461076765, 'step': 1110000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1110000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1110000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1110000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1090000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.452040995836258, 'learning_rate': 2.7760154126838217e-05, 'epoch': 1.334390752389707, 'step': 1110500}
INFO:transformers.trainer:{'loss': 2.4504193065166473, 'learning_rate': 2.7750140688804377e-05, 'epoch': 1.3349915586717374, 'step': 1111000}
INFO:transformers.trainer:{'loss': 2.4647854577302932, 'learning_rate': 2.7740127250770538e-05, 'epoch': 1.335592364953768, 'step': 1111500}
INFO:transformers.trainer:{'loss': 2.4407100484371185, 'learning_rate': 2.7730113812736698e-05, 'epoch': 1.3361931712357984, 'step': 1112000}
INFO:transformers.trainer:{'loss': 2.495228991866112, 'learning_rate': 2.772010037470285e-05, 'epoch': 1.336793977517829, 'step': 1112500}
INFO:transformers.trainer:{'loss': 2.4745667543411254, 'learning_rate': 2.7710086936669012e-05, 'epoch': 1.3373947837998594, 'step': 1113000}
INFO:transformers.trainer:{'loss': 2.5051206970214843, 'learning_rate': 2.770007349863517e-05, 'epoch': 1.3379955900818898, 'step': 1113500}
INFO:transformers.trainer:{'loss': 2.4715385690927505, 'learning_rate': 2.769006006060133e-05, 'epoch': 1.3385963963639202, 'step': 1114000}
INFO:transformers.trainer:{'loss': 2.4803895688056947, 'learning_rate': 2.7680046622567483e-05, 'epoch': 1.3391972026459509, 'step': 1114500}
INFO:transformers.trainer:{'loss': 2.468746253490448, 'learning_rate': 2.7670033184533644e-05, 'epoch': 1.3397980089279813, 'step': 1115000}
INFO:transformers.trainer:{'loss': 2.4591928820610045, 'learning_rate': 2.7660019746499804e-05, 'epoch': 1.340398815210012, 'step': 1115500}
INFO:transformers.trainer:{'loss': 2.493814237833023, 'learning_rate': 2.7650006308465965e-05, 'epoch': 1.3409996214920423, 'step': 1116000}
INFO:transformers.trainer:{'loss': 2.478965884923935, 'learning_rate': 2.763999287043212e-05, 'epoch': 1.3416004277740727, 'step': 1116500}
INFO:transformers.trainer:{'loss': 2.4259929381608965, 'learning_rate': 2.762997943239828e-05, 'epoch': 1.3422012340561034, 'step': 1117000}
INFO:transformers.trainer:{'loss': 2.3660569363236426, 'learning_rate': 2.761996599436444e-05, 'epoch': 1.3428020403381338, 'step': 1117500}
INFO:transformers.trainer:{'loss': 2.4950034819841385, 'learning_rate': 2.76099525563306e-05, 'epoch': 1.3434028466201642, 'step': 1118000}
INFO:transformers.trainer:{'loss': 2.4334870080947875, 'learning_rate': 2.7599939118296757e-05, 'epoch': 1.3440036529021948, 'step': 1118500}
INFO:transformers.trainer:{'loss': 2.5385014345645907, 'learning_rate': 2.7589925680262914e-05, 'epoch': 1.3446044591842252, 'step': 1119000}
INFO:transformers.trainer:{'loss': 2.46481815636158, 'learning_rate': 2.757991224222907e-05, 'epoch': 1.3452052654662556, 'step': 1119500}
INFO:transformers.trainer:{'loss': 2.4315764690637587, 'learning_rate': 2.756989880419523e-05, 'epoch': 1.3458060717482863, 'step': 1120000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1120000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1120000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1120000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1100000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.452235852241516, 'learning_rate': 2.7559885366161392e-05, 'epoch': 1.3464068780303167, 'step': 1120500}
INFO:transformers.trainer:{'loss': 2.4547521543502806, 'learning_rate': 2.7549871928127545e-05, 'epoch': 1.3470076843123473, 'step': 1121000}
INFO:transformers.trainer:{'loss': 2.474840847015381, 'learning_rate': 2.7539858490093706e-05, 'epoch': 1.3476084905943777, 'step': 1121500}
INFO:transformers.trainer:{'loss': 2.465701213479042, 'learning_rate': 2.7529845052059866e-05, 'epoch': 1.3482092968764081, 'step': 1122000}
INFO:transformers.trainer:{'loss': 2.490975120663643, 'learning_rate': 2.7519831614026027e-05, 'epoch': 1.3488101031584385, 'step': 1122500}
INFO:transformers.trainer:{'loss': 2.4427576602697374, 'learning_rate': 2.750981817599218e-05, 'epoch': 1.3494109094404692, 'step': 1123000}
INFO:transformers.trainer:{'loss': 2.493010444879532, 'learning_rate': 2.749980473795834e-05, 'epoch': 1.3500117157224996, 'step': 1123500}
INFO:transformers.trainer:{'loss': 2.4727235845327376, 'learning_rate': 2.74897912999245e-05, 'epoch': 1.3506125220045302, 'step': 1124000}
INFO:transformers.trainer:{'loss': 2.479970836639404, 'learning_rate': 2.747977786189066e-05, 'epoch': 1.3512133282865606, 'step': 1124500}
INFO:transformers.trainer:{'loss': 2.4302138966321944, 'learning_rate': 2.746976442385682e-05, 'epoch': 1.351814134568591, 'step': 1125000}
INFO:transformers.trainer:{'loss': 2.4802335678339005, 'learning_rate': 2.7459750985822973e-05, 'epoch': 1.3524149408506214, 'step': 1125500}
INFO:transformers.trainer:{'loss': 2.496844890475273, 'learning_rate': 2.7449737547789133e-05, 'epoch': 1.353015747132652, 'step': 1126000}
INFO:transformers.trainer:{'loss': 2.4613232276439665, 'learning_rate': 2.7439724109755294e-05, 'epoch': 1.3536165534146825, 'step': 1126500}
INFO:transformers.trainer:{'loss': 2.462903317332268, 'learning_rate': 2.7429710671721454e-05, 'epoch': 1.354217359696713, 'step': 1127000}
INFO:transformers.trainer:{'loss': 2.4522611405849455, 'learning_rate': 2.7419697233687608e-05, 'epoch': 1.3548181659787435, 'step': 1127500}
INFO:transformers.trainer:{'loss': 2.444096983551979, 'learning_rate': 2.7409683795653768e-05, 'epoch': 1.355418972260774, 'step': 1128000}
INFO:transformers.trainer:{'loss': 2.4901172652244568, 'learning_rate': 2.739967035761993e-05, 'epoch': 1.3560197785428043, 'step': 1128500}
INFO:transformers.trainer:{'loss': 2.493734192252159, 'learning_rate': 2.738965691958609e-05, 'epoch': 1.356620584824835, 'step': 1129000}
INFO:transformers.trainer:{'loss': 2.445416061878204, 'learning_rate': 2.7379643481552243e-05, 'epoch': 1.3572213911068653, 'step': 1129500}
INFO:transformers.trainer:{'loss': 2.4216408002376557, 'learning_rate': 2.7369630043518403e-05, 'epoch': 1.357822197388896, 'step': 1130000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1130000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1130000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1130000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1110000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.433858502626419, 'learning_rate': 2.735961660548456e-05, 'epoch': 1.3584230036709264, 'step': 1130500}
INFO:transformers.trainer:{'loss': 2.446022804439068, 'learning_rate': 2.734960316745072e-05, 'epoch': 1.3590238099529568, 'step': 1131000}
INFO:transformers.trainer:{'loss': 2.4745643447637558, 'learning_rate': 2.733958972941688e-05, 'epoch': 1.3596246162349874, 'step': 1131500}
INFO:transformers.trainer:{'loss': 2.5093608778715133, 'learning_rate': 2.7329576291383035e-05, 'epoch': 1.3602254225170178, 'step': 1132000}
INFO:transformers.trainer:{'loss': 2.5076203548908236, 'learning_rate': 2.7319562853349195e-05, 'epoch': 1.3608262287990482, 'step': 1132500}
INFO:transformers.trainer:{'loss': 2.48520139837265, 'learning_rate': 2.7309549415315356e-05, 'epoch': 1.3614270350810789, 'step': 1133000}
INFO:transformers.trainer:{'loss': 2.4538786096572878, 'learning_rate': 2.7299535977281516e-05, 'epoch': 1.3620278413631093, 'step': 1133500}
INFO:transformers.trainer:{'loss': 2.4631545922756195, 'learning_rate': 2.728952253924767e-05, 'epoch': 1.3626286476451397, 'step': 1134000}
INFO:transformers.trainer:{'loss': 2.4408330010175705, 'learning_rate': 2.727950910121383e-05, 'epoch': 1.3632294539271703, 'step': 1134500}
INFO:transformers.trainer:{'loss': 2.462601774215698, 'learning_rate': 2.726949566317999e-05, 'epoch': 1.3638302602092007, 'step': 1135000}
INFO:transformers.trainer:{'loss': 2.4433811367750167, 'learning_rate': 2.7259482225146148e-05, 'epoch': 1.3644310664912314, 'step': 1135500}
INFO:transformers.trainer:{'loss': 2.4379167318344117, 'learning_rate': 2.7249468787112305e-05, 'epoch': 1.3650318727732618, 'step': 1136000}
INFO:transformers.trainer:{'loss': 2.4358231736421585, 'learning_rate': 2.7239455349078462e-05, 'epoch': 1.3656326790552922, 'step': 1136500}
INFO:transformers.trainer:{'loss': 2.512796611785889, 'learning_rate': 2.7229441911044622e-05, 'epoch': 1.3662334853373226, 'step': 1137000}
INFO:transformers.trainer:{'loss': 2.441321871161461, 'learning_rate': 2.7219428473010783e-05, 'epoch': 1.3668342916193532, 'step': 1137500}
INFO:transformers.trainer:{'loss': 2.520096798121929, 'learning_rate': 2.7209415034976943e-05, 'epoch': 1.3674350979013836, 'step': 1138000}
INFO:transformers.trainer:{'loss': 2.50399267911911, 'learning_rate': 2.7199401596943097e-05, 'epoch': 1.3680359041834143, 'step': 1138500}
INFO:transformers.trainer:{'loss': 2.462302621483803, 'learning_rate': 2.7189388158909257e-05, 'epoch': 1.3686367104654447, 'step': 1139000}
INFO:transformers.trainer:{'loss': 2.4769199578762056, 'learning_rate': 2.7179374720875418e-05, 'epoch': 1.369237516747475, 'step': 1139500}
INFO:transformers.trainer:{'loss': 2.4479242980480196, 'learning_rate': 2.7169361282841578e-05, 'epoch': 1.3698383230295055, 'step': 1140000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1140000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1140000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1140000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1120000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.391144059062004, 'learning_rate': 2.7159347844807732e-05, 'epoch': 1.370439129311536, 'step': 1140500}
INFO:transformers.trainer:{'loss': 2.477321526110172, 'learning_rate': 2.7149334406773892e-05, 'epoch': 1.3710399355935665, 'step': 1141000}
INFO:transformers.trainer:{'loss': 2.4377799285650252, 'learning_rate': 2.713932096874005e-05, 'epoch': 1.3716407418755971, 'step': 1141500}
INFO:transformers.trainer:{'loss': 2.4502212281227114, 'learning_rate': 2.712930753070621e-05, 'epoch': 1.3722415481576276, 'step': 1142000}
INFO:transformers.trainer:{'loss': 2.426673355937004, 'learning_rate': 2.7119294092672364e-05, 'epoch': 1.372842354439658, 'step': 1142500}
INFO:transformers.trainer:{'loss': 2.44251102155447, 'learning_rate': 2.7109280654638524e-05, 'epoch': 1.3734431607216884, 'step': 1143000}
INFO:transformers.trainer:{'loss': 2.4280254957675935, 'learning_rate': 2.7099267216604685e-05, 'epoch': 1.374043967003719, 'step': 1143500}
INFO:transformers.trainer:{'loss': 2.5252835606336594, 'learning_rate': 2.7089253778570845e-05, 'epoch': 1.3746447732857494, 'step': 1144000}
INFO:transformers.trainer:{'loss': 2.513698575973511, 'learning_rate': 2.7079240340537005e-05, 'epoch': 1.37524557956778, 'step': 1144500}
INFO:transformers.trainer:{'loss': 2.4762687278985975, 'learning_rate': 2.706922690250316e-05, 'epoch': 1.3758463858498104, 'step': 1145000}
INFO:transformers.trainer:{'loss': 2.4070140154361725, 'learning_rate': 2.705921346446932e-05, 'epoch': 1.3764471921318409, 'step': 1145500}
INFO:transformers.trainer:{'loss': 2.4621884537935257, 'learning_rate': 2.704920002643548e-05, 'epoch': 1.3770479984138715, 'step': 1146000}
INFO:transformers.trainer:{'loss': 2.447594319820404, 'learning_rate': 2.7039186588401637e-05, 'epoch': 1.377648804695902, 'step': 1146500}
INFO:transformers.trainer:{'loss': 2.4398562161922457, 'learning_rate': 2.7029173150367794e-05, 'epoch': 1.3782496109779323, 'step': 1147000}
INFO:transformers.trainer:{'loss': 2.4459382580518723, 'learning_rate': 2.701915971233395e-05, 'epoch': 1.378850417259963, 'step': 1147500}
INFO:transformers.trainer:{'loss': 2.4706776411533355, 'learning_rate': 2.700914627430011e-05, 'epoch': 1.3794512235419933, 'step': 1148000}
INFO:transformers.trainer:{'loss': 2.4573327677249908, 'learning_rate': 2.6999132836266272e-05, 'epoch': 1.3800520298240238, 'step': 1148500}
INFO:transformers.trainer:{'loss': 2.510238179445267, 'learning_rate': 2.6989119398232433e-05, 'epoch': 1.3806528361060544, 'step': 1149000}
INFO:transformers.trainer:{'loss': 2.4784136354923247, 'learning_rate': 2.6979105960198586e-05, 'epoch': 1.3812536423880848, 'step': 1149500}
INFO:transformers.trainer:{'loss': 2.4484373651742937, 'learning_rate': 2.6969092522164747e-05, 'epoch': 1.3818544486701154, 'step': 1150000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1150000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1150000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1150000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1130000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3782094569206236, 'learning_rate': 2.6959079084130907e-05, 'epoch': 1.3824552549521458, 'step': 1150500}
INFO:transformers.trainer:{'loss': 2.478758378148079, 'learning_rate': 2.6949065646097064e-05, 'epoch': 1.3830560612341762, 'step': 1151000}
INFO:transformers.trainer:{'loss': 2.5008832397460936, 'learning_rate': 2.693905220806322e-05, 'epoch': 1.3836568675162066, 'step': 1151500}
INFO:transformers.trainer:{'loss': 2.4505592786073684, 'learning_rate': 2.6929038770029382e-05, 'epoch': 1.3842576737982373, 'step': 1152000}
INFO:transformers.trainer:{'loss': 2.4873921444416047, 'learning_rate': 2.691902533199554e-05, 'epoch': 1.3848584800802677, 'step': 1152500}
INFO:transformers.trainer:{'loss': 2.4852593343257903, 'learning_rate': 2.69090118939617e-05, 'epoch': 1.3854592863622983, 'step': 1153000}
INFO:transformers.trainer:{'loss': 2.469525333762169, 'learning_rate': 2.6898998455927853e-05, 'epoch': 1.3860600926443287, 'step': 1153500}
INFO:transformers.trainer:{'loss': 2.4479928684234618, 'learning_rate': 2.6888985017894013e-05, 'epoch': 1.3866608989263591, 'step': 1154000}
INFO:transformers.trainer:{'loss': 2.4772658340930938, 'learning_rate': 2.6878971579860174e-05, 'epoch': 1.3872617052083895, 'step': 1154500}
INFO:transformers.trainer:{'loss': 2.537740015387535, 'learning_rate': 2.6868958141826334e-05, 'epoch': 1.3878625114904202, 'step': 1155000}
INFO:transformers.trainer:{'loss': 2.455073642492294, 'learning_rate': 2.6858944703792495e-05, 'epoch': 1.3884633177724506, 'step': 1155500}
INFO:transformers.trainer:{'loss': 2.5032819279432297, 'learning_rate': 2.684893126575865e-05, 'epoch': 1.3890641240544812, 'step': 1156000}
INFO:transformers.trainer:{'loss': 2.474052241563797, 'learning_rate': 2.683891782772481e-05, 'epoch': 1.3896649303365116, 'step': 1156500}
INFO:transformers.trainer:{'loss': 2.421909051299095, 'learning_rate': 2.6828904389690966e-05, 'epoch': 1.390265736618542, 'step': 1157000}
INFO:transformers.trainer:{'loss': 2.428483236908913, 'learning_rate': 2.6818890951657126e-05, 'epoch': 1.3908665429005724, 'step': 1157500}
INFO:transformers.trainer:{'loss': 2.519769134640694, 'learning_rate': 2.680887751362328e-05, 'epoch': 1.391467349182603, 'step': 1158000}
INFO:transformers.trainer:{'loss': 2.4814060781002043, 'learning_rate': 2.679886407558944e-05, 'epoch': 1.3920681554646335, 'step': 1158500}
INFO:transformers.trainer:{'loss': 2.488883479952812, 'learning_rate': 2.67888506375556e-05, 'epoch': 1.392668961746664, 'step': 1159000}
INFO:transformers.trainer:{'loss': 2.494629853725433, 'learning_rate': 2.677883719952176e-05, 'epoch': 1.3932697680286945, 'step': 1159500}
INFO:transformers.trainer:{'loss': 2.419957481145859, 'learning_rate': 2.6768823761487915e-05, 'epoch': 1.393870574310725, 'step': 1160000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1160000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1160000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1160000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1140000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4966169252991675, 'learning_rate': 2.6758810323454076e-05, 'epoch': 1.3944713805927555, 'step': 1160500}
INFO:transformers.trainer:{'loss': 2.4080507925748824, 'learning_rate': 2.6748796885420236e-05, 'epoch': 1.395072186874786, 'step': 1161000}
INFO:transformers.trainer:{'loss': 2.390374415040016, 'learning_rate': 2.6738783447386396e-05, 'epoch': 1.3956729931568164, 'step': 1161500}
INFO:transformers.trainer:{'loss': 2.451720141649246, 'learning_rate': 2.6728770009352553e-05, 'epoch': 1.396273799438847, 'step': 1162000}
INFO:transformers.trainer:{'loss': 2.4885762868523598, 'learning_rate': 2.671875657131871e-05, 'epoch': 1.3968746057208774, 'step': 1162500}
INFO:transformers.trainer:{'loss': 2.4072340641021728, 'learning_rate': 2.6708743133284868e-05, 'epoch': 1.3974754120029078, 'step': 1163000}
INFO:transformers.trainer:{'loss': 2.4247754666805266, 'learning_rate': 2.6698729695251028e-05, 'epoch': 1.3980762182849384, 'step': 1163500}
INFO:transformers.trainer:{'loss': 2.445269677042961, 'learning_rate': 2.668871625721719e-05, 'epoch': 1.3986770245669689, 'step': 1164000}
INFO:transformers.trainer:{'loss': 2.4470735251903535, 'learning_rate': 2.6678702819183342e-05, 'epoch': 1.3992778308489995, 'step': 1164500}
INFO:transformers.trainer:{'loss': 2.493292420387268, 'learning_rate': 2.6668689381149503e-05, 'epoch': 1.39987863713103, 'step': 1165000}
INFO:transformers.trainer:{'loss': 2.4633580273389817, 'learning_rate': 2.6658675943115663e-05, 'epoch': 1.4004794434130603, 'step': 1165500}
INFO:transformers.trainer:{'loss': 2.466065392255783, 'learning_rate': 2.6648662505081824e-05, 'epoch': 1.4010802496950907, 'step': 1166000}
INFO:transformers.trainer:{'loss': 2.456680718779564, 'learning_rate': 2.6638649067047977e-05, 'epoch': 1.4016810559771213, 'step': 1166500}
INFO:transformers.trainer:{'loss': 2.473483390212059, 'learning_rate': 2.6628635629014138e-05, 'epoch': 1.4022818622591517, 'step': 1167000}
INFO:transformers.trainer:{'loss': 2.4868506963253023, 'learning_rate': 2.6618622190980298e-05, 'epoch': 1.4028826685411824, 'step': 1167500}
INFO:transformers.trainer:{'loss': 2.476560786008835, 'learning_rate': 2.6608608752946455e-05, 'epoch': 1.4034834748232128, 'step': 1168000}
INFO:transformers.trainer:{'loss': 2.5236739161014556, 'learning_rate': 2.6598595314912616e-05, 'epoch': 1.4040842811052432, 'step': 1168500}
INFO:transformers.trainer:{'loss': 2.477378882944584, 'learning_rate': 2.658858187687877e-05, 'epoch': 1.4046850873872736, 'step': 1169000}
INFO:transformers.trainer:{'loss': 2.4129964784383775, 'learning_rate': 2.657856843884493e-05, 'epoch': 1.4052858936693042, 'step': 1169500}
INFO:transformers.trainer:{'loss': 2.464185458779335, 'learning_rate': 2.656855500081109e-05, 'epoch': 1.4058866999513346, 'step': 1170000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1170000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1170000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1170000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1150000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4499754774570466, 'learning_rate': 2.655854156277725e-05, 'epoch': 1.4064875062333653, 'step': 1170500}
INFO:transformers.trainer:{'loss': 2.450173180222511, 'learning_rate': 2.6548528124743404e-05, 'epoch': 1.4070883125153957, 'step': 1171000}
INFO:transformers.trainer:{'loss': 2.469542755007744, 'learning_rate': 2.6538514686709565e-05, 'epoch': 1.407689118797426, 'step': 1171500}
INFO:transformers.trainer:{'loss': 2.4454527204036713, 'learning_rate': 2.6528501248675725e-05, 'epoch': 1.4082899250794565, 'step': 1172000}
INFO:transformers.trainer:{'loss': 2.4792597649097443, 'learning_rate': 2.6518487810641886e-05, 'epoch': 1.4088907313614871, 'step': 1172500}
INFO:transformers.trainer:{'loss': 2.4822783378362656, 'learning_rate': 2.650847437260804e-05, 'epoch': 1.4094915376435175, 'step': 1173000}
INFO:transformers.trainer:{'loss': 2.447528426170349, 'learning_rate': 2.64984609345742e-05, 'epoch': 1.4100923439255482, 'step': 1173500}
INFO:transformers.trainer:{'loss': 2.3955334251523017, 'learning_rate': 2.6488447496540357e-05, 'epoch': 1.4106931502075786, 'step': 1174000}
INFO:transformers.trainer:{'loss': 2.472036108016968, 'learning_rate': 2.6478434058506517e-05, 'epoch': 1.411293956489609, 'step': 1174500}
INFO:transformers.trainer:{'loss': 2.478745488524437, 'learning_rate': 2.6468420620472678e-05, 'epoch': 1.4118947627716396, 'step': 1175000}
INFO:transformers.trainer:{'loss': 2.4718356058597566, 'learning_rate': 2.645840718243883e-05, 'epoch': 1.41249556905367, 'step': 1175500}
INFO:transformers.trainer:{'loss': 2.480749214053154, 'learning_rate': 2.6448393744404992e-05, 'epoch': 1.4130963753357004, 'step': 1176000}
INFO:transformers.trainer:{'loss': 2.4008777880072594, 'learning_rate': 2.6438380306371152e-05, 'epoch': 1.413697181617731, 'step': 1176500}
INFO:transformers.trainer:{'loss': 2.4893618831634523, 'learning_rate': 2.6428366868337313e-05, 'epoch': 1.4142979878997615, 'step': 1177000}
INFO:transformers.trainer:{'loss': 2.4348456392288207, 'learning_rate': 2.6418353430303467e-05, 'epoch': 1.4148987941817919, 'step': 1177500}
INFO:transformers.trainer:{'loss': 2.465528200507164, 'learning_rate': 2.6408339992269627e-05, 'epoch': 1.4154996004638225, 'step': 1178000}
INFO:transformers.trainer:{'loss': 2.4969748066663744, 'learning_rate': 2.6398326554235787e-05, 'epoch': 1.416100406745853, 'step': 1178500}
INFO:transformers.trainer:{'loss': 2.4447696167826654, 'learning_rate': 2.6388313116201944e-05, 'epoch': 1.4167012130278835, 'step': 1179000}
INFO:transformers.trainer:{'loss': 2.494078745484352, 'learning_rate': 2.63782996781681e-05, 'epoch': 1.417302019309914, 'step': 1179500}
INFO:transformers.trainer:{'loss': 2.40782182765007, 'learning_rate': 2.636828624013426e-05, 'epoch': 1.4179028255919444, 'step': 1180000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1180000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1180000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1180000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1160000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4602965461015702, 'learning_rate': 2.635827280210042e-05, 'epoch': 1.4185036318739748, 'step': 1180500}
INFO:transformers.trainer:{'loss': 2.4454310935735704, 'learning_rate': 2.634825936406658e-05, 'epoch': 1.4191044381560054, 'step': 1181000}
INFO:transformers.trainer:{'loss': 2.466877601623535, 'learning_rate': 2.633824592603274e-05, 'epoch': 1.4197052444380358, 'step': 1181500}
INFO:transformers.trainer:{'loss': 2.417083814620972, 'learning_rate': 2.6328232487998894e-05, 'epoch': 1.4203060507200664, 'step': 1182000}
INFO:transformers.trainer:{'loss': 2.4653257623910902, 'learning_rate': 2.6318219049965054e-05, 'epoch': 1.4209068570020968, 'step': 1182500}
INFO:transformers.trainer:{'loss': 2.4664297651052474, 'learning_rate': 2.6308205611931215e-05, 'epoch': 1.4215076632841273, 'step': 1183000}
INFO:transformers.trainer:{'loss': 2.4376660871505735, 'learning_rate': 2.6298192173897375e-05, 'epoch': 1.4221084695661577, 'step': 1183500}
INFO:transformers.trainer:{'loss': 2.4478771350383757, 'learning_rate': 2.628817873586353e-05, 'epoch': 1.4227092758481883, 'step': 1184000}
INFO:transformers.trainer:{'loss': 2.414800229668617, 'learning_rate': 2.627816529782969e-05, 'epoch': 1.4233100821302187, 'step': 1184500}
INFO:transformers.trainer:{'loss': 2.4447407692670824, 'learning_rate': 2.6268151859795846e-05, 'epoch': 1.4239108884122493, 'step': 1185000}
INFO:transformers.trainer:{'loss': 2.4757746367454527, 'learning_rate': 2.6258138421762007e-05, 'epoch': 1.4245116946942797, 'step': 1185500}
INFO:transformers.trainer:{'loss': 2.4907376613616945, 'learning_rate': 2.624812498372816e-05, 'epoch': 1.4251125009763101, 'step': 1186000}
INFO:transformers.trainer:{'loss': 2.4656297957897184, 'learning_rate': 2.623811154569432e-05, 'epoch': 1.4257133072583406, 'step': 1186500}
INFO:transformers.trainer:{'loss': 2.410886221051216, 'learning_rate': 2.622809810766048e-05, 'epoch': 1.4263141135403712, 'step': 1187000}
INFO:transformers.trainer:{'loss': 2.54318776512146, 'learning_rate': 2.621808466962664e-05, 'epoch': 1.4269149198224016, 'step': 1187500}
INFO:transformers.trainer:{'loss': 2.5219729949235914, 'learning_rate': 2.6208071231592802e-05, 'epoch': 1.4275157261044322, 'step': 1188000}
INFO:transformers.trainer:{'loss': 2.4199993844032286, 'learning_rate': 2.6198057793558956e-05, 'epoch': 1.4281165323864626, 'step': 1188500}
INFO:transformers.trainer:{'loss': 2.455613580107689, 'learning_rate': 2.6188044355525116e-05, 'epoch': 1.428717338668493, 'step': 1189000}
INFO:transformers.trainer:{'loss': 2.4710948128700254, 'learning_rate': 2.6178030917491277e-05, 'epoch': 1.4293181449505237, 'step': 1189500}
INFO:transformers.trainer:{'loss': 2.4056949189305303, 'learning_rate': 2.6168017479457434e-05, 'epoch': 1.429918951232554, 'step': 1190000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1190000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1190000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1190000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1170000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5187429149150846, 'learning_rate': 2.615800404142359e-05, 'epoch': 1.4305197575145847, 'step': 1190500}
INFO:transformers.trainer:{'loss': 2.4667079422473908, 'learning_rate': 2.6147990603389748e-05, 'epoch': 1.4311205637966151, 'step': 1191000}
INFO:transformers.trainer:{'loss': 2.416611449599266, 'learning_rate': 2.613797716535591e-05, 'epoch': 1.4317213700786455, 'step': 1191500}
INFO:transformers.trainer:{'loss': 2.4891255106925962, 'learning_rate': 2.612796372732207e-05, 'epoch': 1.432322176360676, 'step': 1192000}
INFO:transformers.trainer:{'loss': 2.486121199965477, 'learning_rate': 2.6117950289288222e-05, 'epoch': 1.4329229826427066, 'step': 1192500}
INFO:transformers.trainer:{'loss': 2.44703141784668, 'learning_rate': 2.6107936851254383e-05, 'epoch': 1.433523788924737, 'step': 1193000}
INFO:transformers.trainer:{'loss': 2.467957342505455, 'learning_rate': 2.6097923413220543e-05, 'epoch': 1.4341245952067676, 'step': 1193500}
INFO:transformers.trainer:{'loss': 2.5097223695516586, 'learning_rate': 2.6087909975186704e-05, 'epoch': 1.434725401488798, 'step': 1194000}
INFO:transformers.trainer:{'loss': 2.4955373315811156, 'learning_rate': 2.6077896537152864e-05, 'epoch': 1.4353262077708284, 'step': 1194500}
INFO:transformers.trainer:{'loss': 2.469757738232613, 'learning_rate': 2.6067883099119018e-05, 'epoch': 1.4359270140528588, 'step': 1195000}
INFO:transformers.trainer:{'loss': 2.43874194586277, 'learning_rate': 2.605786966108518e-05, 'epoch': 1.4365278203348895, 'step': 1195500}
INFO:transformers.trainer:{'loss': 2.40702378320694, 'learning_rate': 2.6047856223051335e-05, 'epoch': 1.4371286266169199, 'step': 1196000}
INFO:transformers.trainer:{'loss': 2.412990716457367, 'learning_rate': 2.6037842785017496e-05, 'epoch': 1.4377294328989505, 'step': 1196500}
INFO:transformers.trainer:{'loss': 2.4632462602853775, 'learning_rate': 2.602782934698365e-05, 'epoch': 1.438330239180981, 'step': 1197000}
INFO:transformers.trainer:{'loss': 2.449583123803139, 'learning_rate': 2.601781590894981e-05, 'epoch': 1.4389310454630113, 'step': 1197500}
INFO:transformers.trainer:{'loss': 2.435891532063484, 'learning_rate': 2.600780247091597e-05, 'epoch': 1.4395318517450417, 'step': 1198000}
INFO:transformers.trainer:{'loss': 2.457601511478424, 'learning_rate': 2.599778903288213e-05, 'epoch': 1.4401326580270724, 'step': 1198500}
INFO:transformers.trainer:{'loss': 2.4247641105651856, 'learning_rate': 2.598777559484829e-05, 'epoch': 1.4407334643091028, 'step': 1199000}
INFO:transformers.trainer:{'loss': 2.457438969254494, 'learning_rate': 2.5977762156814445e-05, 'epoch': 1.4413342705911334, 'step': 1199500}
INFO:transformers.trainer:{'loss': 2.4178185381293296, 'learning_rate': 2.5967748718780606e-05, 'epoch': 1.4419350768731638, 'step': 1200000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1200000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1200000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1200000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1180000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.5085002697706225, 'learning_rate': 2.5957735280746766e-05, 'epoch': 1.4425358831551942, 'step': 1200500}
INFO:transformers.trainer:{'loss': 2.4968564854860307, 'learning_rate': 2.5947721842712923e-05, 'epoch': 1.4431366894372246, 'step': 1201000}
INFO:transformers.trainer:{'loss': 2.452459369182587, 'learning_rate': 2.593770840467908e-05, 'epoch': 1.4437374957192552, 'step': 1201500}
INFO:transformers.trainer:{'loss': 2.4735926698446273, 'learning_rate': 2.5927694966645237e-05, 'epoch': 1.4443383020012857, 'step': 1202000}
INFO:transformers.trainer:{'loss': 2.441766569495201, 'learning_rate': 2.5917681528611398e-05, 'epoch': 1.4449391082833163, 'step': 1202500}
INFO:transformers.trainer:{'loss': 2.423333453297615, 'learning_rate': 2.5907668090577558e-05, 'epoch': 1.4455399145653467, 'step': 1203000}
INFO:transformers.trainer:{'loss': 2.4531282423734666, 'learning_rate': 2.5897654652543712e-05, 'epoch': 1.446140720847377, 'step': 1203500}
INFO:transformers.trainer:{'loss': 2.455467234134674, 'learning_rate': 2.5887641214509872e-05, 'epoch': 1.4467415271294077, 'step': 1204000}
INFO:transformers.trainer:{'loss': 2.4846305409669878, 'learning_rate': 2.5877627776476033e-05, 'epoch': 1.4473423334114381, 'step': 1204500}
INFO:transformers.trainer:{'loss': 2.4324480589032174, 'learning_rate': 2.5867614338442193e-05, 'epoch': 1.4479431396934688, 'step': 1205000}
INFO:transformers.trainer:{'loss': 2.435570011973381, 'learning_rate': 2.5857600900408354e-05, 'epoch': 1.4485439459754992, 'step': 1205500}
INFO:transformers.trainer:{'loss': 2.4495222771167757, 'learning_rate': 2.5847587462374507e-05, 'epoch': 1.4491447522575296, 'step': 1206000}
INFO:transformers.trainer:{'loss': 2.501442937016487, 'learning_rate': 2.5837574024340668e-05, 'epoch': 1.44974555853956, 'step': 1206500}
INFO:transformers.trainer:{'loss': 2.4862406942844393, 'learning_rate': 2.5827560586306825e-05, 'epoch': 1.4503463648215906, 'step': 1207000}
INFO:transformers.trainer:{'loss': 2.412334751486778, 'learning_rate': 2.5817547148272985e-05, 'epoch': 1.450947171103621, 'step': 1207500}
INFO:transformers.trainer:{'loss': 2.4532379727363587, 'learning_rate': 2.580753371023914e-05, 'epoch': 1.4515479773856517, 'step': 1208000}
INFO:transformers.trainer:{'loss': 2.5071173417568207, 'learning_rate': 2.57975202722053e-05, 'epoch': 1.452148783667682, 'step': 1208500}
INFO:transformers.trainer:{'loss': 2.4937454850673677, 'learning_rate': 2.578750683417146e-05, 'epoch': 1.4527495899497125, 'step': 1209000}
INFO:transformers.trainer:{'loss': 2.443170847892761, 'learning_rate': 2.577749339613762e-05, 'epoch': 1.453350396231743, 'step': 1209500}
INFO:transformers.trainer:{'loss': 2.4204544458389283, 'learning_rate': 2.5767479958103774e-05, 'epoch': 1.4539512025137735, 'step': 1210000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1210000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1210000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1210000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1190000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4377158659100533, 'learning_rate': 2.5757466520069934e-05, 'epoch': 1.454552008795804, 'step': 1210500}
INFO:transformers.trainer:{'loss': 2.4608702292442324, 'learning_rate': 2.5747453082036095e-05, 'epoch': 1.4551528150778346, 'step': 1211000}
INFO:transformers.trainer:{'loss': 2.4470465396642687, 'learning_rate': 2.5737439644002252e-05, 'epoch': 1.455753621359865, 'step': 1211500}
INFO:transformers.trainer:{'loss': 2.433942212224007, 'learning_rate': 2.5727426205968412e-05, 'epoch': 1.4563544276418954, 'step': 1212000}
INFO:transformers.trainer:{'loss': 2.4409859105348586, 'learning_rate': 2.571741276793457e-05, 'epoch': 1.4569552339239258, 'step': 1212500}
INFO:transformers.trainer:{'loss': 2.4442016078233717, 'learning_rate': 2.5707399329900727e-05, 'epoch': 1.4575560402059564, 'step': 1213000}
INFO:transformers.trainer:{'loss': 2.4598917181491853, 'learning_rate': 2.5697385891866887e-05, 'epoch': 1.4581568464879868, 'step': 1213500}
INFO:transformers.trainer:{'loss': 2.4074491411447525, 'learning_rate': 2.5687372453833047e-05, 'epoch': 1.4587576527700175, 'step': 1214000}
INFO:transformers.trainer:{'loss': 2.448980842590332, 'learning_rate': 2.56773590157992e-05, 'epoch': 1.4593584590520479, 'step': 1214500}
INFO:transformers.trainer:{'loss': 2.4034196900129317, 'learning_rate': 2.566734557776536e-05, 'epoch': 1.4599592653340783, 'step': 1215000}
INFO:transformers.trainer:{'loss': 2.4421272669434546, 'learning_rate': 2.5657332139731522e-05, 'epoch': 1.4605600716161087, 'step': 1215500}
INFO:transformers.trainer:{'loss': 2.4572963804006576, 'learning_rate': 2.5647318701697682e-05, 'epoch': 1.4611608778981393, 'step': 1216000}
INFO:transformers.trainer:{'loss': 2.488541466116905, 'learning_rate': 2.5637305263663836e-05, 'epoch': 1.4617616841801697, 'step': 1216500}
INFO:transformers.trainer:{'loss': 2.4198123646974565, 'learning_rate': 2.5627291825629997e-05, 'epoch': 1.4623624904622003, 'step': 1217000}
INFO:transformers.trainer:{'loss': 2.3974634644985198, 'learning_rate': 2.5617278387596154e-05, 'epoch': 1.4629632967442308, 'step': 1217500}
INFO:transformers.trainer:{'loss': 2.384548843681812, 'learning_rate': 2.5607264949562314e-05, 'epoch': 1.4635641030262612, 'step': 1218000}
INFO:transformers.trainer:{'loss': 2.447643714785576, 'learning_rate': 2.5597251511528475e-05, 'epoch': 1.4641649093082918, 'step': 1218500}
INFO:transformers.trainer:{'loss': 2.4510336196422577, 'learning_rate': 2.5587238073494628e-05, 'epoch': 1.4647657155903222, 'step': 1219000}
INFO:transformers.trainer:{'loss': 2.4629908769130706, 'learning_rate': 2.557722463546079e-05, 'epoch': 1.4653665218723528, 'step': 1219500}
INFO:transformers.trainer:{'loss': 2.445986932516098, 'learning_rate': 2.556721119742695e-05, 'epoch': 1.4659673281543832, 'step': 1220000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1220000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1220000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1220000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1200000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4530248868465425, 'learning_rate': 2.555719775939311e-05, 'epoch': 1.4665681344364137, 'step': 1220500}
INFO:transformers.trainer:{'loss': 2.4253300468921664, 'learning_rate': 2.5547184321359263e-05, 'epoch': 1.467168940718444, 'step': 1221000}
INFO:transformers.trainer:{'loss': 2.4313654028177263, 'learning_rate': 2.5537170883325424e-05, 'epoch': 1.4677697470004747, 'step': 1221500}
INFO:transformers.trainer:{'loss': 2.4542841008901597, 'learning_rate': 2.5527157445291584e-05, 'epoch': 1.468370553282505, 'step': 1222000}
INFO:transformers.trainer:{'loss': 2.45594174516201, 'learning_rate': 2.551714400725774e-05, 'epoch': 1.4689713595645357, 'step': 1222500}
INFO:transformers.trainer:{'loss': 2.456619406342506, 'learning_rate': 2.5507130569223898e-05, 'epoch': 1.4695721658465661, 'step': 1223000}
INFO:transformers.trainer:{'loss': 2.380751767516136, 'learning_rate': 2.5497117131190055e-05, 'epoch': 1.4701729721285965, 'step': 1223500}
INFO:transformers.trainer:{'loss': 2.4918339680433275, 'learning_rate': 2.5487103693156216e-05, 'epoch': 1.470773778410627, 'step': 1224000}
INFO:transformers.trainer:{'loss': 2.403927956223488, 'learning_rate': 2.5477090255122376e-05, 'epoch': 1.4713745846926576, 'step': 1224500}
INFO:transformers.trainer:{'loss': 2.4827611199617388, 'learning_rate': 2.5467076817088537e-05, 'epoch': 1.471975390974688, 'step': 1225000}
INFO:transformers.trainer:{'loss': 2.4230754537582397, 'learning_rate': 2.545706337905469e-05, 'epoch': 1.4725761972567186, 'step': 1225500}
INFO:transformers.trainer:{'loss': 2.425115394592285, 'learning_rate': 2.544704994102085e-05, 'epoch': 1.473177003538749, 'step': 1226000}
INFO:transformers.trainer:{'loss': 2.4896589790582655, 'learning_rate': 2.543703650298701e-05, 'epoch': 1.4737778098207794, 'step': 1226500}
INFO:transformers.trainer:{'loss': 2.401386767268181, 'learning_rate': 2.5427023064953172e-05, 'epoch': 1.4743786161028098, 'step': 1227000}
INFO:transformers.trainer:{'loss': 2.4232371240854262, 'learning_rate': 2.5417009626919325e-05, 'epoch': 1.4749794223848405, 'step': 1227500}
INFO:transformers.trainer:{'loss': 2.454782230973244, 'learning_rate': 2.5406996188885486e-05, 'epoch': 1.4755802286668709, 'step': 1228000}
INFO:transformers.trainer:{'loss': 2.470054609775543, 'learning_rate': 2.5396982750851643e-05, 'epoch': 1.4761810349489015, 'step': 1228500}
INFO:transformers.trainer:{'loss': 2.4817812370061874, 'learning_rate': 2.5386969312817803e-05, 'epoch': 1.476781841230932, 'step': 1229000}
INFO:transformers.trainer:{'loss': 2.43519029545784, 'learning_rate': 2.5376955874783957e-05, 'epoch': 1.4773826475129623, 'step': 1229500}
INFO:transformers.trainer:{'loss': 2.41225845682621, 'learning_rate': 2.5366942436750118e-05, 'epoch': 1.477983453794993, 'step': 1230000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1230000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1230000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1230000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1210000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.456540048241615, 'learning_rate': 2.5356928998716278e-05, 'epoch': 1.4785842600770234, 'step': 1230500}
INFO:transformers.trainer:{'loss': 2.4292733688056467, 'learning_rate': 2.534691556068244e-05, 'epoch': 1.4791850663590538, 'step': 1231000}
INFO:transformers.trainer:{'loss': 2.3911109731197357, 'learning_rate': 2.53369021226486e-05, 'epoch': 1.4797858726410844, 'step': 1231500}
INFO:transformers.trainer:{'loss': 2.469021664083004, 'learning_rate': 2.5326888684614753e-05, 'epoch': 1.4803866789231148, 'step': 1232000}
INFO:transformers.trainer:{'loss': 2.4381155391931535, 'learning_rate': 2.5316875246580913e-05, 'epoch': 1.4809874852051452, 'step': 1232500}
INFO:transformers.trainer:{'loss': 2.4287236430644987, 'learning_rate': 2.5306861808547073e-05, 'epoch': 1.4815882914871759, 'step': 1233000}
INFO:transformers.trainer:{'loss': 2.443854335308075, 'learning_rate': 2.529684837051323e-05, 'epoch': 1.4821890977692063, 'step': 1233500}
INFO:transformers.trainer:{'loss': 2.4309301451444627, 'learning_rate': 2.5286834932479388e-05, 'epoch': 1.482789904051237, 'step': 1234000}
INFO:transformers.trainer:{'loss': 2.3849396476745603, 'learning_rate': 2.5276821494445545e-05, 'epoch': 1.4833907103332673, 'step': 1234500}
INFO:transformers.trainer:{'loss': 2.413324124932289, 'learning_rate': 2.5266808056411705e-05, 'epoch': 1.4839915166152977, 'step': 1235000}
INFO:transformers.trainer:{'loss': 2.429876519680023, 'learning_rate': 2.5256794618377866e-05, 'epoch': 1.4845923228973281, 'step': 1235500}
INFO:transformers.trainer:{'loss': 2.4624528926610947, 'learning_rate': 2.524678118034402e-05, 'epoch': 1.4851931291793588, 'step': 1236000}
INFO:transformers.trainer:{'loss': 2.461309537768364, 'learning_rate': 2.523676774231018e-05, 'epoch': 1.4857939354613892, 'step': 1236500}
INFO:transformers.trainer:{'loss': 2.430060301542282, 'learning_rate': 2.522675430427634e-05, 'epoch': 1.4863947417434198, 'step': 1237000}
INFO:transformers.trainer:{'loss': 2.417333351969719, 'learning_rate': 2.52167408662425e-05, 'epoch': 1.4869955480254502, 'step': 1237500}
INFO:transformers.trainer:{'loss': 2.4229347746372225, 'learning_rate': 2.520672742820866e-05, 'epoch': 1.4875963543074806, 'step': 1238000}
INFO:transformers.trainer:{'loss': 2.415579891204834, 'learning_rate': 2.5196713990174815e-05, 'epoch': 1.488197160589511, 'step': 1238500}
INFO:transformers.trainer:{'loss': 2.3901317987442017, 'learning_rate': 2.5186700552140975e-05, 'epoch': 1.4887979668715416, 'step': 1239000}
INFO:transformers.trainer:{'loss': 2.4350628225803375, 'learning_rate': 2.5176687114107132e-05, 'epoch': 1.489398773153572, 'step': 1239500}
INFO:transformers.trainer:{'loss': 2.4016022219657898, 'learning_rate': 2.5166673676073293e-05, 'epoch': 1.4899995794356027, 'step': 1240000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1240000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1240000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1240000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1220000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.452090749144554, 'learning_rate': 2.5156660238039446e-05, 'epoch': 1.490600385717633, 'step': 1240500}
INFO:transformers.trainer:{'loss': 2.395004164814949, 'learning_rate': 2.5146646800005607e-05, 'epoch': 1.4912011919996635, 'step': 1241000}
INFO:transformers.trainer:{'loss': 2.468948515176773, 'learning_rate': 2.5136633361971767e-05, 'epoch': 1.491801998281694, 'step': 1241500}
INFO:transformers.trainer:{'loss': 2.4076248505115507, 'learning_rate': 2.5126619923937928e-05, 'epoch': 1.4924028045637245, 'step': 1242000}
INFO:transformers.trainer:{'loss': 2.407217124462128, 'learning_rate': 2.511660648590408e-05, 'epoch': 1.493003610845755, 'step': 1242500}
INFO:transformers.trainer:{'loss': 2.400426064968109, 'learning_rate': 2.5106593047870242e-05, 'epoch': 1.4936044171277856, 'step': 1243000}
INFO:transformers.trainer:{'loss': 2.4340874071121217, 'learning_rate': 2.5096579609836402e-05, 'epoch': 1.494205223409816, 'step': 1243500}
INFO:transformers.trainer:{'loss': 2.4543177483081817, 'learning_rate': 2.5086566171802563e-05, 'epoch': 1.4948060296918464, 'step': 1244000}
INFO:transformers.trainer:{'loss': 2.3834717457294463, 'learning_rate': 2.507655273376872e-05, 'epoch': 1.495406835973877, 'step': 1244500}
INFO:transformers.trainer:{'loss': 2.421639874815941, 'learning_rate': 2.5066539295734877e-05, 'epoch': 1.4960076422559074, 'step': 1245000}
INFO:transformers.trainer:{'loss': 2.4076403158903124, 'learning_rate': 2.5056525857701034e-05, 'epoch': 1.4966084485379378, 'step': 1245500}
INFO:transformers.trainer:{'loss': 2.4643210877776145, 'learning_rate': 2.5046512419667194e-05, 'epoch': 1.4972092548199685, 'step': 1246000}
INFO:transformers.trainer:{'loss': 2.4984040966033936, 'learning_rate': 2.5036498981633355e-05, 'epoch': 1.4978100611019989, 'step': 1246500}
INFO:transformers.trainer:{'loss': 2.436629051923752, 'learning_rate': 2.502648554359951e-05, 'epoch': 1.4984108673840293, 'step': 1247000}
INFO:transformers.trainer:{'loss': 2.4136600399017336, 'learning_rate': 2.501647210556567e-05, 'epoch': 1.49901167366606, 'step': 1247500}
INFO:transformers.trainer:{'loss': 2.4293710139989853, 'learning_rate': 2.500645866753183e-05, 'epoch': 1.4996124799480903, 'step': 1248000}
INFO:transformers.trainer:{'loss': 2.4469207488298417, 'learning_rate': 2.4996445229497986e-05, 'epoch': 1.500213286230121, 'step': 1248500}
INFO:transformers.trainer:{'loss': 2.430479486107826, 'learning_rate': 2.4986431791464147e-05, 'epoch': 1.5008140925121514, 'step': 1249000}
INFO:transformers.trainer:{'loss': 2.3986403377056122, 'learning_rate': 2.4976418353430307e-05, 'epoch': 1.5014148987941818, 'step': 1249500}
INFO:transformers.trainer:{'loss': 2.4215110238790514, 'learning_rate': 2.4966404915396464e-05, 'epoch': 1.5020157050762122, 'step': 1250000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1250000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1250000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1250000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1230000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4003748497962953, 'learning_rate': 2.495639147736262e-05, 'epoch': 1.5026165113582428, 'step': 1250500}
INFO:transformers.trainer:{'loss': 2.480710807919502, 'learning_rate': 2.494637803932878e-05, 'epoch': 1.5032173176402732, 'step': 1251000}
INFO:transformers.trainer:{'loss': 2.375912377357483, 'learning_rate': 2.493636460129494e-05, 'epoch': 1.5038181239223039, 'step': 1251500}
INFO:transformers.trainer:{'loss': 2.465790729641914, 'learning_rate': 2.4926351163261096e-05, 'epoch': 1.5044189302043343, 'step': 1252000}
INFO:transformers.trainer:{'loss': 2.4068432471752166, 'learning_rate': 2.4916337725227257e-05, 'epoch': 1.5050197364863647, 'step': 1252500}
INFO:transformers.trainer:{'loss': 2.374860631465912, 'learning_rate': 2.4906324287193414e-05, 'epoch': 1.505620542768395, 'step': 1253000}
INFO:transformers.trainer:{'loss': 2.4265470625162124, 'learning_rate': 2.4896310849159574e-05, 'epoch': 1.5062213490504257, 'step': 1253500}
INFO:transformers.trainer:{'loss': 2.3836778385639192, 'learning_rate': 2.488629741112573e-05, 'epoch': 1.5068221553324561, 'step': 1254000}
INFO:transformers.trainer:{'loss': 2.4685348702669145, 'learning_rate': 2.487628397309189e-05, 'epoch': 1.5074229616144867, 'step': 1254500}
INFO:transformers.trainer:{'loss': 2.447533200740814, 'learning_rate': 2.486627053505805e-05, 'epoch': 1.5080237678965172, 'step': 1255000}
INFO:transformers.trainer:{'loss': 2.464744079947472, 'learning_rate': 2.485625709702421e-05, 'epoch': 1.5086245741785476, 'step': 1255500}
INFO:transformers.trainer:{'loss': 2.444027383685112, 'learning_rate': 2.4846243658990366e-05, 'epoch': 1.509225380460578, 'step': 1256000}
INFO:transformers.trainer:{'loss': 2.3984782645702363, 'learning_rate': 2.4836230220956523e-05, 'epoch': 1.5098261867426086, 'step': 1256500}
INFO:transformers.trainer:{'loss': 2.4242518612146378, 'learning_rate': 2.4826216782922684e-05, 'epoch': 1.5104269930246392, 'step': 1257000}
INFO:transformers.trainer:{'loss': 2.4301752507686616, 'learning_rate': 2.481620334488884e-05, 'epoch': 1.5110277993066696, 'step': 1257500}
INFO:transformers.trainer:{'loss': 2.4114511643648147, 'learning_rate': 2.4806189906855e-05, 'epoch': 1.5116286055887, 'step': 1258000}
INFO:transformers.trainer:{'loss': 2.4053506324291227, 'learning_rate': 2.4796176468821158e-05, 'epoch': 1.5122294118707305, 'step': 1258500}
INFO:transformers.trainer:{'loss': 2.443895196199417, 'learning_rate': 2.478616303078732e-05, 'epoch': 1.5128302181527609, 'step': 1259000}
INFO:transformers.trainer:{'loss': 2.4444444957971574, 'learning_rate': 2.4776149592753476e-05, 'epoch': 1.5134310244347915, 'step': 1259500}
INFO:transformers.trainer:{'loss': 2.407584258079529, 'learning_rate': 2.4766136154719636e-05, 'epoch': 1.5140318307168221, 'step': 1260000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1260000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1260000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1260000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1240000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4211344826221466, 'learning_rate': 2.4756122716685793e-05, 'epoch': 1.5146326369988525, 'step': 1260500}
INFO:transformers.trainer:{'loss': 2.4399051073789595, 'learning_rate': 2.4746109278651954e-05, 'epoch': 1.515233443280883, 'step': 1261000}
INFO:transformers.trainer:{'loss': 2.490494922041893, 'learning_rate': 2.473609584061811e-05, 'epoch': 1.5158342495629133, 'step': 1261500}
INFO:transformers.trainer:{'loss': 2.405442962050438, 'learning_rate': 2.4726082402584268e-05, 'epoch': 1.5164350558449438, 'step': 1262000}
INFO:transformers.trainer:{'loss': 2.4304807438850404, 'learning_rate': 2.471606896455043e-05, 'epoch': 1.5170358621269744, 'step': 1262500}
INFO:transformers.trainer:{'loss': 2.4692377109527586, 'learning_rate': 2.4706055526516585e-05, 'epoch': 1.517636668409005, 'step': 1263000}
INFO:transformers.trainer:{'loss': 2.4120117000341414, 'learning_rate': 2.4696042088482746e-05, 'epoch': 1.5182374746910354, 'step': 1263500}
INFO:transformers.trainer:{'loss': 2.426090815782547, 'learning_rate': 2.4686028650448903e-05, 'epoch': 1.5188382809730658, 'step': 1264000}
INFO:transformers.trainer:{'loss': 2.452948483467102, 'learning_rate': 2.4676015212415063e-05, 'epoch': 1.5194390872550962, 'step': 1264500}
INFO:transformers.trainer:{'loss': 2.427670988380909, 'learning_rate': 2.466600177438122e-05, 'epoch': 1.5200398935371269, 'step': 1265000}
INFO:transformers.trainer:{'loss': 2.4345293638706207, 'learning_rate': 2.465598833634738e-05, 'epoch': 1.5206406998191573, 'step': 1265500}
INFO:transformers.trainer:{'loss': 2.4517724142074586, 'learning_rate': 2.4645974898313538e-05, 'epoch': 1.521241506101188, 'step': 1266000}
INFO:transformers.trainer:{'loss': 2.429664904952049, 'learning_rate': 2.46359614602797e-05, 'epoch': 1.5218423123832183, 'step': 1266500}
INFO:transformers.trainer:{'loss': 2.432807013630867, 'learning_rate': 2.4625948022245855e-05, 'epoch': 1.5224431186652487, 'step': 1267000}
INFO:transformers.trainer:{'loss': 2.411051747560501, 'learning_rate': 2.4615934584212013e-05, 'epoch': 1.5230439249472791, 'step': 1267500}
INFO:transformers.trainer:{'loss': 2.4568375626802443, 'learning_rate': 2.4605921146178173e-05, 'epoch': 1.5236447312293098, 'step': 1268000}
INFO:transformers.trainer:{'loss': 2.464797931909561, 'learning_rate': 2.459590770814433e-05, 'epoch': 1.5242455375113402, 'step': 1268500}
INFO:transformers.trainer:{'loss': 2.389872419476509, 'learning_rate': 2.458589427011049e-05, 'epoch': 1.5248463437933708, 'step': 1269000}
INFO:transformers.trainer:{'loss': 2.4183442933261396, 'learning_rate': 2.4575880832076648e-05, 'epoch': 1.5254471500754012, 'step': 1269500}
INFO:transformers.trainer:{'loss': 2.4759855197668075, 'learning_rate': 2.4565867394042808e-05, 'epoch': 1.5260479563574316, 'step': 1270000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1270000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1270000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1270000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1250000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.404165701508522, 'learning_rate': 2.4555853956008965e-05, 'epoch': 1.526648762639462, 'step': 1270500}
INFO:transformers.trainer:{'loss': 2.456361475944519, 'learning_rate': 2.4545840517975126e-05, 'epoch': 1.5272495689214927, 'step': 1271000}
INFO:transformers.trainer:{'loss': 2.467859473824501, 'learning_rate': 2.4535827079941283e-05, 'epoch': 1.5278503752035233, 'step': 1271500}
INFO:transformers.trainer:{'loss': 2.451395056962967, 'learning_rate': 2.4525813641907443e-05, 'epoch': 1.5284511814855537, 'step': 1272000}
INFO:transformers.trainer:{'loss': 2.3962982983589174, 'learning_rate': 2.45158002038736e-05, 'epoch': 1.529051987767584, 'step': 1272500}
INFO:transformers.trainer:{'loss': 2.422660445213318, 'learning_rate': 2.4505786765839757e-05, 'epoch': 1.5296527940496145, 'step': 1273000}
INFO:transformers.trainer:{'loss': 2.398660822272301, 'learning_rate': 2.4495773327805914e-05, 'epoch': 1.530253600331645, 'step': 1273500}
INFO:transformers.trainer:{'loss': 2.413964523732662, 'learning_rate': 2.4485759889772075e-05, 'epoch': 1.5308544066136756, 'step': 1274000}
INFO:transformers.trainer:{'loss': 2.4249289680719377, 'learning_rate': 2.4475746451738235e-05, 'epoch': 1.5314552128957062, 'step': 1274500}
INFO:transformers.trainer:{'loss': 2.4045029826164246, 'learning_rate': 2.4465733013704392e-05, 'epoch': 1.5320560191777366, 'step': 1275000}
INFO:transformers.trainer:{'loss': 2.4503485810756684, 'learning_rate': 2.4455719575670553e-05, 'epoch': 1.532656825459767, 'step': 1275500}
INFO:transformers.trainer:{'loss': 2.417956346988678, 'learning_rate': 2.444570613763671e-05, 'epoch': 1.5332576317417974, 'step': 1276000}
INFO:transformers.trainer:{'loss': 2.474038366317749, 'learning_rate': 2.443569269960287e-05, 'epoch': 1.5338584380238278, 'step': 1276500}
INFO:transformers.trainer:{'loss': 2.451081209897995, 'learning_rate': 2.4425679261569027e-05, 'epoch': 1.5344592443058584, 'step': 1277000}
INFO:transformers.trainer:{'loss': 2.449171733379364, 'learning_rate': 2.4415665823535184e-05, 'epoch': 1.535060050587889, 'step': 1277500}
INFO:transformers.trainer:{'loss': 2.4242302075624464, 'learning_rate': 2.440565238550134e-05, 'epoch': 1.5356608568699195, 'step': 1278000}
INFO:transformers.trainer:{'loss': 2.423073236823082, 'learning_rate': 2.4395638947467502e-05, 'epoch': 1.53626166315195, 'step': 1278500}
INFO:transformers.trainer:{'loss': 2.3636828252077104, 'learning_rate': 2.438562550943366e-05, 'epoch': 1.5368624694339803, 'step': 1279000}
INFO:transformers.trainer:{'loss': 2.423623036623001, 'learning_rate': 2.437561207139982e-05, 'epoch': 1.537463275716011, 'step': 1279500}
INFO:transformers.trainer:{'loss': 2.487447112798691, 'learning_rate': 2.4365598633365976e-05, 'epoch': 1.5380640819980413, 'step': 1280000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1280000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1280000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1280000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1260000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4952310636043546, 'learning_rate': 2.4355585195332137e-05, 'epoch': 1.538664888280072, 'step': 1280500}
INFO:transformers.trainer:{'loss': 2.4093675093650817, 'learning_rate': 2.4345571757298297e-05, 'epoch': 1.5392656945621024, 'step': 1281000}
INFO:transformers.trainer:{'loss': 2.419355937361717, 'learning_rate': 2.4335558319264454e-05, 'epoch': 1.5398665008441328, 'step': 1281500}
INFO:transformers.trainer:{'loss': 2.415920277416706, 'learning_rate': 2.4325544881230615e-05, 'epoch': 1.5404673071261632, 'step': 1282000}
INFO:transformers.trainer:{'loss': 2.4431690834760666, 'learning_rate': 2.4315531443196772e-05, 'epoch': 1.5410681134081938, 'step': 1282500}
INFO:transformers.trainer:{'loss': 2.4025084002017976, 'learning_rate': 2.430551800516293e-05, 'epoch': 1.5416689196902242, 'step': 1283000}
INFO:transformers.trainer:{'loss': 2.4325710746049882, 'learning_rate': 2.4295504567129086e-05, 'epoch': 1.5422697259722549, 'step': 1283500}
INFO:transformers.trainer:{'loss': 2.433951078891754, 'learning_rate': 2.4285491129095246e-05, 'epoch': 1.5428705322542853, 'step': 1284000}
INFO:transformers.trainer:{'loss': 2.393522624254227, 'learning_rate': 2.4275477691061404e-05, 'epoch': 1.5434713385363157, 'step': 1284500}
INFO:transformers.trainer:{'loss': 2.407374026656151, 'learning_rate': 2.4265464253027564e-05, 'epoch': 1.544072144818346, 'step': 1285000}
INFO:transformers.trainer:{'loss': 2.456183884859085, 'learning_rate': 2.425545081499372e-05, 'epoch': 1.5446729511003767, 'step': 1285500}
INFO:transformers.trainer:{'loss': 2.4451007038354873, 'learning_rate': 2.424543737695988e-05, 'epoch': 1.5452737573824074, 'step': 1286000}
INFO:transformers.trainer:{'loss': 2.4539194772243498, 'learning_rate': 2.4235423938926042e-05, 'epoch': 1.5458745636644378, 'step': 1286500}
INFO:transformers.trainer:{'loss': 2.4816638745069506, 'learning_rate': 2.42254105008922e-05, 'epoch': 1.5464753699464682, 'step': 1287000}
INFO:transformers.trainer:{'loss': 2.4790167862176897, 'learning_rate': 2.421539706285836e-05, 'epoch': 1.5470761762284986, 'step': 1287500}
INFO:transformers.trainer:{'loss': 2.4708355264663697, 'learning_rate': 2.4205383624824517e-05, 'epoch': 1.547676982510529, 'step': 1288000}
INFO:transformers.trainer:{'loss': 2.396081815838814, 'learning_rate': 2.4195370186790674e-05, 'epoch': 1.5482777887925596, 'step': 1288500}
INFO:transformers.trainer:{'loss': 2.430927084952593, 'learning_rate': 2.418535674875683e-05, 'epoch': 1.5488785950745902, 'step': 1289000}
INFO:transformers.trainer:{'loss': 2.413500388145447, 'learning_rate': 2.417534331072299e-05, 'epoch': 1.5494794013566207, 'step': 1289500}
INFO:transformers.trainer:{'loss': 2.403193994641304, 'learning_rate': 2.4165329872689148e-05, 'epoch': 1.550080207638651, 'step': 1290000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1290000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1290000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1290000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1270000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4117840160131454, 'learning_rate': 2.415531643465531e-05, 'epoch': 1.5506810139206815, 'step': 1290500}
INFO:transformers.trainer:{'loss': 2.4255732828378678, 'learning_rate': 2.4145302996621466e-05, 'epoch': 1.5512818202027119, 'step': 1291000}
INFO:transformers.trainer:{'loss': 2.4600206007957457, 'learning_rate': 2.4135289558587626e-05, 'epoch': 1.5518826264847425, 'step': 1291500}
INFO:transformers.trainer:{'loss': 2.4380078716278075, 'learning_rate': 2.4125276120553783e-05, 'epoch': 1.5524834327667731, 'step': 1292000}
INFO:transformers.trainer:{'loss': 2.397390117764473, 'learning_rate': 2.4115262682519944e-05, 'epoch': 1.5530842390488035, 'step': 1292500}
INFO:transformers.trainer:{'loss': 2.4594840941429137, 'learning_rate': 2.4105249244486104e-05, 'epoch': 1.553685045330834, 'step': 1293000}
INFO:transformers.trainer:{'loss': 2.426768344044685, 'learning_rate': 2.409523580645226e-05, 'epoch': 1.5542858516128644, 'step': 1293500}
INFO:transformers.trainer:{'loss': 2.429491251707077, 'learning_rate': 2.4085222368418418e-05, 'epoch': 1.554886657894895, 'step': 1294000}
INFO:transformers.trainer:{'loss': 2.464837483882904, 'learning_rate': 2.4075208930384575e-05, 'epoch': 1.5554874641769254, 'step': 1294500}
INFO:transformers.trainer:{'loss': 2.402407062292099, 'learning_rate': 2.4065195492350736e-05, 'epoch': 1.556088270458956, 'step': 1295000}
INFO:transformers.trainer:{'loss': 2.4497880474328997, 'learning_rate': 2.4055182054316893e-05, 'epoch': 1.5566890767409864, 'step': 1295500}
INFO:transformers.trainer:{'loss': 2.4220826486349107, 'learning_rate': 2.4045168616283053e-05, 'epoch': 1.5572898830230169, 'step': 1296000}
INFO:transformers.trainer:{'loss': 2.4638203567266466, 'learning_rate': 2.403515517824921e-05, 'epoch': 1.5578906893050473, 'step': 1296500}
INFO:transformers.trainer:{'loss': 2.4400067024230956, 'learning_rate': 2.402514174021537e-05, 'epoch': 1.558491495587078, 'step': 1297000}
INFO:transformers.trainer:{'loss': 2.4561115934848785, 'learning_rate': 2.4015128302181528e-05, 'epoch': 1.5590923018691083, 'step': 1297500}
INFO:transformers.trainer:{'loss': 2.4625317655801773, 'learning_rate': 2.400511486414769e-05, 'epoch': 1.559693108151139, 'step': 1298000}
INFO:transformers.trainer:{'loss': 2.4457247410416603, 'learning_rate': 2.3995101426113845e-05, 'epoch': 1.5602939144331693, 'step': 1298500}
INFO:transformers.trainer:{'loss': 2.4042877769470214, 'learning_rate': 2.3985087988080006e-05, 'epoch': 1.5608947207151997, 'step': 1299000}
INFO:transformers.trainer:{'loss': 2.364328364610672, 'learning_rate': 2.3975074550046163e-05, 'epoch': 1.5614955269972302, 'step': 1299500}
INFO:transformers.trainer:{'loss': 2.4247822659015656, 'learning_rate': 2.396506111201232e-05, 'epoch': 1.5620963332792608, 'step': 1300000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1300000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1300000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1300000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1280000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3997698677778243, 'learning_rate': 2.395504767397848e-05, 'epoch': 1.5626971395612914, 'step': 1300500}
INFO:transformers.trainer:{'loss': 2.4342473376989364, 'learning_rate': 2.3945034235944637e-05, 'epoch': 1.5632979458433218, 'step': 1301000}
INFO:transformers.trainer:{'loss': 2.385094111084938, 'learning_rate': 2.3935020797910798e-05, 'epoch': 1.5638987521253522, 'step': 1301500}
INFO:transformers.trainer:{'loss': 2.4333729362487793, 'learning_rate': 2.3925007359876955e-05, 'epoch': 1.5644995584073826, 'step': 1302000}
INFO:transformers.trainer:{'loss': 2.419966821074486, 'learning_rate': 2.3914993921843115e-05, 'epoch': 1.565100364689413, 'step': 1302500}
INFO:transformers.trainer:{'loss': 2.421914938926697, 'learning_rate': 2.3904980483809273e-05, 'epoch': 1.5657011709714437, 'step': 1303000}
INFO:transformers.trainer:{'loss': 2.403567014813423, 'learning_rate': 2.3894967045775433e-05, 'epoch': 1.5663019772534743, 'step': 1303500}
INFO:transformers.trainer:{'loss': 2.4952116087675096, 'learning_rate': 2.388495360774159e-05, 'epoch': 1.5669027835355047, 'step': 1304000}
INFO:transformers.trainer:{'loss': 2.3979599655866624, 'learning_rate': 2.387494016970775e-05, 'epoch': 1.5675035898175351, 'step': 1304500}
INFO:transformers.trainer:{'loss': 2.4671706651449203, 'learning_rate': 2.3864926731673908e-05, 'epoch': 1.5681043960995655, 'step': 1305000}
INFO:transformers.trainer:{'loss': 2.387032401919365, 'learning_rate': 2.3854913293640065e-05, 'epoch': 1.568705202381596, 'step': 1305500}
INFO:transformers.trainer:{'loss': 2.4452432339191437, 'learning_rate': 2.3844899855606225e-05, 'epoch': 1.5693060086636266, 'step': 1306000}
INFO:transformers.trainer:{'loss': 2.413568877339363, 'learning_rate': 2.3834886417572382e-05, 'epoch': 1.5699068149456572, 'step': 1306500}
INFO:transformers.trainer:{'loss': 2.4162017372846605, 'learning_rate': 2.3824872979538543e-05, 'epoch': 1.5705076212276876, 'step': 1307000}
INFO:transformers.trainer:{'loss': 2.458960478067398, 'learning_rate': 2.38148595415047e-05, 'epoch': 1.571108427509718, 'step': 1307500}
INFO:transformers.trainer:{'loss': 2.4373715435266496, 'learning_rate': 2.380484610347086e-05, 'epoch': 1.5717092337917484, 'step': 1308000}
INFO:transformers.trainer:{'loss': 2.462261496901512, 'learning_rate': 2.3794832665437017e-05, 'epoch': 1.572310040073779, 'step': 1308500}
INFO:transformers.trainer:{'loss': 2.4430012934207914, 'learning_rate': 2.3784819227403178e-05, 'epoch': 1.5729108463558095, 'step': 1309000}
INFO:transformers.trainer:{'loss': 2.410718510389328, 'learning_rate': 2.3774805789369335e-05, 'epoch': 1.57351165263784, 'step': 1309500}
INFO:transformers.trainer:{'loss': 2.401762573003769, 'learning_rate': 2.3764792351335495e-05, 'epoch': 1.5741124589198705, 'step': 1310000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1310000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1310000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1310000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1290000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.39081631565094, 'learning_rate': 2.3754778913301652e-05, 'epoch': 1.574713265201901, 'step': 1310500}
INFO:transformers.trainer:{'loss': 2.4038986825048925, 'learning_rate': 2.374476547526781e-05, 'epoch': 1.5753140714839313, 'step': 1311000}
INFO:transformers.trainer:{'loss': 2.3773123930692672, 'learning_rate': 2.373475203723397e-05, 'epoch': 1.575914877765962, 'step': 1311500}
INFO:transformers.trainer:{'loss': 2.393279598712921, 'learning_rate': 2.3724738599200127e-05, 'epoch': 1.5765156840479924, 'step': 1312000}
INFO:transformers.trainer:{'loss': 2.3939411255121232, 'learning_rate': 2.3714725161166287e-05, 'epoch': 1.577116490330023, 'step': 1312500}
INFO:transformers.trainer:{'loss': 2.3932638710141183, 'learning_rate': 2.3704711723132444e-05, 'epoch': 1.5777172966120534, 'step': 1313000}
INFO:transformers.trainer:{'loss': 2.4755525394678117, 'learning_rate': 2.3694698285098605e-05, 'epoch': 1.5783181028940838, 'step': 1313500}
INFO:transformers.trainer:{'loss': 2.397460139513016, 'learning_rate': 2.3684684847064762e-05, 'epoch': 1.5789189091761142, 'step': 1314000}
INFO:transformers.trainer:{'loss': 2.370098010659218, 'learning_rate': 2.3674671409030922e-05, 'epoch': 1.5795197154581448, 'step': 1314500}
INFO:transformers.trainer:{'loss': 2.4224921003580095, 'learning_rate': 2.366465797099708e-05, 'epoch': 1.5801205217401755, 'step': 1315000}
INFO:transformers.trainer:{'loss': 2.4172517689466475, 'learning_rate': 2.365464453296324e-05, 'epoch': 1.5807213280222059, 'step': 1315500}
INFO:transformers.trainer:{'loss': 2.4051966404914857, 'learning_rate': 2.3644631094929397e-05, 'epoch': 1.5813221343042363, 'step': 1316000}
INFO:transformers.trainer:{'loss': 2.4127446755170823, 'learning_rate': 2.3634617656895554e-05, 'epoch': 1.5819229405862667, 'step': 1316500}
INFO:transformers.trainer:{'loss': 2.440790908932686, 'learning_rate': 2.362460421886171e-05, 'epoch': 1.582523746868297, 'step': 1317000}
INFO:transformers.trainer:{'loss': 2.3936909596920013, 'learning_rate': 2.361459078082787e-05, 'epoch': 1.5831245531503277, 'step': 1317500}
INFO:transformers.trainer:{'loss': 2.4648538509607314, 'learning_rate': 2.3604577342794032e-05, 'epoch': 1.5837253594323584, 'step': 1318000}
INFO:transformers.trainer:{'loss': 2.40410684299469, 'learning_rate': 2.359456390476019e-05, 'epoch': 1.5843261657143888, 'step': 1318500}
INFO:transformers.trainer:{'loss': 2.467401999950409, 'learning_rate': 2.358455046672635e-05, 'epoch': 1.5849269719964192, 'step': 1319000}
INFO:transformers.trainer:{'loss': 2.409776188135147, 'learning_rate': 2.3574537028692506e-05, 'epoch': 1.5855277782784496, 'step': 1319500}
INFO:transformers.trainer:{'loss': 2.398418245315552, 'learning_rate': 2.3564523590658667e-05, 'epoch': 1.58612858456048, 'step': 1320000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1320000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1320000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1320000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1300000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.451515331983566, 'learning_rate': 2.3554510152624824e-05, 'epoch': 1.5867293908425106, 'step': 1320500}
INFO:transformers.trainer:{'loss': 2.409705770432949, 'learning_rate': 2.3544496714590984e-05, 'epoch': 1.5873301971245413, 'step': 1321000}
INFO:transformers.trainer:{'loss': 2.4570080964565277, 'learning_rate': 2.353448327655714e-05, 'epoch': 1.5879310034065717, 'step': 1321500}
INFO:transformers.trainer:{'loss': 2.447526770114899, 'learning_rate': 2.35244698385233e-05, 'epoch': 1.588531809688602, 'step': 1322000}
INFO:transformers.trainer:{'loss': 2.40304067838192, 'learning_rate': 2.3514456400489456e-05, 'epoch': 1.5891326159706325, 'step': 1322500}
INFO:transformers.trainer:{'loss': 2.4374338252544403, 'learning_rate': 2.3504442962455616e-05, 'epoch': 1.5897334222526631, 'step': 1323000}
INFO:transformers.trainer:{'loss': 2.4187370618581774, 'learning_rate': 2.3494429524421773e-05, 'epoch': 1.5903342285346935, 'step': 1323500}
INFO:transformers.trainer:{'loss': 2.371649451971054, 'learning_rate': 2.3484416086387934e-05, 'epoch': 1.5909350348167242, 'step': 1324000}
INFO:transformers.trainer:{'loss': 2.4052843741178513, 'learning_rate': 2.3474402648354094e-05, 'epoch': 1.5915358410987546, 'step': 1324500}
INFO:transformers.trainer:{'loss': 2.438391173362732, 'learning_rate': 2.346438921032025e-05, 'epoch': 1.592136647380785, 'step': 1325000}
INFO:transformers.trainer:{'loss': 2.4058143633604048, 'learning_rate': 2.345437577228641e-05, 'epoch': 1.5927374536628154, 'step': 1325500}
INFO:transformers.trainer:{'loss': 2.4336511983275413, 'learning_rate': 2.344436233425257e-05, 'epoch': 1.593338259944846, 'step': 1326000}
INFO:transformers.trainer:{'loss': 2.4162894092798233, 'learning_rate': 2.343434889621873e-05, 'epoch': 1.5939390662268764, 'step': 1326500}
INFO:transformers.trainer:{'loss': 2.3833120852708816, 'learning_rate': 2.3424335458184886e-05, 'epoch': 1.594539872508907, 'step': 1327000}
INFO:transformers.trainer:{'loss': 2.428308308362961, 'learning_rate': 2.3414322020151043e-05, 'epoch': 1.5951406787909375, 'step': 1327500}
INFO:transformers.trainer:{'loss': 2.3466669750213622, 'learning_rate': 2.34043085821172e-05, 'epoch': 1.5957414850729679, 'step': 1328000}
INFO:transformers.trainer:{'loss': 2.447022894859314, 'learning_rate': 2.339429514408336e-05, 'epoch': 1.5963422913549983, 'step': 1328500}
INFO:transformers.trainer:{'loss': 2.442010249733925, 'learning_rate': 2.3384281706049518e-05, 'epoch': 1.596943097637029, 'step': 1329000}
INFO:transformers.trainer:{'loss': 2.4262998479604723, 'learning_rate': 2.3374268268015678e-05, 'epoch': 1.5975439039190595, 'step': 1329500}
INFO:transformers.trainer:{'loss': 2.3851526185274126, 'learning_rate': 2.336425482998184e-05, 'epoch': 1.59814471020109, 'step': 1330000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1330000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1330000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1330000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1310000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4350180112123487, 'learning_rate': 2.3354241391947996e-05, 'epoch': 1.5987455164831204, 'step': 1330500}
INFO:transformers.trainer:{'loss': 2.421302022814751, 'learning_rate': 2.3344227953914156e-05, 'epoch': 1.5993463227651508, 'step': 1331000}
INFO:transformers.trainer:{'loss': 2.463897940635681, 'learning_rate': 2.3334214515880313e-05, 'epoch': 1.5999471290471812, 'step': 1331500}
INFO:transformers.trainer:{'loss': 2.408840104699135, 'learning_rate': 2.3324201077846474e-05, 'epoch': 1.6005479353292118, 'step': 1332000}
INFO:transformers.trainer:{'loss': 2.455390902400017, 'learning_rate': 2.331418763981263e-05, 'epoch': 1.6011487416112424, 'step': 1332500}
INFO:transformers.trainer:{'loss': 2.378242197394371, 'learning_rate': 2.3304174201778788e-05, 'epoch': 1.6017495478932728, 'step': 1333000}
INFO:transformers.trainer:{'loss': 2.387693625807762, 'learning_rate': 2.3294160763744945e-05, 'epoch': 1.6023503541753032, 'step': 1333500}
INFO:transformers.trainer:{'loss': 2.417075534105301, 'learning_rate': 2.3284147325711105e-05, 'epoch': 1.6029511604573337, 'step': 1334000}
INFO:transformers.trainer:{'loss': 2.38590854036808, 'learning_rate': 2.3274133887677262e-05, 'epoch': 1.603551966739364, 'step': 1334500}
INFO:transformers.trainer:{'loss': 2.3972849205732345, 'learning_rate': 2.3264120449643423e-05, 'epoch': 1.6041527730213947, 'step': 1335000}
INFO:transformers.trainer:{'loss': 2.4253925479650498, 'learning_rate': 2.325410701160958e-05, 'epoch': 1.6047535793034253, 'step': 1335500}
INFO:transformers.trainer:{'loss': 2.3162773079276087, 'learning_rate': 2.324409357357574e-05, 'epoch': 1.6053543855854557, 'step': 1336000}
INFO:transformers.trainer:{'loss': 2.4316778229475022, 'learning_rate': 2.32340801355419e-05, 'epoch': 1.6059551918674861, 'step': 1336500}
INFO:transformers.trainer:{'loss': 2.4245797610282898, 'learning_rate': 2.3224066697508058e-05, 'epoch': 1.6065559981495166, 'step': 1337000}
INFO:transformers.trainer:{'loss': 2.407234384059906, 'learning_rate': 2.3214053259474215e-05, 'epoch': 1.6071568044315472, 'step': 1337500}
INFO:transformers.trainer:{'loss': 2.3829383758306504, 'learning_rate': 2.3204039821440372e-05, 'epoch': 1.6077576107135776, 'step': 1338000}
INFO:transformers.trainer:{'loss': 2.394033879518509, 'learning_rate': 2.3194026383406532e-05, 'epoch': 1.6083584169956082, 'step': 1338500}
INFO:transformers.trainer:{'loss': 2.4635331970453263, 'learning_rate': 2.318401294537269e-05, 'epoch': 1.6089592232776386, 'step': 1339000}
INFO:transformers.trainer:{'loss': 2.367162133693695, 'learning_rate': 2.317399950733885e-05, 'epoch': 1.609560029559669, 'step': 1339500}
INFO:transformers.trainer:{'loss': 2.4145378730297087, 'learning_rate': 2.3163986069305007e-05, 'epoch': 1.6101608358416994, 'step': 1340000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1340000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1340000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1340000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1320000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.463248579740524, 'learning_rate': 2.3153972631271168e-05, 'epoch': 1.61076164212373, 'step': 1340500}
INFO:transformers.trainer:{'loss': 2.4448592451810836, 'learning_rate': 2.3143959193237325e-05, 'epoch': 1.6113624484057605, 'step': 1341000}
INFO:transformers.trainer:{'loss': 2.3964960448741914, 'learning_rate': 2.3133945755203485e-05, 'epoch': 1.6119632546877911, 'step': 1341500}
INFO:transformers.trainer:{'loss': 2.408910355448723, 'learning_rate': 2.3123932317169642e-05, 'epoch': 1.6125640609698215, 'step': 1342000}
INFO:transformers.trainer:{'loss': 2.3963258697986602, 'learning_rate': 2.3113918879135803e-05, 'epoch': 1.613164867251852, 'step': 1342500}
INFO:transformers.trainer:{'loss': 2.456683648109436, 'learning_rate': 2.310390544110196e-05, 'epoch': 1.6137656735338823, 'step': 1343000}
INFO:transformers.trainer:{'loss': 2.385350205183029, 'learning_rate': 2.3093892003068117e-05, 'epoch': 1.614366479815913, 'step': 1343500}
INFO:transformers.trainer:{'loss': 2.3523271771669387, 'learning_rate': 2.3083878565034277e-05, 'epoch': 1.6149672860979436, 'step': 1344000}
INFO:transformers.trainer:{'loss': 2.433374451994896, 'learning_rate': 2.3073865127000434e-05, 'epoch': 1.615568092379974, 'step': 1344500}
INFO:transformers.trainer:{'loss': 2.405353532612324, 'learning_rate': 2.3063851688966595e-05, 'epoch': 1.6161688986620044, 'step': 1345000}
INFO:transformers.trainer:{'loss': 2.4180075700283052, 'learning_rate': 2.3053838250932752e-05, 'epoch': 1.6167697049440348, 'step': 1345500}
INFO:transformers.trainer:{'loss': 2.4568196964263915, 'learning_rate': 2.3043824812898912e-05, 'epoch': 1.6173705112260652, 'step': 1346000}
INFO:transformers.trainer:{'loss': 2.444105551838875, 'learning_rate': 2.303381137486507e-05, 'epoch': 1.6179713175080959, 'step': 1346500}
INFO:transformers.trainer:{'loss': 2.467689790725708, 'learning_rate': 2.302379793683123e-05, 'epoch': 1.6185721237901265, 'step': 1347000}
INFO:transformers.trainer:{'loss': 2.4433539633750914, 'learning_rate': 2.3013784498797387e-05, 'epoch': 1.619172930072157, 'step': 1347500}
INFO:transformers.trainer:{'loss': 2.4623514308929444, 'learning_rate': 2.3003771060763547e-05, 'epoch': 1.6197737363541873, 'step': 1348000}
INFO:transformers.trainer:{'loss': 2.431861813187599, 'learning_rate': 2.2993757622729704e-05, 'epoch': 1.6203745426362177, 'step': 1348500}
INFO:transformers.trainer:{'loss': 2.3849961581230166, 'learning_rate': 2.298374418469586e-05, 'epoch': 1.6209753489182481, 'step': 1349000}
INFO:transformers.trainer:{'loss': 2.45863950073719, 'learning_rate': 2.2973730746662022e-05, 'epoch': 1.6215761552002788, 'step': 1349500}
INFO:transformers.trainer:{'loss': 2.3973897465467453, 'learning_rate': 2.296371730862818e-05, 'epoch': 1.6221769614823094, 'step': 1350000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1350000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1350000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1350000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1330000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4028948969841, 'learning_rate': 2.295370387059434e-05, 'epoch': 1.6227777677643398, 'step': 1350500}
INFO:transformers.trainer:{'loss': 2.3616657462120054, 'learning_rate': 2.2943690432560496e-05, 'epoch': 1.6233785740463702, 'step': 1351000}
INFO:transformers.trainer:{'loss': 2.463593155503273, 'learning_rate': 2.2933676994526657e-05, 'epoch': 1.6239793803284006, 'step': 1351500}
INFO:transformers.trainer:{'loss': 2.4013038394451143, 'learning_rate': 2.2923663556492814e-05, 'epoch': 1.6245801866104312, 'step': 1352000}
INFO:transformers.trainer:{'loss': 2.3831078164577484, 'learning_rate': 2.2913650118458974e-05, 'epoch': 1.6251809928924617, 'step': 1352500}
INFO:transformers.trainer:{'loss': 2.457559251308441, 'learning_rate': 2.290363668042513e-05, 'epoch': 1.6257817991744923, 'step': 1353000}
INFO:transformers.trainer:{'loss': 2.397184189558029, 'learning_rate': 2.2893623242391292e-05, 'epoch': 1.6263826054565227, 'step': 1353500}
INFO:transformers.trainer:{'loss': 2.4262323690652847, 'learning_rate': 2.288360980435745e-05, 'epoch': 1.626983411738553, 'step': 1354000}
INFO:transformers.trainer:{'loss': 2.50227603161335, 'learning_rate': 2.2873596366323606e-05, 'epoch': 1.6275842180205835, 'step': 1354500}
INFO:transformers.trainer:{'loss': 2.4310619953870773, 'learning_rate': 2.2863582928289766e-05, 'epoch': 1.6281850243026141, 'step': 1355000}
INFO:transformers.trainer:{'loss': 2.43115535736084, 'learning_rate': 2.2853569490255923e-05, 'epoch': 1.6287858305846445, 'step': 1355500}
INFO:transformers.trainer:{'loss': 2.394419098496437, 'learning_rate': 2.2843556052222084e-05, 'epoch': 1.6293866368666752, 'step': 1356000}
INFO:transformers.trainer:{'loss': 2.4408153297901154, 'learning_rate': 2.283354261418824e-05, 'epoch': 1.6299874431487056, 'step': 1356500}
INFO:transformers.trainer:{'loss': 2.411156891465187, 'learning_rate': 2.28235291761544e-05, 'epoch': 1.630588249430736, 'step': 1357000}
INFO:transformers.trainer:{'loss': 2.396077853322029, 'learning_rate': 2.281351573812056e-05, 'epoch': 1.6311890557127664, 'step': 1357500}
INFO:transformers.trainer:{'loss': 2.441505039215088, 'learning_rate': 2.280350230008672e-05, 'epoch': 1.631789861994797, 'step': 1358000}
INFO:transformers.trainer:{'loss': 2.4673730680942536, 'learning_rate': 2.2793488862052876e-05, 'epoch': 1.6323906682768277, 'step': 1358500}
INFO:transformers.trainer:{'loss': 2.3985953414440155, 'learning_rate': 2.2783475424019036e-05, 'epoch': 1.632991474558858, 'step': 1359000}
INFO:transformers.trainer:{'loss': 2.392154850125313, 'learning_rate': 2.2773461985985194e-05, 'epoch': 1.6335922808408885, 'step': 1359500}
INFO:transformers.trainer:{'loss': 2.3963377685546874, 'learning_rate': 2.276344854795135e-05, 'epoch': 1.6341930871229189, 'step': 1360000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1360000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1360000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1360000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1340000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4754783947467804, 'learning_rate': 2.2753435109917508e-05, 'epoch': 1.6347938934049493, 'step': 1360500}
INFO:transformers.trainer:{'loss': 2.3710828384160996, 'learning_rate': 2.2743421671883668e-05, 'epoch': 1.63539469968698, 'step': 1361000}
INFO:transformers.trainer:{'loss': 2.390970445394516, 'learning_rate': 2.273340823384983e-05, 'epoch': 1.6359955059690106, 'step': 1361500}
INFO:transformers.trainer:{'loss': 2.4394677439928056, 'learning_rate': 2.2723394795815986e-05, 'epoch': 1.636596312251041, 'step': 1362000}
INFO:transformers.trainer:{'loss': 2.3885361120700837, 'learning_rate': 2.2713381357782146e-05, 'epoch': 1.6371971185330714, 'step': 1362500}
INFO:transformers.trainer:{'loss': 2.416083299994469, 'learning_rate': 2.2703367919748303e-05, 'epoch': 1.6377979248151018, 'step': 1363000}
INFO:transformers.trainer:{'loss': 2.3823284093141557, 'learning_rate': 2.2693354481714464e-05, 'epoch': 1.6383987310971322, 'step': 1363500}
INFO:transformers.trainer:{'loss': 2.405655597805977, 'learning_rate': 2.268334104368062e-05, 'epoch': 1.6389995373791628, 'step': 1364000}
INFO:transformers.trainer:{'loss': 2.344180796146393, 'learning_rate': 2.267332760564678e-05, 'epoch': 1.6396003436611934, 'step': 1364500}
INFO:transformers.trainer:{'loss': 2.444188654303551, 'learning_rate': 2.2663314167612938e-05, 'epoch': 1.6402011499432239, 'step': 1365000}
INFO:transformers.trainer:{'loss': 2.3858253108263017, 'learning_rate': 2.2653300729579095e-05, 'epoch': 1.6408019562252543, 'step': 1365500}
INFO:transformers.trainer:{'loss': 2.4192386390566827, 'learning_rate': 2.2643287291545252e-05, 'epoch': 1.6414027625072847, 'step': 1366000}
INFO:transformers.trainer:{'loss': 2.3953894687891006, 'learning_rate': 2.2633273853511413e-05, 'epoch': 1.6420035687893153, 'step': 1366500}
INFO:transformers.trainer:{'loss': 2.3595444023609162, 'learning_rate': 2.262326041547757e-05, 'epoch': 1.6426043750713457, 'step': 1367000}
INFO:transformers.trainer:{'loss': 2.436982648611069, 'learning_rate': 2.261324697744373e-05, 'epoch': 1.6432051813533763, 'step': 1367500}
INFO:transformers.trainer:{'loss': 2.4298803774118425, 'learning_rate': 2.260323353940989e-05, 'epoch': 1.6438059876354068, 'step': 1368000}
INFO:transformers.trainer:{'loss': 2.426239333987236, 'learning_rate': 2.2593220101376048e-05, 'epoch': 1.6444067939174372, 'step': 1368500}
INFO:transformers.trainer:{'loss': 2.4198577983379366, 'learning_rate': 2.2583206663342208e-05, 'epoch': 1.6450076001994676, 'step': 1369000}
INFO:transformers.trainer:{'loss': 2.3739921813607214, 'learning_rate': 2.2573193225308365e-05, 'epoch': 1.6456084064814982, 'step': 1369500}
INFO:transformers.trainer:{'loss': 2.4255880144834516, 'learning_rate': 2.2563179787274526e-05, 'epoch': 1.6462092127635286, 'step': 1370000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1370000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1370000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1370000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1350000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4392883787155153, 'learning_rate': 2.2553166349240683e-05, 'epoch': 1.6468100190455592, 'step': 1370500}
INFO:transformers.trainer:{'loss': 2.4350275721549988, 'learning_rate': 2.254315291120684e-05, 'epoch': 1.6474108253275896, 'step': 1371000}
INFO:transformers.trainer:{'loss': 2.399213897228241, 'learning_rate': 2.2533139473172997e-05, 'epoch': 1.64801163160962, 'step': 1371500}
INFO:transformers.trainer:{'loss': 2.4314488438367845, 'learning_rate': 2.2523126035139157e-05, 'epoch': 1.6486124378916505, 'step': 1372000}
INFO:transformers.trainer:{'loss': 2.436379442691803, 'learning_rate': 2.2513112597105314e-05, 'epoch': 1.649213244173681, 'step': 1372500}
INFO:transformers.trainer:{'loss': 2.42496856880188, 'learning_rate': 2.2503099159071475e-05, 'epoch': 1.6498140504557117, 'step': 1373000}
INFO:transformers.trainer:{'loss': 2.4316494467258454, 'learning_rate': 2.2493085721037632e-05, 'epoch': 1.6504148567377421, 'step': 1373500}
INFO:transformers.trainer:{'loss': 2.4185843292474747, 'learning_rate': 2.2483072283003792e-05, 'epoch': 1.6510156630197725, 'step': 1374000}
INFO:transformers.trainer:{'loss': 2.438590291976929, 'learning_rate': 2.2473058844969953e-05, 'epoch': 1.651616469301803, 'step': 1374500}
INFO:transformers.trainer:{'loss': 2.4624954121112825, 'learning_rate': 2.246304540693611e-05, 'epoch': 1.6522172755838334, 'step': 1375000}
INFO:transformers.trainer:{'loss': 2.428908648967743, 'learning_rate': 2.245303196890227e-05, 'epoch': 1.652818081865864, 'step': 1375500}
INFO:transformers.trainer:{'loss': 2.364762790322304, 'learning_rate': 2.2443018530868427e-05, 'epoch': 1.6534188881478946, 'step': 1376000}
INFO:transformers.trainer:{'loss': 2.409547392129898, 'learning_rate': 2.2433005092834585e-05, 'epoch': 1.654019694429925, 'step': 1376500}
INFO:transformers.trainer:{'loss': 2.3593060643672943, 'learning_rate': 2.242299165480074e-05, 'epoch': 1.6546205007119554, 'step': 1377000}
INFO:transformers.trainer:{'loss': 2.3773329219818113, 'learning_rate': 2.2412978216766902e-05, 'epoch': 1.6552213069939858, 'step': 1377500}
INFO:transformers.trainer:{'loss': 2.4524027898311616, 'learning_rate': 2.240296477873306e-05, 'epoch': 1.6558221132760162, 'step': 1378000}
INFO:transformers.trainer:{'loss': 2.411068375468254, 'learning_rate': 2.239295134069922e-05, 'epoch': 1.6564229195580469, 'step': 1378500}
INFO:transformers.trainer:{'loss': 2.3687844606637953, 'learning_rate': 2.2382937902665377e-05, 'epoch': 1.6570237258400775, 'step': 1379000}
INFO:transformers.trainer:{'loss': 2.4520462775230407, 'learning_rate': 2.2372924464631537e-05, 'epoch': 1.657624532122108, 'step': 1379500}
INFO:transformers.trainer:{'loss': 2.407279248714447, 'learning_rate': 2.2362911026597698e-05, 'epoch': 1.6582253384041383, 'step': 1380000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1380000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1380000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1380000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1360000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.407333322286606, 'learning_rate': 2.2352897588563855e-05, 'epoch': 1.6588261446861687, 'step': 1380500}
INFO:transformers.trainer:{'loss': 2.4200641386508943, 'learning_rate': 2.2342884150530015e-05, 'epoch': 1.6594269509681994, 'step': 1381000}
INFO:transformers.trainer:{'loss': 2.383784550011158, 'learning_rate': 2.2332870712496172e-05, 'epoch': 1.6600277572502298, 'step': 1381500}
INFO:transformers.trainer:{'loss': 2.4027080942392347, 'learning_rate': 2.232285727446233e-05, 'epoch': 1.6606285635322604, 'step': 1382000}
INFO:transformers.trainer:{'loss': 2.4326452827453613, 'learning_rate': 2.2312843836428486e-05, 'epoch': 1.6612293698142908, 'step': 1382500}
INFO:transformers.trainer:{'loss': 2.437766902565956, 'learning_rate': 2.2302830398394647e-05, 'epoch': 1.6618301760963212, 'step': 1383000}
INFO:transformers.trainer:{'loss': 2.416751858353615, 'learning_rate': 2.2292816960360804e-05, 'epoch': 1.6624309823783516, 'step': 1383500}
INFO:transformers.trainer:{'loss': 2.4720510071516038, 'learning_rate': 2.2282803522326964e-05, 'epoch': 1.6630317886603823, 'step': 1384000}
INFO:transformers.trainer:{'loss': 2.4042404274344444, 'learning_rate': 2.227279008429312e-05, 'epoch': 1.6636325949424127, 'step': 1384500}
INFO:transformers.trainer:{'loss': 2.449356993317604, 'learning_rate': 2.2262776646259282e-05, 'epoch': 1.6642334012244433, 'step': 1385000}
INFO:transformers.trainer:{'loss': 2.4228502901792526, 'learning_rate': 2.225276320822544e-05, 'epoch': 1.6648342075064737, 'step': 1385500}
INFO:transformers.trainer:{'loss': 2.393848115563393, 'learning_rate': 2.22427497701916e-05, 'epoch': 1.6654350137885041, 'step': 1386000}
INFO:transformers.trainer:{'loss': 2.425490453958511, 'learning_rate': 2.223273633215776e-05, 'epoch': 1.6660358200705345, 'step': 1386500}
INFO:transformers.trainer:{'loss': 2.3959845954179766, 'learning_rate': 2.2222722894123917e-05, 'epoch': 1.6666366263525652, 'step': 1387000}
INFO:transformers.trainer:{'loss': 2.408689547657967, 'learning_rate': 2.2212709456090074e-05, 'epoch': 1.6672374326345958, 'step': 1387500}
INFO:transformers.trainer:{'loss': 2.3845488340854644, 'learning_rate': 2.220269601805623e-05, 'epoch': 1.6678382389166262, 'step': 1388000}
INFO:transformers.trainer:{'loss': 2.3869888043403624, 'learning_rate': 2.219268258002239e-05, 'epoch': 1.6684390451986566, 'step': 1388500}
INFO:transformers.trainer:{'loss': 2.4390112317800523, 'learning_rate': 2.218266914198855e-05, 'epoch': 1.669039851480687, 'step': 1389000}
INFO:transformers.trainer:{'loss': 2.3869447968006132, 'learning_rate': 2.217265570395471e-05, 'epoch': 1.6696406577627174, 'step': 1389500}
INFO:transformers.trainer:{'loss': 2.4254211122989653, 'learning_rate': 2.2162642265920866e-05, 'epoch': 1.670241464044748, 'step': 1390000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1390000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1390000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1390000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1370000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3909241299629214, 'learning_rate': 2.2152628827887026e-05, 'epoch': 1.6708422703267787, 'step': 1390500}
INFO:transformers.trainer:{'loss': 2.4244104413986207, 'learning_rate': 2.2142615389853183e-05, 'epoch': 1.671443076608809, 'step': 1391000}
INFO:transformers.trainer:{'loss': 2.381631670832634, 'learning_rate': 2.2132601951819344e-05, 'epoch': 1.6720438828908395, 'step': 1391500}
INFO:transformers.trainer:{'loss': 2.4073809690475465, 'learning_rate': 2.21225885137855e-05, 'epoch': 1.67264468917287, 'step': 1392000}
INFO:transformers.trainer:{'loss': 2.334179581284523, 'learning_rate': 2.211257507575166e-05, 'epoch': 1.6732454954549003, 'step': 1392500}
INFO:transformers.trainer:{'loss': 2.436997986793518, 'learning_rate': 2.210256163771782e-05, 'epoch': 1.673846301736931, 'step': 1393000}
INFO:transformers.trainer:{'loss': 2.4310280635356905, 'learning_rate': 2.2092548199683976e-05, 'epoch': 1.6744471080189616, 'step': 1393500}
INFO:transformers.trainer:{'loss': 2.4254449266195297, 'learning_rate': 2.2082534761650136e-05, 'epoch': 1.675047914300992, 'step': 1394000}
INFO:transformers.trainer:{'loss': 2.3789666525125504, 'learning_rate': 2.2072521323616293e-05, 'epoch': 1.6756487205830224, 'step': 1394500}
INFO:transformers.trainer:{'loss': 2.458170576930046, 'learning_rate': 2.2062507885582454e-05, 'epoch': 1.6762495268650528, 'step': 1395000}
INFO:transformers.trainer:{'loss': 2.3616311826705934, 'learning_rate': 2.205249444754861e-05, 'epoch': 1.6768503331470834, 'step': 1395500}
INFO:transformers.trainer:{'loss': 2.425721955895424, 'learning_rate': 2.204248100951477e-05, 'epoch': 1.6774511394291138, 'step': 1396000}
INFO:transformers.trainer:{'loss': 2.4332298908233643, 'learning_rate': 2.2032467571480928e-05, 'epoch': 1.6780519457111445, 'step': 1396500}
INFO:transformers.trainer:{'loss': 2.426407439351082, 'learning_rate': 2.202245413344709e-05, 'epoch': 1.6786527519931749, 'step': 1397000}
INFO:transformers.trainer:{'loss': 2.3744081008434295, 'learning_rate': 2.2012440695413246e-05, 'epoch': 1.6792535582752053, 'step': 1397500}
INFO:transformers.trainer:{'loss': 2.37573086476326, 'learning_rate': 2.2002427257379403e-05, 'epoch': 1.6798543645572357, 'step': 1398000}
INFO:transformers.trainer:{'loss': 2.3544885746240616, 'learning_rate': 2.199241381934556e-05, 'epoch': 1.6804551708392663, 'step': 1398500}
INFO:transformers.trainer:{'loss': 2.384072963833809, 'learning_rate': 2.198240038131172e-05, 'epoch': 1.6810559771212967, 'step': 1399000}
INFO:transformers.trainer:{'loss': 2.3930373229384423, 'learning_rate': 2.197238694327788e-05, 'epoch': 1.6816567834033274, 'step': 1399500}
INFO:transformers.trainer:{'loss': 2.4262833771705625, 'learning_rate': 2.1962373505244038e-05, 'epoch': 1.6822575896853578, 'step': 1400000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1400000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1400000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1400000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1380000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3931642379164697, 'learning_rate': 2.1952360067210198e-05, 'epoch': 1.6828583959673882, 'step': 1400500}
INFO:transformers.trainer:{'loss': 2.4625215632915496, 'learning_rate': 2.1942346629176355e-05, 'epoch': 1.6834592022494186, 'step': 1401000}
INFO:transformers.trainer:{'loss': 2.3929387134313584, 'learning_rate': 2.1932333191142516e-05, 'epoch': 1.6840600085314492, 'step': 1401500}
INFO:transformers.trainer:{'loss': 2.366519785284996, 'learning_rate': 2.1922319753108673e-05, 'epoch': 1.6846608148134798, 'step': 1402000}
INFO:transformers.trainer:{'loss': 2.3390755989551546, 'learning_rate': 2.1912306315074833e-05, 'epoch': 1.6852616210955103, 'step': 1402500}
INFO:transformers.trainer:{'loss': 2.4186700029373167, 'learning_rate': 2.190229287704099e-05, 'epoch': 1.6858624273775407, 'step': 1403000}
INFO:transformers.trainer:{'loss': 2.4451872066259384, 'learning_rate': 2.1892279439007147e-05, 'epoch': 1.686463233659571, 'step': 1403500}
INFO:transformers.trainer:{'loss': 2.4249577857255935, 'learning_rate': 2.1882266000973304e-05, 'epoch': 1.6870640399416015, 'step': 1404000}
INFO:transformers.trainer:{'loss': 2.355776411533356, 'learning_rate': 2.1872252562939465e-05, 'epoch': 1.687664846223632, 'step': 1404500}
INFO:transformers.trainer:{'loss': 2.392128716349602, 'learning_rate': 2.1862239124905625e-05, 'epoch': 1.6882656525056627, 'step': 1405000}
INFO:transformers.trainer:{'loss': 2.423029935359955, 'learning_rate': 2.1852225686871782e-05, 'epoch': 1.6888664587876931, 'step': 1405500}
INFO:transformers.trainer:{'loss': 2.400421932041645, 'learning_rate': 2.1842212248837943e-05, 'epoch': 1.6894672650697236, 'step': 1406000}
INFO:transformers.trainer:{'loss': 2.4345172160863875, 'learning_rate': 2.18321988108041e-05, 'epoch': 1.690068071351754, 'step': 1406500}
INFO:transformers.trainer:{'loss': 2.3797992267012598, 'learning_rate': 2.182218537277026e-05, 'epoch': 1.6906688776337844, 'step': 1407000}
INFO:transformers.trainer:{'loss': 2.427546474456787, 'learning_rate': 2.1812171934736417e-05, 'epoch': 1.691269683915815, 'step': 1407500}
INFO:transformers.trainer:{'loss': 2.439567160487175, 'learning_rate': 2.1802158496702578e-05, 'epoch': 1.6918704901978456, 'step': 1408000}
INFO:transformers.trainer:{'loss': 2.333267654776573, 'learning_rate': 2.1792145058668735e-05, 'epoch': 1.692471296479876, 'step': 1408500}
INFO:transformers.trainer:{'loss': 2.4259756635427476, 'learning_rate': 2.1782131620634892e-05, 'epoch': 1.6930721027619065, 'step': 1409000}
INFO:transformers.trainer:{'loss': 2.4126202626228332, 'learning_rate': 2.177211818260105e-05, 'epoch': 1.6936729090439369, 'step': 1409500}
INFO:transformers.trainer:{'loss': 2.4283736587762834, 'learning_rate': 2.176210474456721e-05, 'epoch': 1.6942737153259675, 'step': 1410000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1410000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1410000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1410000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1390000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4036311975717544, 'learning_rate': 2.1752091306533367e-05, 'epoch': 1.694874521607998, 'step': 1410500}
INFO:transformers.trainer:{'loss': 2.3782979959249495, 'learning_rate': 2.1742077868499527e-05, 'epoch': 1.6954753278900285, 'step': 1411000}
INFO:transformers.trainer:{'loss': 2.3911917085647585, 'learning_rate': 2.1732064430465687e-05, 'epoch': 1.696076134172059, 'step': 1411500}
INFO:transformers.trainer:{'loss': 2.398183176636696, 'learning_rate': 2.1722050992431845e-05, 'epoch': 1.6966769404540893, 'step': 1412000}
INFO:transformers.trainer:{'loss': 2.3813496198654174, 'learning_rate': 2.1712037554398005e-05, 'epoch': 1.6972777467361198, 'step': 1412500}
INFO:transformers.trainer:{'loss': 2.4147388795614244, 'learning_rate': 2.1702024116364162e-05, 'epoch': 1.6978785530181504, 'step': 1413000}
INFO:transformers.trainer:{'loss': 2.4606102315187455, 'learning_rate': 2.1692010678330323e-05, 'epoch': 1.6984793593001808, 'step': 1413500}
INFO:transformers.trainer:{'loss': 2.377345103621483, 'learning_rate': 2.168199724029648e-05, 'epoch': 1.6990801655822114, 'step': 1414000}
INFO:transformers.trainer:{'loss': 2.4444134647846223, 'learning_rate': 2.1671983802262637e-05, 'epoch': 1.6996809718642418, 'step': 1414500}
INFO:transformers.trainer:{'loss': 2.392419435143471, 'learning_rate': 2.1661970364228794e-05, 'epoch': 1.7002817781462722, 'step': 1415000}
INFO:transformers.trainer:{'loss': 2.370191972076893, 'learning_rate': 2.1651956926194954e-05, 'epoch': 1.7008825844283026, 'step': 1415500}
INFO:transformers.trainer:{'loss': 2.4057910336256025, 'learning_rate': 2.164194348816111e-05, 'epoch': 1.7014833907103333, 'step': 1416000}
INFO:transformers.trainer:{'loss': 2.414523366212845, 'learning_rate': 2.163193005012727e-05, 'epoch': 1.702084196992364, 'step': 1416500}
INFO:transformers.trainer:{'loss': 2.4038714398145675, 'learning_rate': 2.162191661209343e-05, 'epoch': 1.7026850032743943, 'step': 1417000}
INFO:transformers.trainer:{'loss': 2.396776846051216, 'learning_rate': 2.161190317405959e-05, 'epoch': 1.7032858095564247, 'step': 1417500}
INFO:transformers.trainer:{'loss': 2.4146725405454634, 'learning_rate': 2.160188973602575e-05, 'epoch': 1.7038866158384551, 'step': 1418000}
INFO:transformers.trainer:{'loss': 2.379537750005722, 'learning_rate': 2.1591876297991907e-05, 'epoch': 1.7044874221204855, 'step': 1418500}
INFO:transformers.trainer:{'loss': 2.3988649455308915, 'learning_rate': 2.1581862859958067e-05, 'epoch': 1.7050882284025162, 'step': 1419000}
INFO:transformers.trainer:{'loss': 2.3965036107301714, 'learning_rate': 2.1571849421924224e-05, 'epoch': 1.7056890346845468, 'step': 1419500}
INFO:transformers.trainer:{'loss': 2.399895297169685, 'learning_rate': 2.156183598389038e-05, 'epoch': 1.7062898409665772, 'step': 1420000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1420000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1420000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1420000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1400000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4168175800442695, 'learning_rate': 2.155182254585654e-05, 'epoch': 1.7068906472486076, 'step': 1420500}
INFO:transformers.trainer:{'loss': 2.351066634297371, 'learning_rate': 2.15418091078227e-05, 'epoch': 1.707491453530638, 'step': 1421000}
INFO:transformers.trainer:{'loss': 2.3750475132465363, 'learning_rate': 2.1531795669788856e-05, 'epoch': 1.7080922598126687, 'step': 1421500}
INFO:transformers.trainer:{'loss': 2.3562742162942887, 'learning_rate': 2.1521782231755016e-05, 'epoch': 1.708693066094699, 'step': 1422000}
INFO:transformers.trainer:{'loss': 2.3542812271118163, 'learning_rate': 2.1511768793721173e-05, 'epoch': 1.7092938723767297, 'step': 1422500}
INFO:transformers.trainer:{'loss': 2.3826635830402374, 'learning_rate': 2.1501755355687334e-05, 'epoch': 1.70989467865876, 'step': 1423000}
INFO:transformers.trainer:{'loss': 2.474220920443535, 'learning_rate': 2.149174191765349e-05, 'epoch': 1.7104954849407905, 'step': 1423500}
INFO:transformers.trainer:{'loss': 2.3599672013521196, 'learning_rate': 2.148172847961965e-05, 'epoch': 1.711096291222821, 'step': 1424000}
INFO:transformers.trainer:{'loss': 2.3370476923584937, 'learning_rate': 2.1471715041585812e-05, 'epoch': 1.7116970975048516, 'step': 1424500}
INFO:transformers.trainer:{'loss': 2.3942696619033814, 'learning_rate': 2.146170160355197e-05, 'epoch': 1.712297903786882, 'step': 1425000}
INFO:transformers.trainer:{'loss': 2.424923632502556, 'learning_rate': 2.1451688165518126e-05, 'epoch': 1.7128987100689126, 'step': 1425500}
INFO:transformers.trainer:{'loss': 2.4086590156555174, 'learning_rate': 2.1441674727484283e-05, 'epoch': 1.713499516350943, 'step': 1426000}
INFO:transformers.trainer:{'loss': 2.3300392796993257, 'learning_rate': 2.1431661289450443e-05, 'epoch': 1.7141003226329734, 'step': 1426500}
INFO:transformers.trainer:{'loss': 2.4245887166261673, 'learning_rate': 2.14216478514166e-05, 'epoch': 1.7147011289150038, 'step': 1427000}
INFO:transformers.trainer:{'loss': 2.3960797529220583, 'learning_rate': 2.141163441338276e-05, 'epoch': 1.7153019351970344, 'step': 1427500}
INFO:transformers.trainer:{'loss': 2.3465723804235457, 'learning_rate': 2.1401620975348918e-05, 'epoch': 1.7159027414790649, 'step': 1428000}
INFO:transformers.trainer:{'loss': 2.416817944765091, 'learning_rate': 2.139160753731508e-05, 'epoch': 1.7165035477610955, 'step': 1428500}
INFO:transformers.trainer:{'loss': 2.4379614889621735, 'learning_rate': 2.1381594099281236e-05, 'epoch': 1.717104354043126, 'step': 1429000}
INFO:transformers.trainer:{'loss': 2.395221207678318, 'learning_rate': 2.1371580661247396e-05, 'epoch': 1.7177051603251563, 'step': 1429500}
INFO:transformers.trainer:{'loss': 2.3655406259298326, 'learning_rate': 2.1361567223213556e-05, 'epoch': 1.7183059666071867, 'step': 1430000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1430000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1430000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1430000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1410000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.404320490837097, 'learning_rate': 2.1351553785179714e-05, 'epoch': 1.7189067728892173, 'step': 1430500}
INFO:transformers.trainer:{'loss': 2.4294405460357664, 'learning_rate': 2.134154034714587e-05, 'epoch': 1.719507579171248, 'step': 1431000}
INFO:transformers.trainer:{'loss': 2.349176200270653, 'learning_rate': 2.1331526909112028e-05, 'epoch': 1.7201083854532784, 'step': 1431500}
INFO:transformers.trainer:{'loss': 2.3803629364967347, 'learning_rate': 2.1321513471078188e-05, 'epoch': 1.7207091917353088, 'step': 1432000}
INFO:transformers.trainer:{'loss': 2.404535128593445, 'learning_rate': 2.1311500033044345e-05, 'epoch': 1.7213099980173392, 'step': 1432500}
INFO:transformers.trainer:{'loss': 2.3850045533180237, 'learning_rate': 2.1301486595010506e-05, 'epoch': 1.7219108042993696, 'step': 1433000}
INFO:transformers.trainer:{'loss': 2.367554829955101, 'learning_rate': 2.1291473156976663e-05, 'epoch': 1.7225116105814002, 'step': 1433500}
INFO:transformers.trainer:{'loss': 2.367248806118965, 'learning_rate': 2.1281459718942823e-05, 'epoch': 1.7231124168634309, 'step': 1434000}
INFO:transformers.trainer:{'loss': 2.394754132628441, 'learning_rate': 2.127144628090898e-05, 'epoch': 1.7237132231454613, 'step': 1434500}
INFO:transformers.trainer:{'loss': 2.399020922780037, 'learning_rate': 2.126143284287514e-05, 'epoch': 1.7243140294274917, 'step': 1435000}
INFO:transformers.trainer:{'loss': 2.404228944301605, 'learning_rate': 2.1251419404841298e-05, 'epoch': 1.724914835709522, 'step': 1435500}
INFO:transformers.trainer:{'loss': 2.383214437842369, 'learning_rate': 2.1241405966807458e-05, 'epoch': 1.7255156419915527, 'step': 1436000}
INFO:transformers.trainer:{'loss': 2.3121837232112883, 'learning_rate': 2.1231392528773615e-05, 'epoch': 1.7261164482735831, 'step': 1436500}
INFO:transformers.trainer:{'loss': 2.4330933396816254, 'learning_rate': 2.1221379090739772e-05, 'epoch': 1.7267172545556138, 'step': 1437000}
INFO:transformers.trainer:{'loss': 2.400697188735008, 'learning_rate': 2.1211365652705933e-05, 'epoch': 1.7273180608376442, 'step': 1437500}
INFO:transformers.trainer:{'loss': 2.3985536071658133, 'learning_rate': 2.120135221467209e-05, 'epoch': 1.7279188671196746, 'step': 1438000}
INFO:transformers.trainer:{'loss': 2.3560203622579574, 'learning_rate': 2.119133877663825e-05, 'epoch': 1.728519673401705, 'step': 1438500}
INFO:transformers.trainer:{'loss': 2.328020794093609, 'learning_rate': 2.1181325338604407e-05, 'epoch': 1.7291204796837356, 'step': 1439000}
INFO:transformers.trainer:{'loss': 2.3726117612123487, 'learning_rate': 2.1171311900570568e-05, 'epoch': 1.729721285965766, 'step': 1439500}
INFO:transformers.trainer:{'loss': 2.3670730583667754, 'learning_rate': 2.1161298462536725e-05, 'epoch': 1.7303220922477967, 'step': 1440000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1440000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1440000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1440000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1420000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3979162026643754, 'learning_rate': 2.1151285024502885e-05, 'epoch': 1.730922898529827, 'step': 1440500}
INFO:transformers.trainer:{'loss': 2.3953732229471205, 'learning_rate': 2.1141271586469042e-05, 'epoch': 1.7315237048118575, 'step': 1441000}
INFO:transformers.trainer:{'loss': 2.4020494480133054, 'learning_rate': 2.1131258148435203e-05, 'epoch': 1.7321245110938879, 'step': 1441500}
INFO:transformers.trainer:{'loss': 2.336034774303436, 'learning_rate': 2.112124471040136e-05, 'epoch': 1.7327253173759185, 'step': 1442000}
INFO:transformers.trainer:{'loss': 2.3303719365596773, 'learning_rate': 2.1111231272367517e-05, 'epoch': 1.733326123657949, 'step': 1442500}
INFO:transformers.trainer:{'loss': 2.4361409789323805, 'learning_rate': 2.1101217834333677e-05, 'epoch': 1.7339269299399795, 'step': 1443000}
INFO:transformers.trainer:{'loss': 2.45118691277504, 'learning_rate': 2.1091204396299834e-05, 'epoch': 1.73452773622201, 'step': 1443500}
INFO:transformers.trainer:{'loss': 2.366223838329315, 'learning_rate': 2.1081190958265995e-05, 'epoch': 1.7351285425040404, 'step': 1444000}
INFO:transformers.trainer:{'loss': 2.396815281510353, 'learning_rate': 2.1071177520232152e-05, 'epoch': 1.7357293487860708, 'step': 1444500}
INFO:transformers.trainer:{'loss': 2.3906108202934266, 'learning_rate': 2.1061164082198312e-05, 'epoch': 1.7363301550681014, 'step': 1445000}
INFO:transformers.trainer:{'loss': 2.4055338115692138, 'learning_rate': 2.105115064416447e-05, 'epoch': 1.736930961350132, 'step': 1445500}
INFO:transformers.trainer:{'loss': 2.408063099861145, 'learning_rate': 2.104113720613063e-05, 'epoch': 1.7375317676321624, 'step': 1446000}
INFO:transformers.trainer:{'loss': 2.3863189572095873, 'learning_rate': 2.1031123768096787e-05, 'epoch': 1.7381325739141928, 'step': 1446500}
INFO:transformers.trainer:{'loss': 2.371054083228111, 'learning_rate': 2.1021110330062947e-05, 'epoch': 1.7387333801962233, 'step': 1447000}
INFO:transformers.trainer:{'loss': 2.3703129163980483, 'learning_rate': 2.1011096892029105e-05, 'epoch': 1.7393341864782537, 'step': 1447500}
INFO:transformers.trainer:{'loss': 2.391579332590103, 'learning_rate': 2.100108345399526e-05, 'epoch': 1.7399349927602843, 'step': 1448000}
INFO:transformers.trainer:{'loss': 2.4388140127658846, 'learning_rate': 2.099107001596142e-05, 'epoch': 1.740535799042315, 'step': 1448500}
INFO:transformers.trainer:{'loss': 2.4019008269309996, 'learning_rate': 2.098105657792758e-05, 'epoch': 1.7411366053243453, 'step': 1449000}
INFO:transformers.trainer:{'loss': 2.4003939899206164, 'learning_rate': 2.097104313989374e-05, 'epoch': 1.7417374116063757, 'step': 1449500}
INFO:transformers.trainer:{'loss': 2.3743092703819273, 'learning_rate': 2.0961029701859897e-05, 'epoch': 1.7423382178884061, 'step': 1450000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1450000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1450000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1450000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1430000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4039134707450867, 'learning_rate': 2.0951016263826057e-05, 'epoch': 1.7429390241704368, 'step': 1450500}
INFO:transformers.trainer:{'loss': 2.4294945542812347, 'learning_rate': 2.0941002825792214e-05, 'epoch': 1.7435398304524672, 'step': 1451000}
INFO:transformers.trainer:{'loss': 2.365617668747902, 'learning_rate': 2.0930989387758375e-05, 'epoch': 1.7441406367344978, 'step': 1451500}
INFO:transformers.trainer:{'loss': 2.378998404383659, 'learning_rate': 2.092097594972453e-05, 'epoch': 1.7447414430165282, 'step': 1452000}
INFO:transformers.trainer:{'loss': 2.3495746052265165, 'learning_rate': 2.0910962511690692e-05, 'epoch': 1.7453422492985586, 'step': 1452500}
INFO:transformers.trainer:{'loss': 2.3923620127439498, 'learning_rate': 2.090094907365685e-05, 'epoch': 1.745943055580589, 'step': 1453000}
INFO:transformers.trainer:{'loss': 2.434238377690315, 'learning_rate': 2.0890935635623006e-05, 'epoch': 1.7465438618626197, 'step': 1453500}
INFO:transformers.trainer:{'loss': 2.381493362188339, 'learning_rate': 2.0880922197589163e-05, 'epoch': 1.74714466814465, 'step': 1454000}
INFO:transformers.trainer:{'loss': 2.4321608222723006, 'learning_rate': 2.0870908759555324e-05, 'epoch': 1.7477454744266807, 'step': 1454500}
INFO:transformers.trainer:{'loss': 2.3948613165616988, 'learning_rate': 2.0860895321521484e-05, 'epoch': 1.7483462807087111, 'step': 1455000}
INFO:transformers.trainer:{'loss': 2.398719661593437, 'learning_rate': 2.085088188348764e-05, 'epoch': 1.7489470869907415, 'step': 1455500}
INFO:transformers.trainer:{'loss': 2.3905771856307982, 'learning_rate': 2.0840868445453802e-05, 'epoch': 1.749547893272772, 'step': 1456000}
INFO:transformers.trainer:{'loss': 2.4062351917028426, 'learning_rate': 2.083085500741996e-05, 'epoch': 1.7501486995548026, 'step': 1456500}
INFO:transformers.trainer:{'loss': 2.3196055603027346, 'learning_rate': 2.082084156938612e-05, 'epoch': 1.750749505836833, 'step': 1457000}
INFO:transformers.trainer:{'loss': 2.370865937232971, 'learning_rate': 2.0810828131352276e-05, 'epoch': 1.7513503121188636, 'step': 1457500}
INFO:transformers.trainer:{'loss': 2.425047003030777, 'learning_rate': 2.0800814693318433e-05, 'epoch': 1.751951118400894, 'step': 1458000}
INFO:transformers.trainer:{'loss': 2.376558589577675, 'learning_rate': 2.079080125528459e-05, 'epoch': 1.7525519246829244, 'step': 1458500}
INFO:transformers.trainer:{'loss': 2.363333055138588, 'learning_rate': 2.078078781725075e-05, 'epoch': 1.7531527309649548, 'step': 1459000}
INFO:transformers.trainer:{'loss': 2.353012748003006, 'learning_rate': 2.0770774379216908e-05, 'epoch': 1.7537535372469855, 'step': 1459500}
INFO:transformers.trainer:{'loss': 2.399633957862854, 'learning_rate': 2.076076094118307e-05, 'epoch': 1.754354343529016, 'step': 1460000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1460000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1460000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1460000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1440000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3840645290613174, 'learning_rate': 2.0750747503149225e-05, 'epoch': 1.7549551498110465, 'step': 1460500}
INFO:transformers.trainer:{'loss': 2.344745134949684, 'learning_rate': 2.0740734065115386e-05, 'epoch': 1.755555956093077, 'step': 1461000}
INFO:transformers.trainer:{'loss': 2.3725004810094834, 'learning_rate': 2.0730720627081546e-05, 'epoch': 1.7561567623751073, 'step': 1461500}
INFO:transformers.trainer:{'loss': 2.3908626511096953, 'learning_rate': 2.0720707189047703e-05, 'epoch': 1.7567575686571377, 'step': 1462000}
INFO:transformers.trainer:{'loss': 2.3618392423391343, 'learning_rate': 2.0710693751013864e-05, 'epoch': 1.7573583749391684, 'step': 1462500}
INFO:transformers.trainer:{'loss': 2.34383573281765, 'learning_rate': 2.070068031298002e-05, 'epoch': 1.757959181221199, 'step': 1463000}
INFO:transformers.trainer:{'loss': 2.3468922933340073, 'learning_rate': 2.0690666874946178e-05, 'epoch': 1.7585599875032294, 'step': 1463500}
INFO:transformers.trainer:{'loss': 2.445652477145195, 'learning_rate': 2.0680653436912335e-05, 'epoch': 1.7591607937852598, 'step': 1464000}
INFO:transformers.trainer:{'loss': 2.3466410673856735, 'learning_rate': 2.0670639998878496e-05, 'epoch': 1.7597616000672902, 'step': 1464500}
INFO:transformers.trainer:{'loss': 2.3618174381256103, 'learning_rate': 2.0660626560844653e-05, 'epoch': 1.7603624063493208, 'step': 1465000}
INFO:transformers.trainer:{'loss': 2.4134006145000457, 'learning_rate': 2.0650613122810813e-05, 'epoch': 1.7609632126313512, 'step': 1465500}
INFO:transformers.trainer:{'loss': 2.3903871467113493, 'learning_rate': 2.064059968477697e-05, 'epoch': 1.7615640189133819, 'step': 1466000}
INFO:transformers.trainer:{'loss': 2.36192720580101, 'learning_rate': 2.063058624674313e-05, 'epoch': 1.7621648251954123, 'step': 1466500}
INFO:transformers.trainer:{'loss': 2.383679861664772, 'learning_rate': 2.0620572808709288e-05, 'epoch': 1.7627656314774427, 'step': 1467000}
INFO:transformers.trainer:{'loss': 2.340079160928726, 'learning_rate': 2.0610559370675448e-05, 'epoch': 1.763366437759473, 'step': 1467500}
INFO:transformers.trainer:{'loss': 2.313317456007004, 'learning_rate': 2.060054593264161e-05, 'epoch': 1.7639672440415037, 'step': 1468000}
INFO:transformers.trainer:{'loss': 2.3677505957484244, 'learning_rate': 2.0590532494607766e-05, 'epoch': 1.7645680503235341, 'step': 1468500}
INFO:transformers.trainer:{'loss': 2.3714975694417952, 'learning_rate': 2.0580519056573923e-05, 'epoch': 1.7651688566055648, 'step': 1469000}
INFO:transformers.trainer:{'loss': 2.3724484338760377, 'learning_rate': 2.057050561854008e-05, 'epoch': 1.7657696628875952, 'step': 1469500}
INFO:transformers.trainer:{'loss': 2.346602556347847, 'learning_rate': 2.056049218050624e-05, 'epoch': 1.7663704691696256, 'step': 1470000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1470000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1470000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1470000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1450000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.412013220191002, 'learning_rate': 2.0550478742472397e-05, 'epoch': 1.766971275451656, 'step': 1470500}
INFO:transformers.trainer:{'loss': 2.348334911584854, 'learning_rate': 2.0540465304438558e-05, 'epoch': 1.7675720817336866, 'step': 1471000}
INFO:transformers.trainer:{'loss': 2.336551463961601, 'learning_rate': 2.0530451866404715e-05, 'epoch': 1.768172888015717, 'step': 1471500}
INFO:transformers.trainer:{'loss': 2.410290223121643, 'learning_rate': 2.0520438428370875e-05, 'epoch': 1.7687736942977477, 'step': 1472000}
INFO:transformers.trainer:{'loss': 2.3840318084955214, 'learning_rate': 2.0510424990337032e-05, 'epoch': 1.769374500579778, 'step': 1472500}
INFO:transformers.trainer:{'loss': 2.368320355415344, 'learning_rate': 2.0500411552303193e-05, 'epoch': 1.7699753068618085, 'step': 1473000}
INFO:transformers.trainer:{'loss': 2.3957112596035004, 'learning_rate': 2.049039811426935e-05, 'epoch': 1.770576113143839, 'step': 1473500}
INFO:transformers.trainer:{'loss': 2.399389892578125, 'learning_rate': 2.048038467623551e-05, 'epoch': 1.7711769194258695, 'step': 1474000}
INFO:transformers.trainer:{'loss': 2.3253422726392747, 'learning_rate': 2.0470371238201667e-05, 'epoch': 1.7717777257079002, 'step': 1474500}
INFO:transformers.trainer:{'loss': 2.417102090716362, 'learning_rate': 2.0460357800167824e-05, 'epoch': 1.7723785319899306, 'step': 1475000}
INFO:transformers.trainer:{'loss': 2.403551530897617, 'learning_rate': 2.0450344362133985e-05, 'epoch': 1.772979338271961, 'step': 1475500}
INFO:transformers.trainer:{'loss': 2.41886541056633, 'learning_rate': 2.0440330924100142e-05, 'epoch': 1.7735801445539914, 'step': 1476000}
INFO:transformers.trainer:{'loss': 2.37746879029274, 'learning_rate': 2.0430317486066302e-05, 'epoch': 1.7741809508360218, 'step': 1476500}
INFO:transformers.trainer:{'loss': 2.4195931833982467, 'learning_rate': 2.042030404803246e-05, 'epoch': 1.7747817571180524, 'step': 1477000}
INFO:transformers.trainer:{'loss': 2.314179624199867, 'learning_rate': 2.041029060999862e-05, 'epoch': 1.775382563400083, 'step': 1477500}
INFO:transformers.trainer:{'loss': 2.4025094599723817, 'learning_rate': 2.0400277171964777e-05, 'epoch': 1.7759833696821135, 'step': 1478000}
INFO:transformers.trainer:{'loss': 2.3161533818244933, 'learning_rate': 2.0390263733930937e-05, 'epoch': 1.7765841759641439, 'step': 1478500}
INFO:transformers.trainer:{'loss': 2.381801629245281, 'learning_rate': 2.0380250295897094e-05, 'epoch': 1.7771849822461743, 'step': 1479000}
INFO:transformers.trainer:{'loss': 2.4273038325309755, 'learning_rate': 2.0370236857863255e-05, 'epoch': 1.777785788528205, 'step': 1479500}
INFO:transformers.trainer:{'loss': 2.365010456442833, 'learning_rate': 2.0360223419829412e-05, 'epoch': 1.7783865948102353, 'step': 1480000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1480000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1480000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1480000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1460000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3752973350286486, 'learning_rate': 2.035020998179557e-05, 'epoch': 1.778987401092266, 'step': 1480500}
INFO:transformers.trainer:{'loss': 2.459543715596199, 'learning_rate': 2.034019654376173e-05, 'epoch': 1.7795882073742963, 'step': 1481000}
INFO:transformers.trainer:{'loss': 2.324263366818428, 'learning_rate': 2.0330183105727887e-05, 'epoch': 1.7801890136563268, 'step': 1481500}
INFO:transformers.trainer:{'loss': 2.3572564074993134, 'learning_rate': 2.0320169667694047e-05, 'epoch': 1.7807898199383572, 'step': 1482000}
INFO:transformers.trainer:{'loss': 2.313877908706665, 'learning_rate': 2.0310156229660204e-05, 'epoch': 1.7813906262203878, 'step': 1482500}
INFO:transformers.trainer:{'loss': 2.370221412062645, 'learning_rate': 2.0300142791626365e-05, 'epoch': 1.7819914325024182, 'step': 1483000}
INFO:transformers.trainer:{'loss': 2.422543540596962, 'learning_rate': 2.029012935359252e-05, 'epoch': 1.7825922387844488, 'step': 1483500}
INFO:transformers.trainer:{'loss': 2.4102696089744566, 'learning_rate': 2.0280115915558682e-05, 'epoch': 1.7831930450664792, 'step': 1484000}
INFO:transformers.trainer:{'loss': 2.3897017966508867, 'learning_rate': 2.027010247752484e-05, 'epoch': 1.7837938513485097, 'step': 1484500}
INFO:transformers.trainer:{'loss': 2.391726013422012, 'learning_rate': 2.0260089039491e-05, 'epoch': 1.78439465763054, 'step': 1485000}
INFO:transformers.trainer:{'loss': 2.3798442569971083, 'learning_rate': 2.0250075601457157e-05, 'epoch': 1.7849954639125707, 'step': 1485500}
INFO:transformers.trainer:{'loss': 2.373507956027985, 'learning_rate': 2.0240062163423314e-05, 'epoch': 1.7855962701946013, 'step': 1486000}
INFO:transformers.trainer:{'loss': 2.3676500294804574, 'learning_rate': 2.0230048725389474e-05, 'epoch': 1.7861970764766317, 'step': 1486500}
INFO:transformers.trainer:{'loss': 2.425291105747223, 'learning_rate': 2.022003528735563e-05, 'epoch': 1.7867978827586621, 'step': 1487000}
INFO:transformers.trainer:{'loss': 2.3614282358884813, 'learning_rate': 2.021002184932179e-05, 'epoch': 1.7873986890406925, 'step': 1487500}
INFO:transformers.trainer:{'loss': 2.3450812888145447, 'learning_rate': 2.020000841128795e-05, 'epoch': 1.787999495322723, 'step': 1488000}
INFO:transformers.trainer:{'loss': 2.3520359836816787, 'learning_rate': 2.018999497325411e-05, 'epoch': 1.7886003016047536, 'step': 1488500}
INFO:transformers.trainer:{'loss': 2.3621919181346893, 'learning_rate': 2.0179981535220266e-05, 'epoch': 1.7892011078867842, 'step': 1489000}
INFO:transformers.trainer:{'loss': 2.3399187141656874, 'learning_rate': 2.0169968097186427e-05, 'epoch': 1.7898019141688146, 'step': 1489500}
INFO:transformers.trainer:{'loss': 2.36704703104496, 'learning_rate': 2.0159954659152584e-05, 'epoch': 1.790402720450845, 'step': 1490000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1490000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1490000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1490000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1470000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3882327392101286, 'learning_rate': 2.0149941221118744e-05, 'epoch': 1.7910035267328754, 'step': 1490500}
INFO:transformers.trainer:{'loss': 2.3216862806081773, 'learning_rate': 2.01399277830849e-05, 'epoch': 1.7916043330149058, 'step': 1491000}
INFO:transformers.trainer:{'loss': 2.387273351997137, 'learning_rate': 2.012991434505106e-05, 'epoch': 1.7922051392969365, 'step': 1491500}
INFO:transformers.trainer:{'loss': 2.436544211268425, 'learning_rate': 2.0119900907017215e-05, 'epoch': 1.792805945578967, 'step': 1492000}
INFO:transformers.trainer:{'loss': 2.355600136399269, 'learning_rate': 2.0109887468983376e-05, 'epoch': 1.7934067518609975, 'step': 1492500}
INFO:transformers.trainer:{'loss': 2.3743217779397963, 'learning_rate': 2.0099874030949536e-05, 'epoch': 1.794007558143028, 'step': 1493000}
INFO:transformers.trainer:{'loss': 2.3627056090831755, 'learning_rate': 2.0089860592915693e-05, 'epoch': 1.7946083644250583, 'step': 1493500}
INFO:transformers.trainer:{'loss': 2.4020487996339797, 'learning_rate': 2.0079847154881854e-05, 'epoch': 1.795209170707089, 'step': 1494000}
INFO:transformers.trainer:{'loss': 2.4395973719358444, 'learning_rate': 2.006983371684801e-05, 'epoch': 1.7958099769891194, 'step': 1494500}
INFO:transformers.trainer:{'loss': 2.3178306658267975, 'learning_rate': 2.005982027881417e-05, 'epoch': 1.79641078327115, 'step': 1495000}
INFO:transformers.trainer:{'loss': 2.3291925147175787, 'learning_rate': 2.004980684078033e-05, 'epoch': 1.7970115895531804, 'step': 1495500}
INFO:transformers.trainer:{'loss': 2.4510749024152756, 'learning_rate': 2.003979340274649e-05, 'epoch': 1.7976123958352108, 'step': 1496000}
INFO:transformers.trainer:{'loss': 2.3104618075489998, 'learning_rate': 2.0029779964712646e-05, 'epoch': 1.7982132021172412, 'step': 1496500}
INFO:transformers.trainer:{'loss': 2.3829523606300356, 'learning_rate': 2.0019766526678803e-05, 'epoch': 1.7988140083992719, 'step': 1497000}
INFO:transformers.trainer:{'loss': 2.349914570093155, 'learning_rate': 2.000975308864496e-05, 'epoch': 1.7994148146813023, 'step': 1497500}
INFO:transformers.trainer:{'loss': 2.3626376280784607, 'learning_rate': 1.999973965061112e-05, 'epoch': 1.800015620963333, 'step': 1498000}
INFO:transformers.trainer:{'loss': 2.3608779790401457, 'learning_rate': 1.9989726212577278e-05, 'epoch': 1.8006164272453633, 'step': 1498500}
INFO:transformers.trainer:{'loss': 2.361858302474022, 'learning_rate': 1.9979712774543438e-05, 'epoch': 1.8012172335273937, 'step': 1499000}
INFO:transformers.trainer:{'loss': 2.36711394071579, 'learning_rate': 1.99696993365096e-05, 'epoch': 1.8018180398094241, 'step': 1499500}
INFO:transformers.trainer:{'loss': 2.367687205314636, 'learning_rate': 1.9959685898475756e-05, 'epoch': 1.8024188460914548, 'step': 1500000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1500000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1500000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1500000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1480000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3647490228414534, 'learning_rate': 1.9949672460441916e-05, 'epoch': 1.8030196523734854, 'step': 1500500}
INFO:transformers.trainer:{'loss': 2.3722945185899733, 'learning_rate': 1.9939659022408073e-05, 'epoch': 1.8036204586555158, 'step': 1501000}
INFO:transformers.trainer:{'loss': 2.374204132318497, 'learning_rate': 1.9929645584374233e-05, 'epoch': 1.8042212649375462, 'step': 1501500}
INFO:transformers.trainer:{'loss': 2.3740353890657424, 'learning_rate': 1.991963214634039e-05, 'epoch': 1.8048220712195766, 'step': 1502000}
INFO:transformers.trainer:{'loss': 2.36499108672142, 'learning_rate': 1.9909618708306548e-05, 'epoch': 1.805422877501607, 'step': 1502500}
INFO:transformers.trainer:{'loss': 2.337378720164299, 'learning_rate': 1.9899605270272705e-05, 'epoch': 1.8060236837836376, 'step': 1503000}
INFO:transformers.trainer:{'loss': 2.3707465822696685, 'learning_rate': 1.9889591832238865e-05, 'epoch': 1.8066244900656683, 'step': 1503500}
INFO:transformers.trainer:{'loss': 2.347624682545662, 'learning_rate': 1.9879578394205022e-05, 'epoch': 1.8072252963476987, 'step': 1504000}
INFO:transformers.trainer:{'loss': 2.3494965280294418, 'learning_rate': 1.9869564956171183e-05, 'epoch': 1.807826102629729, 'step': 1504500}
INFO:transformers.trainer:{'loss': 2.3554581614732744, 'learning_rate': 1.9859551518137343e-05, 'epoch': 1.8084269089117595, 'step': 1505000}
INFO:transformers.trainer:{'loss': 2.3725276495218277, 'learning_rate': 1.98495380801035e-05, 'epoch': 1.80902771519379, 'step': 1505500}
INFO:transformers.trainer:{'loss': 2.4168299260139467, 'learning_rate': 1.983952464206966e-05, 'epoch': 1.8096285214758205, 'step': 1506000}
INFO:transformers.trainer:{'loss': 2.3395662693977357, 'learning_rate': 1.9829511204035818e-05, 'epoch': 1.8102293277578512, 'step': 1506500}
INFO:transformers.trainer:{'loss': 2.4035845320224762, 'learning_rate': 1.9819497766001978e-05, 'epoch': 1.8108301340398816, 'step': 1507000}
INFO:transformers.trainer:{'loss': 2.368774413347244, 'learning_rate': 1.9809484327968135e-05, 'epoch': 1.811430940321912, 'step': 1507500}
INFO:transformers.trainer:{'loss': 2.408968792319298, 'learning_rate': 1.9799470889934292e-05, 'epoch': 1.8120317466039424, 'step': 1508000}
INFO:transformers.trainer:{'loss': 2.432013726592064, 'learning_rate': 1.978945745190045e-05, 'epoch': 1.812632552885973, 'step': 1508500}
INFO:transformers.trainer:{'loss': 2.3910908081531526, 'learning_rate': 1.977944401386661e-05, 'epoch': 1.8132333591680034, 'step': 1509000}
INFO:transformers.trainer:{'loss': 2.362333563566208, 'learning_rate': 1.9769430575832767e-05, 'epoch': 1.813834165450034, 'step': 1509500}
INFO:transformers.trainer:{'loss': 2.3450233602523802, 'learning_rate': 1.9759417137798927e-05, 'epoch': 1.8144349717320645, 'step': 1510000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1510000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1510000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1510000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1490000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3553335216641424, 'learning_rate': 1.9749403699765084e-05, 'epoch': 1.8150357780140949, 'step': 1510500}
INFO:transformers.trainer:{'loss': 2.3332706431150436, 'learning_rate': 1.9739390261731245e-05, 'epoch': 1.8156365842961253, 'step': 1511000}
INFO:transformers.trainer:{'loss': 2.378729060173035, 'learning_rate': 1.9729376823697405e-05, 'epoch': 1.816237390578156, 'step': 1511500}
INFO:transformers.trainer:{'loss': 2.3739897081851957, 'learning_rate': 1.9719363385663562e-05, 'epoch': 1.8168381968601863, 'step': 1512000}
INFO:transformers.trainer:{'loss': 2.3542329496741297, 'learning_rate': 1.9709349947629723e-05, 'epoch': 1.817439003142217, 'step': 1512500}
INFO:transformers.trainer:{'loss': 2.3907375560998916, 'learning_rate': 1.969933650959588e-05, 'epoch': 1.8180398094242474, 'step': 1513000}
INFO:transformers.trainer:{'loss': 2.3981278462409974, 'learning_rate': 1.9689323071562037e-05, 'epoch': 1.8186406157062778, 'step': 1513500}
INFO:transformers.trainer:{'loss': 2.3688826823830604, 'learning_rate': 1.9679309633528194e-05, 'epoch': 1.8192414219883082, 'step': 1514000}
INFO:transformers.trainer:{'loss': 2.416452994346619, 'learning_rate': 1.9669296195494354e-05, 'epoch': 1.8198422282703388, 'step': 1514500}
INFO:transformers.trainer:{'loss': 2.325665726900101, 'learning_rate': 1.965928275746051e-05, 'epoch': 1.8204430345523694, 'step': 1515000}
INFO:transformers.trainer:{'loss': 2.3341323838233947, 'learning_rate': 1.9649269319426672e-05, 'epoch': 1.8210438408343999, 'step': 1515500}
INFO:transformers.trainer:{'loss': 2.3827337186336517, 'learning_rate': 1.963925588139283e-05, 'epoch': 1.8216446471164303, 'step': 1516000}
INFO:transformers.trainer:{'loss': 2.379252076745033, 'learning_rate': 1.962924244335899e-05, 'epoch': 1.8222454533984607, 'step': 1516500}
INFO:transformers.trainer:{'loss': 2.386230395555496, 'learning_rate': 1.9619229005325147e-05, 'epoch': 1.822846259680491, 'step': 1517000}
INFO:transformers.trainer:{'loss': 2.3222229706048965, 'learning_rate': 1.9609215567291307e-05, 'epoch': 1.8234470659625217, 'step': 1517500}
INFO:transformers.trainer:{'loss': 2.337859096169472, 'learning_rate': 1.9599202129257464e-05, 'epoch': 1.8240478722445523, 'step': 1518000}
INFO:transformers.trainer:{'loss': 2.3918162823915483, 'learning_rate': 1.958918869122362e-05, 'epoch': 1.8246486785265827, 'step': 1518500}
INFO:transformers.trainer:{'loss': 2.3847210896015167, 'learning_rate': 1.957917525318978e-05, 'epoch': 1.8252494848086132, 'step': 1519000}
INFO:transformers.trainer:{'loss': 2.3716936280727388, 'learning_rate': 1.956916181515594e-05, 'epoch': 1.8258502910906436, 'step': 1519500}
INFO:transformers.trainer:{'loss': 2.366171956002712, 'learning_rate': 1.95591483771221e-05, 'epoch': 1.826451097372674, 'step': 1520000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1520000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1520000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1520000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1500000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3372138875722883, 'learning_rate': 1.9549134939088256e-05, 'epoch': 1.8270519036547046, 'step': 1520500}
INFO:transformers.trainer:{'loss': 2.3791823996305466, 'learning_rate': 1.9539121501054417e-05, 'epoch': 1.8276527099367352, 'step': 1521000}
INFO:transformers.trainer:{'loss': 2.374631217837334, 'learning_rate': 1.9529108063020574e-05, 'epoch': 1.8282535162187656, 'step': 1521500}
INFO:transformers.trainer:{'loss': 2.3382650455236433, 'learning_rate': 1.9519094624986734e-05, 'epoch': 1.828854322500796, 'step': 1522000}
INFO:transformers.trainer:{'loss': 2.3588435177803038, 'learning_rate': 1.950908118695289e-05, 'epoch': 1.8294551287828265, 'step': 1522500}
INFO:transformers.trainer:{'loss': 2.3757602013349532, 'learning_rate': 1.949906774891905e-05, 'epoch': 1.830055935064857, 'step': 1523000}
INFO:transformers.trainer:{'loss': 2.39579781794548, 'learning_rate': 1.948905431088521e-05, 'epoch': 1.8306567413468875, 'step': 1523500}
INFO:transformers.trainer:{'loss': 2.4004931926727293, 'learning_rate': 1.9479040872851366e-05, 'epoch': 1.8312575476289181, 'step': 1524000}
INFO:transformers.trainer:{'loss': 2.3653180446624757, 'learning_rate': 1.9469027434817526e-05, 'epoch': 1.8318583539109485, 'step': 1524500}
INFO:transformers.trainer:{'loss': 2.337472694516182, 'learning_rate': 1.9459013996783683e-05, 'epoch': 1.832459160192979, 'step': 1525000}
INFO:transformers.trainer:{'loss': 2.3411859320402146, 'learning_rate': 1.9449000558749844e-05, 'epoch': 1.8330599664750094, 'step': 1525500}
INFO:transformers.trainer:{'loss': 2.3723902229070664, 'learning_rate': 1.9438987120716e-05, 'epoch': 1.83366077275704, 'step': 1526000}
INFO:transformers.trainer:{'loss': 2.3477304594516752, 'learning_rate': 1.942897368268216e-05, 'epoch': 1.8342615790390704, 'step': 1526500}
INFO:transformers.trainer:{'loss': 2.4387119116783142, 'learning_rate': 1.9418960244648318e-05, 'epoch': 1.834862385321101, 'step': 1527000}
INFO:transformers.trainer:{'loss': 2.373479528427124, 'learning_rate': 1.940894680661448e-05, 'epoch': 1.8354631916031314, 'step': 1527500}
INFO:transformers.trainer:{'loss': 2.433411319732666, 'learning_rate': 1.9398933368580636e-05, 'epoch': 1.8360639978851618, 'step': 1528000}
INFO:transformers.trainer:{'loss': 2.3488173003196717, 'learning_rate': 1.9388919930546796e-05, 'epoch': 1.8366648041671922, 'step': 1528500}
INFO:transformers.trainer:{'loss': 2.3888801617622377, 'learning_rate': 1.9378906492512953e-05, 'epoch': 1.8372656104492229, 'step': 1529000}
INFO:transformers.trainer:{'loss': 2.3918544137477875, 'learning_rate': 1.936889305447911e-05, 'epoch': 1.8378664167312535, 'step': 1529500}
INFO:transformers.trainer:{'loss': 2.4102262500524523, 'learning_rate': 1.935887961644527e-05, 'epoch': 1.838467223013284, 'step': 1530000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1530000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1530000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1530000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1510000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.4135444456338884, 'learning_rate': 1.9348866178411428e-05, 'epoch': 1.8390680292953143, 'step': 1530500}
INFO:transformers.trainer:{'loss': 2.380858041524887, 'learning_rate': 1.933885274037759e-05, 'epoch': 1.8396688355773447, 'step': 1531000}
INFO:transformers.trainer:{'loss': 2.4167788652181623, 'learning_rate': 1.9328839302343745e-05, 'epoch': 1.8402696418593751, 'step': 1531500}
INFO:transformers.trainer:{'loss': 2.37048531293869, 'learning_rate': 1.9318825864309906e-05, 'epoch': 1.8408704481414058, 'step': 1532000}
INFO:transformers.trainer:{'loss': 2.3469933400154113, 'learning_rate': 1.9308812426276063e-05, 'epoch': 1.8414712544234364, 'step': 1532500}
INFO:transformers.trainer:{'loss': 2.3659364657402038, 'learning_rate': 1.9298798988242223e-05, 'epoch': 1.8420720607054668, 'step': 1533000}
INFO:transformers.trainer:{'loss': 2.26990204167366, 'learning_rate': 1.928878555020838e-05, 'epoch': 1.8426728669874972, 'step': 1533500}
INFO:transformers.trainer:{'loss': 2.343247791647911, 'learning_rate': 1.927877211217454e-05, 'epoch': 1.8432736732695276, 'step': 1534000}
INFO:transformers.trainer:{'loss': 2.3428772711753845, 'learning_rate': 1.9268758674140698e-05, 'epoch': 1.843874479551558, 'step': 1534500}
INFO:transformers.trainer:{'loss': 2.4166385153532026, 'learning_rate': 1.9258745236106855e-05, 'epoch': 1.8444752858335887, 'step': 1535000}
INFO:transformers.trainer:{'loss': 2.371347471833229, 'learning_rate': 1.9248731798073012e-05, 'epoch': 1.8450760921156193, 'step': 1535500}
INFO:transformers.trainer:{'loss': 2.3616787655353546, 'learning_rate': 1.9238718360039173e-05, 'epoch': 1.8456768983976497, 'step': 1536000}
INFO:transformers.trainer:{'loss': 2.343225228071213, 'learning_rate': 1.9228704922005333e-05, 'epoch': 1.84627770467968, 'step': 1536500}
INFO:transformers.trainer:{'loss': 2.341168236732483, 'learning_rate': 1.921869148397149e-05, 'epoch': 1.8468785109617105, 'step': 1537000}
INFO:transformers.trainer:{'loss': 2.355381167769432, 'learning_rate': 1.920867804593765e-05, 'epoch': 1.8474793172437411, 'step': 1537500}
INFO:transformers.trainer:{'loss': 2.34988914334774, 'learning_rate': 1.9198664607903808e-05, 'epoch': 1.8480801235257716, 'step': 1538000}
INFO:transformers.trainer:{'loss': 2.3412904990911483, 'learning_rate': 1.9188651169869968e-05, 'epoch': 1.8486809298078022, 'step': 1538500}
INFO:transformers.trainer:{'loss': 2.3421723682284354, 'learning_rate': 1.9178637731836125e-05, 'epoch': 1.8492817360898326, 'step': 1539000}
INFO:transformers.trainer:{'loss': 2.356823424220085, 'learning_rate': 1.9168624293802286e-05, 'epoch': 1.849882542371863, 'step': 1539500}
INFO:transformers.trainer:{'loss': 2.4527756234407425, 'learning_rate': 1.9158610855768443e-05, 'epoch': 1.8504833486538934, 'step': 1540000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1540000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1540000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1540000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1520000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3480922569036484, 'learning_rate': 1.91485974177346e-05, 'epoch': 1.851084154935924, 'step': 1540500}
INFO:transformers.trainer:{'loss': 2.3759709285497665, 'learning_rate': 1.9138583979700757e-05, 'epoch': 1.8516849612179545, 'step': 1541000}
INFO:transformers.trainer:{'loss': 2.3902694504261017, 'learning_rate': 1.9128570541666917e-05, 'epoch': 1.852285767499985, 'step': 1541500}
INFO:transformers.trainer:{'loss': 2.376470906496048, 'learning_rate': 1.9118557103633074e-05, 'epoch': 1.8528865737820155, 'step': 1542000}
INFO:transformers.trainer:{'loss': 2.4037406343221663, 'learning_rate': 1.9108543665599235e-05, 'epoch': 1.853487380064046, 'step': 1542500}
INFO:transformers.trainer:{'loss': 2.320657689332962, 'learning_rate': 1.9098530227565395e-05, 'epoch': 1.8540881863460763, 'step': 1543000}
INFO:transformers.trainer:{'loss': 2.383274020075798, 'learning_rate': 1.9088516789531552e-05, 'epoch': 1.854688992628107, 'step': 1543500}
INFO:transformers.trainer:{'loss': 2.33661729824543, 'learning_rate': 1.9078503351497713e-05, 'epoch': 1.8552897989101376, 'step': 1544000}
INFO:transformers.trainer:{'loss': 2.3683961888551712, 'learning_rate': 1.906848991346387e-05, 'epoch': 1.855890605192168, 'step': 1544500}
INFO:transformers.trainer:{'loss': 2.4302556455135345, 'learning_rate': 1.905847647543003e-05, 'epoch': 1.8564914114741984, 'step': 1545000}
INFO:transformers.trainer:{'loss': 2.407449489593506, 'learning_rate': 1.9048463037396187e-05, 'epoch': 1.8570922177562288, 'step': 1545500}
INFO:transformers.trainer:{'loss': 2.375027670621872, 'learning_rate': 1.9038449599362344e-05, 'epoch': 1.8576930240382592, 'step': 1546000}
INFO:transformers.trainer:{'loss': 2.332766768813133, 'learning_rate': 1.90284361613285e-05, 'epoch': 1.8582938303202898, 'step': 1546500}
INFO:transformers.trainer:{'loss': 2.357451107621193, 'learning_rate': 1.9018422723294662e-05, 'epoch': 1.8588946366023205, 'step': 1547000}
INFO:transformers.trainer:{'loss': 2.3242279469966887, 'learning_rate': 1.900840928526082e-05, 'epoch': 1.8594954428843509, 'step': 1547500}
INFO:transformers.trainer:{'loss': 2.359332666158676, 'learning_rate': 1.899839584722698e-05, 'epoch': 1.8600962491663813, 'step': 1548000}
INFO:transformers.trainer:{'loss': 2.341174481451511, 'learning_rate': 1.8988382409193136e-05, 'epoch': 1.8606970554484117, 'step': 1548500}
INFO:transformers.trainer:{'loss': 2.3831340799331664, 'learning_rate': 1.8978368971159297e-05, 'epoch': 1.861297861730442, 'step': 1549000}
INFO:transformers.trainer:{'loss': 2.326103302359581, 'learning_rate': 1.8968355533125457e-05, 'epoch': 1.8618986680124727, 'step': 1549500}
INFO:transformers.trainer:{'loss': 2.339954561948776, 'learning_rate': 1.8958342095091614e-05, 'epoch': 1.8624994742945034, 'step': 1550000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1550000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1550000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1550000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1530000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3532399128675463, 'learning_rate': 1.8948328657057775e-05, 'epoch': 1.8631002805765338, 'step': 1550500}
INFO:transformers.trainer:{'loss': 2.390803016901016, 'learning_rate': 1.8938315219023932e-05, 'epoch': 1.8637010868585642, 'step': 1551000}
INFO:transformers.trainer:{'loss': 2.349366551399231, 'learning_rate': 1.892830178099009e-05, 'epoch': 1.8643018931405946, 'step': 1551500}
INFO:transformers.trainer:{'loss': 2.3811172918081285, 'learning_rate': 1.8918288342956246e-05, 'epoch': 1.8649026994226252, 'step': 1552000}
INFO:transformers.trainer:{'loss': 2.382877289891243, 'learning_rate': 1.8908274904922406e-05, 'epoch': 1.8655035057046556, 'step': 1552500}
INFO:transformers.trainer:{'loss': 2.3617604943513872, 'learning_rate': 1.8898261466888564e-05, 'epoch': 1.8661043119866862, 'step': 1553000}
INFO:transformers.trainer:{'loss': 2.3459234654903414, 'learning_rate': 1.8888248028854724e-05, 'epoch': 1.8667051182687167, 'step': 1553500}
INFO:transformers.trainer:{'loss': 2.359397111058235, 'learning_rate': 1.887823459082088e-05, 'epoch': 1.867305924550747, 'step': 1554000}
INFO:transformers.trainer:{'loss': 2.3939227243661882, 'learning_rate': 1.886822115278704e-05, 'epoch': 1.8679067308327775, 'step': 1554500}
INFO:transformers.trainer:{'loss': 2.3522065391540528, 'learning_rate': 1.8858207714753202e-05, 'epoch': 1.868507537114808, 'step': 1555000}
INFO:transformers.trainer:{'loss': 2.3917075282335283, 'learning_rate': 1.884819427671936e-05, 'epoch': 1.8691083433968385, 'step': 1555500}
INFO:transformers.trainer:{'loss': 2.3853916807174684, 'learning_rate': 1.883818083868552e-05, 'epoch': 1.8697091496788691, 'step': 1556000}
INFO:transformers.trainer:{'loss': 2.409113641023636, 'learning_rate': 1.8828167400651677e-05, 'epoch': 1.8703099559608996, 'step': 1556500}
INFO:transformers.trainer:{'loss': 2.331265683054924, 'learning_rate': 1.8818153962617834e-05, 'epoch': 1.87091076224293, 'step': 1557000}
INFO:transformers.trainer:{'loss': 2.365345654606819, 'learning_rate': 1.880814052458399e-05, 'epoch': 1.8715115685249604, 'step': 1557500}
INFO:transformers.trainer:{'loss': 2.369017256855965, 'learning_rate': 1.879812708655015e-05, 'epoch': 1.872112374806991, 'step': 1558000}
INFO:transformers.trainer:{'loss': 2.38160351395607, 'learning_rate': 1.8788113648516308e-05, 'epoch': 1.8727131810890216, 'step': 1558500}
INFO:transformers.trainer:{'loss': 2.3173292735219, 'learning_rate': 1.877810021048247e-05, 'epoch': 1.873313987371052, 'step': 1559000}
INFO:transformers.trainer:{'loss': 2.3558444350957872, 'learning_rate': 1.8768086772448626e-05, 'epoch': 1.8739147936530824, 'step': 1559500}
INFO:transformers.trainer:{'loss': 2.3577972763776778, 'learning_rate': 1.8758073334414786e-05, 'epoch': 1.8745155999351129, 'step': 1560000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1560000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1560000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1560000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1540000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.453662746429443, 'learning_rate': 1.8748059896380943e-05, 'epoch': 1.8751164062171433, 'step': 1560500}
INFO:transformers.trainer:{'loss': 2.4000235805511476, 'learning_rate': 1.8738046458347104e-05, 'epoch': 1.875717212499174, 'step': 1561000}
INFO:transformers.trainer:{'loss': 2.3299844119548796, 'learning_rate': 1.8728033020313264e-05, 'epoch': 1.8763180187812045, 'step': 1561500}
INFO:transformers.trainer:{'loss': 2.3494598971009255, 'learning_rate': 1.871801958227942e-05, 'epoch': 1.876918825063235, 'step': 1562000}
INFO:transformers.trainer:{'loss': 2.319255937576294, 'learning_rate': 1.8708006144245578e-05, 'epoch': 1.8775196313452653, 'step': 1562500}
INFO:transformers.trainer:{'loss': 2.413236819386482, 'learning_rate': 1.8697992706211735e-05, 'epoch': 1.8781204376272957, 'step': 1563000}
INFO:transformers.trainer:{'loss': 2.3933265030384065, 'learning_rate': 1.8687979268177896e-05, 'epoch': 1.8787212439093262, 'step': 1563500}
INFO:transformers.trainer:{'loss': 2.3573880826234817, 'learning_rate': 1.8677965830144053e-05, 'epoch': 1.8793220501913568, 'step': 1564000}
INFO:transformers.trainer:{'loss': 2.3328570194244387, 'learning_rate': 1.8667952392110213e-05, 'epoch': 1.8799228564733874, 'step': 1564500}
INFO:transformers.trainer:{'loss': 2.385245396375656, 'learning_rate': 1.865793895407637e-05, 'epoch': 1.8805236627554178, 'step': 1565000}
INFO:transformers.trainer:{'loss': 2.3808471233844757, 'learning_rate': 1.864792551604253e-05, 'epoch': 1.8811244690374482, 'step': 1565500}
INFO:transformers.trainer:{'loss': 2.333885794043541, 'learning_rate': 1.8637912078008688e-05, 'epoch': 1.8817252753194786, 'step': 1566000}
INFO:transformers.trainer:{'loss': 2.3430851846933365, 'learning_rate': 1.862789863997485e-05, 'epoch': 1.8823260816015093, 'step': 1566500}
INFO:transformers.trainer:{'loss': 2.341194540977478, 'learning_rate': 1.8617885201941005e-05, 'epoch': 1.8829268878835397, 'step': 1567000}
INFO:transformers.trainer:{'loss': 2.39409405374527, 'learning_rate': 1.8607871763907166e-05, 'epoch': 1.8835276941655703, 'step': 1567500}
INFO:transformers.trainer:{'loss': 2.3519465028047564, 'learning_rate': 1.8597858325873323e-05, 'epoch': 1.8841285004476007, 'step': 1568000}
INFO:transformers.trainer:{'loss': 2.3768313242197037, 'learning_rate': 1.858784488783948e-05, 'epoch': 1.8847293067296311, 'step': 1568500}
INFO:transformers.trainer:{'loss': 2.3342210354804993, 'learning_rate': 1.857783144980564e-05, 'epoch': 1.8853301130116615, 'step': 1569000}
INFO:transformers.trainer:{'loss': 2.3577726331949234, 'learning_rate': 1.8567818011771797e-05, 'epoch': 1.8859309192936922, 'step': 1569500}
INFO:transformers.trainer:{'loss': 2.3902274339199066, 'learning_rate': 1.8557804573737958e-05, 'epoch': 1.8865317255757226, 'step': 1570000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1570000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1570000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1570000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1550000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.37106712436676, 'learning_rate': 1.8547791135704115e-05, 'epoch': 1.8871325318577532, 'step': 1570500}
INFO:transformers.trainer:{'loss': 2.354737476229668, 'learning_rate': 1.8537777697670275e-05, 'epoch': 1.8877333381397836, 'step': 1571000}
INFO:transformers.trainer:{'loss': 2.3475752637386322, 'learning_rate': 1.8527764259636433e-05, 'epoch': 1.888334144421814, 'step': 1571500}
INFO:transformers.trainer:{'loss': 2.3594797954559326, 'learning_rate': 1.8517750821602593e-05, 'epoch': 1.8889349507038444, 'step': 1572000}
INFO:transformers.trainer:{'loss': 2.3073820232152937, 'learning_rate': 1.850773738356875e-05, 'epoch': 1.889535756985875, 'step': 1572500}
INFO:transformers.trainer:{'loss': 2.319141646504402, 'learning_rate': 1.849772394553491e-05, 'epoch': 1.8901365632679057, 'step': 1573000}
INFO:transformers.trainer:{'loss': 2.332598276257515, 'learning_rate': 1.8487710507501068e-05, 'epoch': 1.890737369549936, 'step': 1573500}
INFO:transformers.trainer:{'loss': 2.3374043288230895, 'learning_rate': 1.8477697069467225e-05, 'epoch': 1.8913381758319665, 'step': 1574000}
INFO:transformers.trainer:{'loss': 2.372745046854019, 'learning_rate': 1.8467683631433385e-05, 'epoch': 1.891938982113997, 'step': 1574500}
INFO:transformers.trainer:{'loss': 2.3378369505405425, 'learning_rate': 1.8457670193399542e-05, 'epoch': 1.8925397883960273, 'step': 1575000}
INFO:transformers.trainer:{'loss': 2.328693304657936, 'learning_rate': 1.8447656755365703e-05, 'epoch': 1.893140594678058, 'step': 1575500}
INFO:transformers.trainer:{'loss': 2.2789484837055207, 'learning_rate': 1.843764331733186e-05, 'epoch': 1.8937414009600886, 'step': 1576000}
INFO:transformers.trainer:{'loss': 2.408052238047123, 'learning_rate': 1.842762987929802e-05, 'epoch': 1.894342207242119, 'step': 1576500}
INFO:transformers.trainer:{'loss': 2.3201562728881835, 'learning_rate': 1.8417616441264177e-05, 'epoch': 1.8949430135241494, 'step': 1577000}
INFO:transformers.trainer:{'loss': 2.352013950228691, 'learning_rate': 1.8407603003230338e-05, 'epoch': 1.8955438198061798, 'step': 1577500}
INFO:transformers.trainer:{'loss': 2.3667521188259126, 'learning_rate': 1.8397589565196495e-05, 'epoch': 1.8961446260882102, 'step': 1578000}
INFO:transformers.trainer:{'loss': 2.350023071408272, 'learning_rate': 1.8387576127162652e-05, 'epoch': 1.8967454323702408, 'step': 1578500}
INFO:transformers.trainer:{'loss': 2.354928058743477, 'learning_rate': 1.837756268912881e-05, 'epoch': 1.8973462386522715, 'step': 1579000}
INFO:transformers.trainer:{'loss': 2.3929167852401734, 'learning_rate': 1.836754925109497e-05, 'epoch': 1.8979470449343019, 'step': 1579500}
INFO:transformers.trainer:{'loss': 2.3683160282373428, 'learning_rate': 1.835753581306113e-05, 'epoch': 1.8985478512163323, 'step': 1580000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1580000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1580000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1580000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1560000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.364472903013229, 'learning_rate': 1.8347522375027287e-05, 'epoch': 1.8991486574983627, 'step': 1580500}
INFO:transformers.trainer:{'loss': 2.301132126688957, 'learning_rate': 1.8337508936993447e-05, 'epoch': 1.8997494637803933, 'step': 1581000}
INFO:transformers.trainer:{'loss': 2.349713604748249, 'learning_rate': 1.8327495498959604e-05, 'epoch': 1.9003502700624237, 'step': 1581500}
INFO:transformers.trainer:{'loss': 2.3887957496643066, 'learning_rate': 1.8317482060925765e-05, 'epoch': 1.9009510763444544, 'step': 1582000}
INFO:transformers.trainer:{'loss': 2.382643558859825, 'learning_rate': 1.8307468622891922e-05, 'epoch': 1.9015518826264848, 'step': 1582500}
INFO:transformers.trainer:{'loss': 2.37529064643383, 'learning_rate': 1.8297455184858082e-05, 'epoch': 1.9021526889085152, 'step': 1583000}
INFO:transformers.trainer:{'loss': 2.378612048149109, 'learning_rate': 1.828744174682424e-05, 'epoch': 1.9027534951905456, 'step': 1583500}
INFO:transformers.trainer:{'loss': 2.36965094769001, 'learning_rate': 1.8277428308790396e-05, 'epoch': 1.9033543014725762, 'step': 1584000}
INFO:transformers.trainer:{'loss': 2.332440857052803, 'learning_rate': 1.8267414870756553e-05, 'epoch': 1.9039551077546066, 'step': 1584500}
INFO:transformers.trainer:{'loss': 2.348546964287758, 'learning_rate': 1.8257401432722714e-05, 'epoch': 1.9045559140366373, 'step': 1585000}
INFO:transformers.trainer:{'loss': 2.3525085637569427, 'learning_rate': 1.824738799468887e-05, 'epoch': 1.9051567203186677, 'step': 1585500}
INFO:transformers.trainer:{'loss': 2.2842384864091874, 'learning_rate': 1.823737455665503e-05, 'epoch': 1.905757526600698, 'step': 1586000}
INFO:transformers.trainer:{'loss': 2.3176775946617125, 'learning_rate': 1.8227361118621192e-05, 'epoch': 1.9063583328827285, 'step': 1586500}
INFO:transformers.trainer:{'loss': 2.3636850152015687, 'learning_rate': 1.821734768058735e-05, 'epoch': 1.9069591391647591, 'step': 1587000}
INFO:transformers.trainer:{'loss': 2.322088126540184, 'learning_rate': 1.820733424255351e-05, 'epoch': 1.9075599454467898, 'step': 1587500}
INFO:transformers.trainer:{'loss': 2.3317278426885606, 'learning_rate': 1.8197320804519666e-05, 'epoch': 1.9081607517288202, 'step': 1588000}
INFO:transformers.trainer:{'loss': 2.36095617377758, 'learning_rate': 1.8187307366485827e-05, 'epoch': 1.9087615580108506, 'step': 1588500}
INFO:transformers.trainer:{'loss': 2.3173077026605604, 'learning_rate': 1.8177293928451984e-05, 'epoch': 1.909362364292881, 'step': 1589000}
INFO:transformers.trainer:{'loss': 2.3328900911808015, 'learning_rate': 1.816728049041814e-05, 'epoch': 1.9099631705749114, 'step': 1589500}
INFO:transformers.trainer:{'loss': 2.3746485447883607, 'learning_rate': 1.8157267052384298e-05, 'epoch': 1.910563976856942, 'step': 1590000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1590000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1590000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1590000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1570000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.380058666706085, 'learning_rate': 1.814725361435046e-05, 'epoch': 1.9111647831389726, 'step': 1590500}
INFO:transformers.trainer:{'loss': 2.3614058430194853, 'learning_rate': 1.8137240176316616e-05, 'epoch': 1.911765589421003, 'step': 1591000}
INFO:transformers.trainer:{'loss': 2.3732541573047636, 'learning_rate': 1.8127226738282776e-05, 'epoch': 1.9123663957030335, 'step': 1591500}
INFO:transformers.trainer:{'loss': 2.341355676174164, 'learning_rate': 1.8117213300248933e-05, 'epoch': 1.9129672019850639, 'step': 1592000}
INFO:transformers.trainer:{'loss': 2.3071374150514603, 'learning_rate': 1.8107199862215094e-05, 'epoch': 1.9135680082670943, 'step': 1592500}
INFO:transformers.trainer:{'loss': 2.3235243725776673, 'learning_rate': 1.8097186424181254e-05, 'epoch': 1.914168814549125, 'step': 1593000}
INFO:transformers.trainer:{'loss': 2.310584312558174, 'learning_rate': 1.808717298614741e-05, 'epoch': 1.9147696208311555, 'step': 1593500}
INFO:transformers.trainer:{'loss': 2.395055064916611, 'learning_rate': 1.807715954811357e-05, 'epoch': 1.915370427113186, 'step': 1594000}
INFO:transformers.trainer:{'loss': 2.387102519750595, 'learning_rate': 1.806714611007973e-05, 'epoch': 1.9159712333952164, 'step': 1594500}
INFO:transformers.trainer:{'loss': 2.3611662899851797, 'learning_rate': 1.8057132672045886e-05, 'epoch': 1.9165720396772468, 'step': 1595000}
INFO:transformers.trainer:{'loss': 2.3165134869813917, 'learning_rate': 1.8047119234012043e-05, 'epoch': 1.9171728459592774, 'step': 1595500}
INFO:transformers.trainer:{'loss': 2.3140377244353294, 'learning_rate': 1.8037105795978203e-05, 'epoch': 1.9177736522413078, 'step': 1596000}
INFO:transformers.trainer:{'loss': 2.376019300222397, 'learning_rate': 1.802709235794436e-05, 'epoch': 1.9183744585233384, 'step': 1596500}
INFO:transformers.trainer:{'loss': 2.3866215682029726, 'learning_rate': 1.801707891991052e-05, 'epoch': 1.9189752648053688, 'step': 1597000}
INFO:transformers.trainer:{'loss': 2.353599436879158, 'learning_rate': 1.8007065481876678e-05, 'epoch': 1.9195760710873992, 'step': 1597500}
INFO:transformers.trainer:{'loss': 2.330921349167824, 'learning_rate': 1.7997052043842838e-05, 'epoch': 1.9201768773694297, 'step': 1598000}
INFO:transformers.trainer:{'loss': 2.339096039891243, 'learning_rate': 1.7987038605809e-05, 'epoch': 1.9207776836514603, 'step': 1598500}
INFO:transformers.trainer:{'loss': 2.3741826856136323, 'learning_rate': 1.7977025167775156e-05, 'epoch': 1.9213784899334907, 'step': 1599000}
INFO:transformers.trainer:{'loss': 2.354669783473015, 'learning_rate': 1.7967011729741316e-05, 'epoch': 1.9219792962155213, 'step': 1599500}
INFO:transformers.trainer:{'loss': 2.343921013355255, 'learning_rate': 1.7956998291707473e-05, 'epoch': 1.9225801024975517, 'step': 1600000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1600000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1600000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1600000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1580000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3377224103212355, 'learning_rate': 1.794698485367363e-05, 'epoch': 1.9231809087795821, 'step': 1600500}
INFO:transformers.trainer:{'loss': 2.3653964527845384, 'learning_rate': 1.7936971415639787e-05, 'epoch': 1.9237817150616126, 'step': 1601000}
INFO:transformers.trainer:{'loss': 2.3168712905645372, 'learning_rate': 1.7926957977605948e-05, 'epoch': 1.9243825213436432, 'step': 1601500}
INFO:transformers.trainer:{'loss': 2.3717102595567705, 'learning_rate': 1.7916944539572105e-05, 'epoch': 1.9249833276256738, 'step': 1602000}
INFO:transformers.trainer:{'loss': 2.348429244756699, 'learning_rate': 1.7906931101538265e-05, 'epoch': 1.9255841339077042, 'step': 1602500}
INFO:transformers.trainer:{'loss': 2.286853504538536, 'learning_rate': 1.7896917663504422e-05, 'epoch': 1.9261849401897346, 'step': 1603000}
INFO:transformers.trainer:{'loss': 2.353350823223591, 'learning_rate': 1.7886904225470583e-05, 'epoch': 1.926785746471765, 'step': 1603500}
INFO:transformers.trainer:{'loss': 2.3709797900915146, 'learning_rate': 1.787689078743674e-05, 'epoch': 1.9273865527537954, 'step': 1604000}
INFO:transformers.trainer:{'loss': 2.348402042388916, 'learning_rate': 1.78668773494029e-05, 'epoch': 1.927987359035826, 'step': 1604500}
INFO:transformers.trainer:{'loss': 2.3276912356615065, 'learning_rate': 1.785686391136906e-05, 'epoch': 1.9285881653178567, 'step': 1605000}
INFO:transformers.trainer:{'loss': 2.348515923857689, 'learning_rate': 1.7846850473335218e-05, 'epoch': 1.9291889715998871, 'step': 1605500}
INFO:transformers.trainer:{'loss': 2.2913312267065047, 'learning_rate': 1.7836837035301375e-05, 'epoch': 1.9297897778819175, 'step': 1606000}
INFO:transformers.trainer:{'loss': 2.342607991039753, 'learning_rate': 1.7826823597267532e-05, 'epoch': 1.930390584163948, 'step': 1606500}
INFO:transformers.trainer:{'loss': 2.3437250608205797, 'learning_rate': 1.7816810159233693e-05, 'epoch': 1.9309913904459783, 'step': 1607000}
INFO:transformers.trainer:{'loss': 2.340625887095928, 'learning_rate': 1.780679672119985e-05, 'epoch': 1.931592196728009, 'step': 1607500}
INFO:transformers.trainer:{'loss': 2.2965930378437043, 'learning_rate': 1.779678328316601e-05, 'epoch': 1.9321930030100396, 'step': 1608000}
INFO:transformers.trainer:{'loss': 2.352989898324013, 'learning_rate': 1.7786769845132167e-05, 'epoch': 1.93279380929207, 'step': 1608500}
INFO:transformers.trainer:{'loss': 2.3377144899368285, 'learning_rate': 1.7776756407098328e-05, 'epoch': 1.9333946155741004, 'step': 1609000}
INFO:transformers.trainer:{'loss': 2.3221778527498245, 'learning_rate': 1.7766742969064485e-05, 'epoch': 1.9339954218561308, 'step': 1609500}
INFO:transformers.trainer:{'loss': 2.3530849338769912, 'learning_rate': 1.7756729531030645e-05, 'epoch': 1.9345962281381615, 'step': 1610000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1610000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1610000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1610000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1590000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2836051150560377, 'learning_rate': 1.7746716092996802e-05, 'epoch': 1.9351970344201919, 'step': 1610500}
INFO:transformers.trainer:{'loss': 2.2942442833185197, 'learning_rate': 1.7736702654962963e-05, 'epoch': 1.9357978407022225, 'step': 1611000}
INFO:transformers.trainer:{'loss': 2.35614923453331, 'learning_rate': 1.772668921692912e-05, 'epoch': 1.936398646984253, 'step': 1611500}
INFO:transformers.trainer:{'loss': 2.3661974081993105, 'learning_rate': 1.7716675778895277e-05, 'epoch': 1.9369994532662833, 'step': 1612000}
INFO:transformers.trainer:{'loss': 2.3504926067590715, 'learning_rate': 1.7706662340861437e-05, 'epoch': 1.9376002595483137, 'step': 1612500}
INFO:transformers.trainer:{'loss': 2.3925628074407577, 'learning_rate': 1.7696648902827594e-05, 'epoch': 1.9382010658303443, 'step': 1613000}
INFO:transformers.trainer:{'loss': 2.346884918928146, 'learning_rate': 1.7686635464793755e-05, 'epoch': 1.9388018721123748, 'step': 1613500}
INFO:transformers.trainer:{'loss': 2.3728869935274126, 'learning_rate': 1.7676622026759912e-05, 'epoch': 1.9394026783944054, 'step': 1614000}
INFO:transformers.trainer:{'loss': 2.3015537106990815, 'learning_rate': 1.7666608588726072e-05, 'epoch': 1.9400034846764358, 'step': 1614500}
INFO:transformers.trainer:{'loss': 2.338484016299248, 'learning_rate': 1.765659515069223e-05, 'epoch': 1.9406042909584662, 'step': 1615000}
INFO:transformers.trainer:{'loss': 2.3238681704998014, 'learning_rate': 1.764658171265839e-05, 'epoch': 1.9412050972404966, 'step': 1615500}
INFO:transformers.trainer:{'loss': 2.3839410890340806, 'learning_rate': 1.7636568274624547e-05, 'epoch': 1.9418059035225272, 'step': 1616000}
INFO:transformers.trainer:{'loss': 2.33772603225708, 'learning_rate': 1.7626554836590707e-05, 'epoch': 1.9424067098045579, 'step': 1616500}
INFO:transformers.trainer:{'loss': 2.3554216866493225, 'learning_rate': 1.7616541398556864e-05, 'epoch': 1.9430075160865883, 'step': 1617000}
INFO:transformers.trainer:{'loss': 2.354801893115044, 'learning_rate': 1.760652796052302e-05, 'epoch': 1.9436083223686187, 'step': 1617500}
INFO:transformers.trainer:{'loss': 2.3213597019016743, 'learning_rate': 1.7596514522489182e-05, 'epoch': 1.944209128650649, 'step': 1618000}
INFO:transformers.trainer:{'loss': 2.340826377749443, 'learning_rate': 1.758650108445534e-05, 'epoch': 1.9448099349326795, 'step': 1618500}
INFO:transformers.trainer:{'loss': 2.320396188378334, 'learning_rate': 1.75764876464215e-05, 'epoch': 1.9454107412147101, 'step': 1619000}
INFO:transformers.trainer:{'loss': 2.296550953388214, 'learning_rate': 1.7566474208387656e-05, 'epoch': 1.9460115474967408, 'step': 1619500}
INFO:transformers.trainer:{'loss': 2.362220030784607, 'learning_rate': 1.7556460770353817e-05, 'epoch': 1.9466123537787712, 'step': 1620000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1620000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1620000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1620000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1600000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3214462358951566, 'learning_rate': 1.7546447332319974e-05, 'epoch': 1.9472131600608016, 'step': 1620500}
INFO:transformers.trainer:{'loss': 2.3174435811042784, 'learning_rate': 1.7536433894286134e-05, 'epoch': 1.947813966342832, 'step': 1621000}
INFO:transformers.trainer:{'loss': 2.3276029372215272, 'learning_rate': 1.752642045625229e-05, 'epoch': 1.9484147726248624, 'step': 1621500}
INFO:transformers.trainer:{'loss': 2.3200271137952804, 'learning_rate': 1.7516407018218452e-05, 'epoch': 1.949015578906893, 'step': 1622000}
INFO:transformers.trainer:{'loss': 2.31381032538414, 'learning_rate': 1.750639358018461e-05, 'epoch': 1.9496163851889237, 'step': 1622500}
INFO:transformers.trainer:{'loss': 2.3155927505493166, 'learning_rate': 1.7496380142150766e-05, 'epoch': 1.950217191470954, 'step': 1623000}
INFO:transformers.trainer:{'loss': 2.389192823290825, 'learning_rate': 1.7486366704116926e-05, 'epoch': 1.9508179977529845, 'step': 1623500}
INFO:transformers.trainer:{'loss': 2.334397927761078, 'learning_rate': 1.7476353266083084e-05, 'epoch': 1.9514188040350149, 'step': 1624000}
INFO:transformers.trainer:{'loss': 2.3652994273900987, 'learning_rate': 1.7466339828049244e-05, 'epoch': 1.9520196103170455, 'step': 1624500}
INFO:transformers.trainer:{'loss': 2.3457534849643706, 'learning_rate': 1.74563263900154e-05, 'epoch': 1.952620416599076, 'step': 1625000}
INFO:transformers.trainer:{'loss': 2.2874813854694365, 'learning_rate': 1.744631295198156e-05, 'epoch': 1.9532212228811066, 'step': 1625500}
INFO:transformers.trainer:{'loss': 2.3500669012069704, 'learning_rate': 1.743629951394772e-05, 'epoch': 1.953822029163137, 'step': 1626000}
INFO:transformers.trainer:{'loss': 2.3496078612804414, 'learning_rate': 1.742628607591388e-05, 'epoch': 1.9544228354451674, 'step': 1626500}
INFO:transformers.trainer:{'loss': 2.366502288579941, 'learning_rate': 1.7416272637880036e-05, 'epoch': 1.9550236417271978, 'step': 1627000}
INFO:transformers.trainer:{'loss': 2.314231850743294, 'learning_rate': 1.7406259199846197e-05, 'epoch': 1.9556244480092284, 'step': 1627500}
INFO:transformers.trainer:{'loss': 2.301007232785225, 'learning_rate': 1.7396245761812354e-05, 'epoch': 1.9562252542912588, 'step': 1628000}
INFO:transformers.trainer:{'loss': 2.352133549451828, 'learning_rate': 1.738623232377851e-05, 'epoch': 1.9568260605732894, 'step': 1628500}
INFO:transformers.trainer:{'loss': 2.3772523031234742, 'learning_rate': 1.7376218885744668e-05, 'epoch': 1.9574268668553199, 'step': 1629000}
INFO:transformers.trainer:{'loss': 2.303843280076981, 'learning_rate': 1.7366205447710828e-05, 'epoch': 1.9580276731373503, 'step': 1629500}
INFO:transformers.trainer:{'loss': 2.3606478815078735, 'learning_rate': 1.735619200967699e-05, 'epoch': 1.9586284794193807, 'step': 1630000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1630000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1630000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1630000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1610000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3384546337127685, 'learning_rate': 1.7346178571643146e-05, 'epoch': 1.9592292857014113, 'step': 1630500}
INFO:transformers.trainer:{'loss': 2.330212470173836, 'learning_rate': 1.7336165133609306e-05, 'epoch': 1.959830091983442, 'step': 1631000}
INFO:transformers.trainer:{'loss': 2.3183449505567553, 'learning_rate': 1.7326151695575463e-05, 'epoch': 1.9604308982654723, 'step': 1631500}
INFO:transformers.trainer:{'loss': 2.3667229000329972, 'learning_rate': 1.7316138257541624e-05, 'epoch': 1.9610317045475028, 'step': 1632000}
INFO:transformers.trainer:{'loss': 2.3484043967723847, 'learning_rate': 1.730612481950778e-05, 'epoch': 1.9616325108295332, 'step': 1632500}
INFO:transformers.trainer:{'loss': 2.367154940009117, 'learning_rate': 1.729611138147394e-05, 'epoch': 1.9622333171115636, 'step': 1633000}
INFO:transformers.trainer:{'loss': 2.3524125379920005, 'learning_rate': 1.7286097943440098e-05, 'epoch': 1.9628341233935942, 'step': 1633500}
INFO:transformers.trainer:{'loss': 2.3284623209834097, 'learning_rate': 1.7276084505406255e-05, 'epoch': 1.9634349296756248, 'step': 1634000}
INFO:transformers.trainer:{'loss': 2.3223247969150544, 'learning_rate': 1.7266071067372412e-05, 'epoch': 1.9640357359576552, 'step': 1634500}
INFO:transformers.trainer:{'loss': 2.3454235936403274, 'learning_rate': 1.7256057629338573e-05, 'epoch': 1.9646365422396856, 'step': 1635000}
INFO:transformers.trainer:{'loss': 2.365981181383133, 'learning_rate': 1.724604419130473e-05, 'epoch': 1.965237348521716, 'step': 1635500}
INFO:transformers.trainer:{'loss': 2.302898265004158, 'learning_rate': 1.723603075327089e-05, 'epoch': 1.9658381548037465, 'step': 1636000}
INFO:transformers.trainer:{'loss': 2.3349948050379754, 'learning_rate': 1.722601731523705e-05, 'epoch': 1.966438961085777, 'step': 1636500}
INFO:transformers.trainer:{'loss': 2.3499803233146666, 'learning_rate': 1.7216003877203208e-05, 'epoch': 1.9670397673678077, 'step': 1637000}
INFO:transformers.trainer:{'loss': 2.3156937551498413, 'learning_rate': 1.7205990439169368e-05, 'epoch': 1.9676405736498381, 'step': 1637500}
INFO:transformers.trainer:{'loss': 2.343502778530121, 'learning_rate': 1.7195977001135525e-05, 'epoch': 1.9682413799318685, 'step': 1638000}
INFO:transformers.trainer:{'loss': 2.337443166434765, 'learning_rate': 1.7185963563101682e-05, 'epoch': 1.968842186213899, 'step': 1638500}
INFO:transformers.trainer:{'loss': 2.3810095952153207, 'learning_rate': 1.717595012506784e-05, 'epoch': 1.9694429924959296, 'step': 1639000}
INFO:transformers.trainer:{'loss': 2.3466396930217743, 'learning_rate': 1.7165936687034e-05, 'epoch': 1.97004379877796, 'step': 1639500}
INFO:transformers.trainer:{'loss': 2.331900064945221, 'learning_rate': 1.7155923249000157e-05, 'epoch': 1.9706446050599906, 'step': 1640000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1640000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1640000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1640000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1620000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.319587435603142, 'learning_rate': 1.7145909810966317e-05, 'epoch': 1.971245411342021, 'step': 1640500}
INFO:transformers.trainer:{'loss': 2.3549622025489807, 'learning_rate': 1.7135896372932475e-05, 'epoch': 1.9718462176240514, 'step': 1641000}
INFO:transformers.trainer:{'loss': 2.318403832435608, 'learning_rate': 1.7125882934898635e-05, 'epoch': 1.9724470239060818, 'step': 1641500}
INFO:transformers.trainer:{'loss': 2.3503654927015303, 'learning_rate': 1.7115869496864792e-05, 'epoch': 1.9730478301881125, 'step': 1642000}
INFO:transformers.trainer:{'loss': 2.365983969926834, 'learning_rate': 1.7105856058830952e-05, 'epoch': 1.9736486364701429, 'step': 1642500}
INFO:transformers.trainer:{'loss': 2.345129077196121, 'learning_rate': 1.7095842620797113e-05, 'epoch': 1.9742494427521735, 'step': 1643000}
INFO:transformers.trainer:{'loss': 2.2868442338705064, 'learning_rate': 1.708582918276327e-05, 'epoch': 1.974850249034204, 'step': 1643500}
INFO:transformers.trainer:{'loss': 2.3000435788631437, 'learning_rate': 1.7075815744729427e-05, 'epoch': 1.9754510553162343, 'step': 1644000}
INFO:transformers.trainer:{'loss': 2.3038559786081314, 'learning_rate': 1.7065802306695584e-05, 'epoch': 1.9760518615982647, 'step': 1644500}
INFO:transformers.trainer:{'loss': 2.340450142979622, 'learning_rate': 1.7055788868661745e-05, 'epoch': 1.9766526678802954, 'step': 1645000}
INFO:transformers.trainer:{'loss': 2.308887369275093, 'learning_rate': 1.70457754306279e-05, 'epoch': 1.977253474162326, 'step': 1645500}
INFO:transformers.trainer:{'loss': 2.3800536799430847, 'learning_rate': 1.7035761992594062e-05, 'epoch': 1.9778542804443564, 'step': 1646000}
INFO:transformers.trainer:{'loss': 2.334091369509697, 'learning_rate': 1.702574855456022e-05, 'epoch': 1.9784550867263868, 'step': 1646500}
INFO:transformers.trainer:{'loss': 2.3500438363552094, 'learning_rate': 1.701573511652638e-05, 'epoch': 1.9790558930084172, 'step': 1647000}
INFO:transformers.trainer:{'loss': 2.369383038520813, 'learning_rate': 1.7005721678492537e-05, 'epoch': 1.9796566992904476, 'step': 1647500}
INFO:transformers.trainer:{'loss': 2.3798199545145033, 'learning_rate': 1.6995708240458697e-05, 'epoch': 1.9802575055724783, 'step': 1648000}
INFO:transformers.trainer:{'loss': 2.3631307858228685, 'learning_rate': 1.6985694802424858e-05, 'epoch': 1.980858311854509, 'step': 1648500}
INFO:transformers.trainer:{'loss': 2.3799563283920286, 'learning_rate': 1.6975681364391015e-05, 'epoch': 1.9814591181365393, 'step': 1649000}
INFO:transformers.trainer:{'loss': 2.348629752397537, 'learning_rate': 1.6965667926357172e-05, 'epoch': 1.9820599244185697, 'step': 1649500}
INFO:transformers.trainer:{'loss': 2.3253213565349578, 'learning_rate': 1.695565448832333e-05, 'epoch': 1.9826607307006001, 'step': 1650000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1650000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1650000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1650000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1630000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.294772847533226, 'learning_rate': 1.694564105028949e-05, 'epoch': 1.9832615369826305, 'step': 1650500}
INFO:transformers.trainer:{'loss': 2.333544102907181, 'learning_rate': 1.6935627612255646e-05, 'epoch': 1.9838623432646612, 'step': 1651000}
INFO:transformers.trainer:{'loss': 2.325002200484276, 'learning_rate': 1.6925614174221807e-05, 'epoch': 1.9844631495466918, 'step': 1651500}
INFO:transformers.trainer:{'loss': 2.359953373789787, 'learning_rate': 1.6915600736187964e-05, 'epoch': 1.9850639558287222, 'step': 1652000}
INFO:transformers.trainer:{'loss': 2.3631153970956804, 'learning_rate': 1.6905587298154124e-05, 'epoch': 1.9856647621107526, 'step': 1652500}
INFO:transformers.trainer:{'loss': 2.3078147947192194, 'learning_rate': 1.689557386012028e-05, 'epoch': 1.986265568392783, 'step': 1653000}
INFO:transformers.trainer:{'loss': 2.3884079564809797, 'learning_rate': 1.6885560422086442e-05, 'epoch': 1.9868663746748136, 'step': 1653500}
INFO:transformers.trainer:{'loss': 2.3038890450000764, 'learning_rate': 1.68755469840526e-05, 'epoch': 1.987467180956844, 'step': 1654000}
INFO:transformers.trainer:{'loss': 2.328911927342415, 'learning_rate': 1.686553354601876e-05, 'epoch': 1.9880679872388747, 'step': 1654500}
INFO:transformers.trainer:{'loss': 2.3550187673568725, 'learning_rate': 1.6855520107984916e-05, 'epoch': 1.988668793520905, 'step': 1655000}
INFO:transformers.trainer:{'loss': 2.3518730754852295, 'learning_rate': 1.6845506669951073e-05, 'epoch': 1.9892695998029355, 'step': 1655500}
INFO:transformers.trainer:{'loss': 2.3202346707582473, 'learning_rate': 1.6835493231917234e-05, 'epoch': 1.989870406084966, 'step': 1656000}
INFO:transformers.trainer:{'loss': 2.3331992835998534, 'learning_rate': 1.682547979388339e-05, 'epoch': 1.9904712123669965, 'step': 1656500}
INFO:transformers.trainer:{'loss': 2.3405474345088004, 'learning_rate': 1.681546635584955e-05, 'epoch': 1.991072018649027, 'step': 1657000}
INFO:transformers.trainer:{'loss': 2.348519855439663, 'learning_rate': 1.680545291781571e-05, 'epoch': 1.9916728249310576, 'step': 1657500}
INFO:transformers.trainer:{'loss': 2.3747824985980985, 'learning_rate': 1.679543947978187e-05, 'epoch': 1.992273631213088, 'step': 1658000}
INFO:transformers.trainer:{'loss': 2.3380791589021683, 'learning_rate': 1.6785426041748026e-05, 'epoch': 1.9928744374951184, 'step': 1658500}
INFO:transformers.trainer:{'loss': 2.325616953730583, 'learning_rate': 1.6775412603714186e-05, 'epoch': 1.9934752437771488, 'step': 1659000}
INFO:transformers.trainer:{'loss': 2.349468068599701, 'learning_rate': 1.6765399165680343e-05, 'epoch': 1.9940760500591794, 'step': 1659500}
INFO:transformers.trainer:{'loss': 2.3217469633817673, 'learning_rate': 1.6755385727646504e-05, 'epoch': 1.99467685634121, 'step': 1660000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1660000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1660000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1660000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1640000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3337435572743415, 'learning_rate': 1.674537228961266e-05, 'epoch': 1.9952776626232405, 'step': 1660500}
INFO:transformers.trainer:{'loss': 2.2894522294998167, 'learning_rate': 1.6735358851578818e-05, 'epoch': 1.9958784689052709, 'step': 1661000}
INFO:transformers.trainer:{'loss': 2.3240792038440703, 'learning_rate': 1.672534541354498e-05, 'epoch': 1.9964792751873013, 'step': 1661500}
INFO:transformers.trainer:{'loss': 2.339479009985924, 'learning_rate': 1.6715331975511136e-05, 'epoch': 1.9970800814693317, 'step': 1662000}
INFO:transformers.trainer:{'loss': 2.340190292954445, 'learning_rate': 1.6705318537477296e-05, 'epoch': 1.9976808877513623, 'step': 1662500}
INFO:transformers.trainer:{'loss': 2.335360924720764, 'learning_rate': 1.6695305099443453e-05, 'epoch': 1.998281694033393, 'step': 1663000}
INFO:transformers.trainer:{'loss': 2.2766129320859907, 'learning_rate': 1.6685291661409614e-05, 'epoch': 1.9988825003154234, 'step': 1663500}
INFO:transformers.trainer:{'loss': 2.3335868771076202, 'learning_rate': 1.667527822337577e-05, 'epoch': 1.9994833065974538, 'step': 1664000}
INFO:transformers.trainer:{'loss': 2.313596365571022, 'learning_rate': 1.666526478534193e-05, 'epoch': 2.000084112879484, 'step': 1664500}
INFO:transformers.trainer:{'loss': 2.346830172657967, 'learning_rate': 1.6655251347308088e-05, 'epoch': 2.0006849191615146, 'step': 1665000}
INFO:transformers.trainer:{'loss': 2.3296192816495895, 'learning_rate': 1.664523790927425e-05, 'epoch': 2.0012857254435454, 'step': 1665500}
INFO:transformers.trainer:{'loss': 2.3004934049844743, 'learning_rate': 1.6635224471240406e-05, 'epoch': 2.001886531725576, 'step': 1666000}
INFO:transformers.trainer:{'loss': 2.313689906954765, 'learning_rate': 1.6625211033206563e-05, 'epoch': 2.0024873380076063, 'step': 1666500}
INFO:transformers.trainer:{'loss': 2.3308872355222703, 'learning_rate': 1.661519759517272e-05, 'epoch': 2.0030881442896367, 'step': 1667000}
INFO:transformers.trainer:{'loss': 2.326946930170059, 'learning_rate': 1.660518415713888e-05, 'epoch': 2.003688950571667, 'step': 1667500}
INFO:transformers.trainer:{'loss': 2.325430470705032, 'learning_rate': 1.659517071910504e-05, 'epoch': 2.0042897568536975, 'step': 1668000}
INFO:transformers.trainer:{'loss': 2.2762878893613814, 'learning_rate': 1.6585157281071198e-05, 'epoch': 2.0048905631357283, 'step': 1668500}
INFO:transformers.trainer:{'loss': 2.2747005739212036, 'learning_rate': 1.6575143843037358e-05, 'epoch': 2.0054913694177587, 'step': 1669000}
INFO:transformers.trainer:{'loss': 2.290033525466919, 'learning_rate': 1.6565130405003515e-05, 'epoch': 2.006092175699789, 'step': 1669500}
INFO:transformers.trainer:{'loss': 2.3157494494915007, 'learning_rate': 1.6555116966969676e-05, 'epoch': 2.0066929819818196, 'step': 1670000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1670000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1670000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1670000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1650000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.390440608739853, 'learning_rate': 1.6545103528935833e-05, 'epoch': 2.00729378826385, 'step': 1670500}
INFO:transformers.trainer:{'loss': 2.332504564285278, 'learning_rate': 1.6535090090901993e-05, 'epoch': 2.0078945945458804, 'step': 1671000}
INFO:transformers.trainer:{'loss': 2.3228039274215697, 'learning_rate': 1.652507665286815e-05, 'epoch': 2.0084954008279112, 'step': 1671500}
INFO:transformers.trainer:{'loss': 2.3597973383665085, 'learning_rate': 1.6515063214834307e-05, 'epoch': 2.0090962071099416, 'step': 1672000}
INFO:transformers.trainer:{'loss': 2.3337539504766465, 'learning_rate': 1.6505049776800464e-05, 'epoch': 2.009697013391972, 'step': 1672500}
INFO:transformers.trainer:{'loss': 2.315086253523827, 'learning_rate': 1.6495036338766625e-05, 'epoch': 2.0102978196740025, 'step': 1673000}
INFO:transformers.trainer:{'loss': 2.3580428322553635, 'learning_rate': 1.6485022900732785e-05, 'epoch': 2.010898625956033, 'step': 1673500}
INFO:transformers.trainer:{'loss': 2.283692073225975, 'learning_rate': 1.6475009462698942e-05, 'epoch': 2.0114994322380633, 'step': 1674000}
INFO:transformers.trainer:{'loss': 2.278493279695511, 'learning_rate': 1.6464996024665103e-05, 'epoch': 2.012100238520094, 'step': 1674500}
INFO:transformers.trainer:{'loss': 2.2815973691344262, 'learning_rate': 1.645498258663126e-05, 'epoch': 2.0127010448021245, 'step': 1675000}
INFO:transformers.trainer:{'loss': 2.364941569328308, 'learning_rate': 1.644496914859742e-05, 'epoch': 2.013301851084155, 'step': 1675500}
INFO:transformers.trainer:{'loss': 2.2832937885522844, 'learning_rate': 1.6434955710563577e-05, 'epoch': 2.0139026573661853, 'step': 1676000}
INFO:transformers.trainer:{'loss': 2.3459864904880523, 'learning_rate': 1.6424942272529738e-05, 'epoch': 2.0145034636482158, 'step': 1676500}
INFO:transformers.trainer:{'loss': 2.303395222425461, 'learning_rate': 1.6414928834495895e-05, 'epoch': 2.0151042699302466, 'step': 1677000}
INFO:transformers.trainer:{'loss': 2.3413615751564505, 'learning_rate': 1.6404915396462052e-05, 'epoch': 2.015705076212277, 'step': 1677500}
INFO:transformers.trainer:{'loss': 2.324044862627983, 'learning_rate': 1.639490195842821e-05, 'epoch': 2.0163058824943074, 'step': 1678000}
INFO:transformers.trainer:{'loss': 2.311969947576523, 'learning_rate': 1.638488852039437e-05, 'epoch': 2.016906688776338, 'step': 1678500}
INFO:transformers.trainer:{'loss': 2.362601258158684, 'learning_rate': 1.6374875082360527e-05, 'epoch': 2.0175074950583682, 'step': 1679000}
INFO:transformers.trainer:{'loss': 2.372703153848648, 'learning_rate': 1.6364861644326687e-05, 'epoch': 2.0181083013403986, 'step': 1679500}
INFO:transformers.trainer:{'loss': 2.2727574427723884, 'learning_rate': 1.6354848206292848e-05, 'epoch': 2.0187091076224295, 'step': 1680000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1680000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1680000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1680000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1660000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3412585897445677, 'learning_rate': 1.6344834768259005e-05, 'epoch': 2.01930991390446, 'step': 1680500}
INFO:transformers.trainer:{'loss': 2.2995269100666045, 'learning_rate': 1.6334821330225165e-05, 'epoch': 2.0199107201864903, 'step': 1681000}
INFO:transformers.trainer:{'loss': 2.2842039207220077, 'learning_rate': 1.6324807892191322e-05, 'epoch': 2.0205115264685207, 'step': 1681500}
INFO:transformers.trainer:{'loss': 2.273872521042824, 'learning_rate': 1.6314794454157483e-05, 'epoch': 2.021112332750551, 'step': 1682000}
INFO:transformers.trainer:{'loss': 2.3688094445466996, 'learning_rate': 1.630478101612364e-05, 'epoch': 2.0217131390325815, 'step': 1682500}
INFO:transformers.trainer:{'loss': 2.3171495453119277, 'learning_rate': 1.6294767578089797e-05, 'epoch': 2.0223139453146124, 'step': 1683000}
INFO:transformers.trainer:{'loss': 2.3088257747888563, 'learning_rate': 1.6284754140055954e-05, 'epoch': 2.022914751596643, 'step': 1683500}
INFO:transformers.trainer:{'loss': 2.270026250064373, 'learning_rate': 1.6274740702022114e-05, 'epoch': 2.023515557878673, 'step': 1684000}
INFO:transformers.trainer:{'loss': 2.332741513967514, 'learning_rate': 1.626472726398827e-05, 'epoch': 2.0241163641607036, 'step': 1684500}
INFO:transformers.trainer:{'loss': 2.2850420804023743, 'learning_rate': 1.6254713825954432e-05, 'epoch': 2.024717170442734, 'step': 1685000}
INFO:transformers.trainer:{'loss': 2.336309515714645, 'learning_rate': 1.624470038792059e-05, 'epoch': 2.0253179767247644, 'step': 1685500}
INFO:transformers.trainer:{'loss': 2.316200272202492, 'learning_rate': 1.623468694988675e-05, 'epoch': 2.0259187830067953, 'step': 1686000}
INFO:transformers.trainer:{'loss': 2.3171771507263186, 'learning_rate': 1.622467351185291e-05, 'epoch': 2.0265195892888257, 'step': 1686500}
INFO:transformers.trainer:{'loss': 2.30457037293911, 'learning_rate': 1.6214660073819067e-05, 'epoch': 2.027120395570856, 'step': 1687000}
INFO:transformers.trainer:{'loss': 2.3058402915000915, 'learning_rate': 1.6204646635785227e-05, 'epoch': 2.0277212018528865, 'step': 1687500}
INFO:transformers.trainer:{'loss': 2.306703165650368, 'learning_rate': 1.6194633197751384e-05, 'epoch': 2.028322008134917, 'step': 1688000}
INFO:transformers.trainer:{'loss': 2.3074715342521666, 'learning_rate': 1.618461975971754e-05, 'epoch': 2.0289228144169473, 'step': 1688500}
INFO:transformers.trainer:{'loss': 2.369152190387249, 'learning_rate': 1.61746063216837e-05, 'epoch': 2.029523620698978, 'step': 1689000}
INFO:transformers.trainer:{'loss': 2.289833968639374, 'learning_rate': 1.616459288364986e-05, 'epoch': 2.0301244269810086, 'step': 1689500}
INFO:transformers.trainer:{'loss': 2.352752365589142, 'learning_rate': 1.6154579445616016e-05, 'epoch': 2.030725233263039, 'step': 1690000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1690000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1690000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1690000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1670000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.324203250706196, 'learning_rate': 1.6144566007582176e-05, 'epoch': 2.0313260395450694, 'step': 1690500}
INFO:transformers.trainer:{'loss': 2.316125853896141, 'learning_rate': 1.6134552569548333e-05, 'epoch': 2.0319268458271, 'step': 1691000}
INFO:transformers.trainer:{'loss': 2.324401784539223, 'learning_rate': 1.6124539131514494e-05, 'epoch': 2.0325276521091307, 'step': 1691500}
INFO:transformers.trainer:{'loss': 2.289294650018215, 'learning_rate': 1.611452569348065e-05, 'epoch': 2.033128458391161, 'step': 1692000}
INFO:transformers.trainer:{'loss': 2.325813902378082, 'learning_rate': 1.610451225544681e-05, 'epoch': 2.0337292646731915, 'step': 1692500}
INFO:transformers.trainer:{'loss': 2.302841768860817, 'learning_rate': 1.6094498817412972e-05, 'epoch': 2.034330070955222, 'step': 1693000}
INFO:transformers.trainer:{'loss': 2.326465062737465, 'learning_rate': 1.608448537937913e-05, 'epoch': 2.0349308772372523, 'step': 1693500}
INFO:transformers.trainer:{'loss': 2.309736347556114, 'learning_rate': 1.6074471941345286e-05, 'epoch': 2.0355316835192827, 'step': 1694000}
INFO:transformers.trainer:{'loss': 2.3119022517204284, 'learning_rate': 1.6064458503311443e-05, 'epoch': 2.0361324898013136, 'step': 1694500}
INFO:transformers.trainer:{'loss': 2.2823263585567473, 'learning_rate': 1.6054445065277603e-05, 'epoch': 2.036733296083344, 'step': 1695000}
INFO:transformers.trainer:{'loss': 2.3235234763622286, 'learning_rate': 1.604443162724376e-05, 'epoch': 2.0373341023653744, 'step': 1695500}
INFO:transformers.trainer:{'loss': 2.3686473524570464, 'learning_rate': 1.603441818920992e-05, 'epoch': 2.037934908647405, 'step': 1696000}
INFO:transformers.trainer:{'loss': 2.339952283501625, 'learning_rate': 1.6024404751176078e-05, 'epoch': 2.038535714929435, 'step': 1696500}
INFO:transformers.trainer:{'loss': 2.2669199302196503, 'learning_rate': 1.601439131314224e-05, 'epoch': 2.0391365212114656, 'step': 1697000}
INFO:transformers.trainer:{'loss': 2.3533080689907075, 'learning_rate': 1.6004377875108396e-05, 'epoch': 2.0397373274934965, 'step': 1697500}
INFO:transformers.trainer:{'loss': 2.3258506516218187, 'learning_rate': 1.5994364437074556e-05, 'epoch': 2.040338133775527, 'step': 1698000}
INFO:transformers.trainer:{'loss': 2.3196275684833525, 'learning_rate': 1.5984350999040713e-05, 'epoch': 2.0409389400575573, 'step': 1698500}
INFO:transformers.trainer:{'loss': 2.3077299830913542, 'learning_rate': 1.597433756100687e-05, 'epoch': 2.0415397463395877, 'step': 1699000}
INFO:transformers.trainer:{'loss': 2.339813466668129, 'learning_rate': 1.596432412297303e-05, 'epoch': 2.042140552621618, 'step': 1699500}
INFO:transformers.trainer:{'loss': 2.297038509249687, 'learning_rate': 1.5954310684939188e-05, 'epoch': 2.0427413589036485, 'step': 1700000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1700000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1700000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1700000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1680000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.321580137908459, 'learning_rate': 1.5944297246905348e-05, 'epoch': 2.0433421651856793, 'step': 1700500}
INFO:transformers.trainer:{'loss': 2.317103640079498, 'learning_rate': 1.5934283808871505e-05, 'epoch': 2.0439429714677098, 'step': 1701000}
INFO:transformers.trainer:{'loss': 2.347397757411003, 'learning_rate': 1.5924270370837666e-05, 'epoch': 2.04454377774974, 'step': 1701500}
INFO:transformers.trainer:{'loss': 2.2920829593539236, 'learning_rate': 1.5914256932803823e-05, 'epoch': 2.0451445840317706, 'step': 1702000}
INFO:transformers.trainer:{'loss': 2.3016272938251494, 'learning_rate': 1.5904243494769983e-05, 'epoch': 2.045745390313801, 'step': 1702500}
INFO:transformers.trainer:{'loss': 2.3192502685785295, 'learning_rate': 1.589423005673614e-05, 'epoch': 2.0463461965958314, 'step': 1703000}
INFO:transformers.trainer:{'loss': 2.374398371219635, 'learning_rate': 1.58842166187023e-05, 'epoch': 2.0469470028778622, 'step': 1703500}
INFO:transformers.trainer:{'loss': 2.3680333309173585, 'learning_rate': 1.5874203180668458e-05, 'epoch': 2.0475478091598927, 'step': 1704000}
INFO:transformers.trainer:{'loss': 2.3674284257888796, 'learning_rate': 1.5864189742634615e-05, 'epoch': 2.048148615441923, 'step': 1704500}
INFO:transformers.trainer:{'loss': 2.304072682380676, 'learning_rate': 1.5854176304600775e-05, 'epoch': 2.0487494217239535, 'step': 1705000}
INFO:transformers.trainer:{'loss': 2.3345937101840972, 'learning_rate': 1.5844162866566932e-05, 'epoch': 2.049350228005984, 'step': 1705500}
INFO:transformers.trainer:{'loss': 2.3048866325616837, 'learning_rate': 1.5834149428533093e-05, 'epoch': 2.0499510342880147, 'step': 1706000}
INFO:transformers.trainer:{'loss': 2.3164924206733706, 'learning_rate': 1.582413599049925e-05, 'epoch': 2.050551840570045, 'step': 1706500}
INFO:transformers.trainer:{'loss': 2.339883239269257, 'learning_rate': 1.581412255246541e-05, 'epoch': 2.0511526468520755, 'step': 1707000}
INFO:transformers.trainer:{'loss': 2.2908979974985124, 'learning_rate': 1.5804109114431567e-05, 'epoch': 2.051753453134106, 'step': 1707500}
INFO:transformers.trainer:{'loss': 2.343095228970051, 'learning_rate': 1.5794095676397728e-05, 'epoch': 2.0523542594161364, 'step': 1708000}
INFO:transformers.trainer:{'loss': 2.370044180393219, 'learning_rate': 1.5784082238363885e-05, 'epoch': 2.0529550656981668, 'step': 1708500}
INFO:transformers.trainer:{'loss': 2.284775405049324, 'learning_rate': 1.5774068800330045e-05, 'epoch': 2.0535558719801976, 'step': 1709000}
INFO:transformers.trainer:{'loss': 2.3072450331449508, 'learning_rate': 1.5764055362296202e-05, 'epoch': 2.054156678262228, 'step': 1709500}
INFO:transformers.trainer:{'loss': 2.328923394382, 'learning_rate': 1.575404192426236e-05, 'epoch': 2.0547574845442584, 'step': 1710000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1710000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1710000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1710000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1690000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.270556377917528, 'learning_rate': 1.5744028486228517e-05, 'epoch': 2.055358290826289, 'step': 1710500}
INFO:transformers.trainer:{'loss': 2.231077546596527, 'learning_rate': 1.5734015048194677e-05, 'epoch': 2.0559590971083193, 'step': 1711000}
INFO:transformers.trainer:{'loss': 2.369290983438492, 'learning_rate': 1.5724001610160837e-05, 'epoch': 2.0565599033903497, 'step': 1711500}
INFO:transformers.trainer:{'loss': 2.3355932203531267, 'learning_rate': 1.5713988172126994e-05, 'epoch': 2.0571607096723805, 'step': 1712000}
INFO:transformers.trainer:{'loss': 2.323768900990486, 'learning_rate': 1.5703974734093155e-05, 'epoch': 2.057761515954411, 'step': 1712500}
INFO:transformers.trainer:{'loss': 2.271348831653595, 'learning_rate': 1.5693961296059312e-05, 'epoch': 2.0583623222364413, 'step': 1713000}
INFO:transformers.trainer:{'loss': 2.279836264252663, 'learning_rate': 1.5683947858025472e-05, 'epoch': 2.0589631285184717, 'step': 1713500}
INFO:transformers.trainer:{'loss': 2.2958612123131754, 'learning_rate': 1.567393441999163e-05, 'epoch': 2.059563934800502, 'step': 1714000}
INFO:transformers.trainer:{'loss': 2.3349023743867874, 'learning_rate': 1.566392098195779e-05, 'epoch': 2.0601647410825326, 'step': 1714500}
INFO:transformers.trainer:{'loss': 2.318079896807671, 'learning_rate': 1.5653907543923947e-05, 'epoch': 2.0607655473645634, 'step': 1715000}
INFO:transformers.trainer:{'loss': 2.3269767360687257, 'learning_rate': 1.5643894105890104e-05, 'epoch': 2.061366353646594, 'step': 1715500}
INFO:transformers.trainer:{'loss': 2.2988431376218794, 'learning_rate': 1.563388066785626e-05, 'epoch': 2.0619671599286242, 'step': 1716000}
INFO:transformers.trainer:{'loss': 2.334659318327904, 'learning_rate': 1.562386722982242e-05, 'epoch': 2.0625679662106546, 'step': 1716500}
INFO:transformers.trainer:{'loss': 2.3041023615598677, 'learning_rate': 1.561385379178858e-05, 'epoch': 2.063168772492685, 'step': 1717000}
INFO:transformers.trainer:{'loss': 2.2747824709415436, 'learning_rate': 1.560384035375474e-05, 'epoch': 2.063769578774716, 'step': 1717500}
INFO:transformers.trainer:{'loss': 2.304731583595276, 'learning_rate': 1.55938269157209e-05, 'epoch': 2.0643703850567463, 'step': 1718000}
INFO:transformers.trainer:{'loss': 2.313810364961624, 'learning_rate': 1.5583813477687057e-05, 'epoch': 2.0649711913387767, 'step': 1718500}
INFO:transformers.trainer:{'loss': 2.3230575239658355, 'learning_rate': 1.5573800039653217e-05, 'epoch': 2.065571997620807, 'step': 1719000}
INFO:transformers.trainer:{'loss': 2.25353274166584, 'learning_rate': 1.5563786601619374e-05, 'epoch': 2.0661728039028375, 'step': 1719500}
INFO:transformers.trainer:{'loss': 2.326018167257309, 'learning_rate': 1.5553773163585535e-05, 'epoch': 2.066773610184868, 'step': 1720000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1720000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1720000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1720000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1700000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2440071650743483, 'learning_rate': 1.554375972555169e-05, 'epoch': 2.067374416466899, 'step': 1720500}
INFO:transformers.trainer:{'loss': 2.3078050560951233, 'learning_rate': 1.553374628751785e-05, 'epoch': 2.067975222748929, 'step': 1721000}
INFO:transformers.trainer:{'loss': 2.3207101356983184, 'learning_rate': 1.5523732849484006e-05, 'epoch': 2.0685760290309596, 'step': 1721500}
INFO:transformers.trainer:{'loss': 2.3213160187005997, 'learning_rate': 1.5513719411450166e-05, 'epoch': 2.06917683531299, 'step': 1722000}
INFO:transformers.trainer:{'loss': 2.335190380334854, 'learning_rate': 1.5503705973416323e-05, 'epoch': 2.0697776415950204, 'step': 1722500}
INFO:transformers.trainer:{'loss': 2.3326513293981552, 'learning_rate': 1.5493692535382484e-05, 'epoch': 2.070378447877051, 'step': 1723000}
INFO:transformers.trainer:{'loss': 2.3289123727679253, 'learning_rate': 1.5483679097348644e-05, 'epoch': 2.0709792541590817, 'step': 1723500}
INFO:transformers.trainer:{'loss': 2.3215224264860153, 'learning_rate': 1.54736656593148e-05, 'epoch': 2.071580060441112, 'step': 1724000}
INFO:transformers.trainer:{'loss': 2.3090724662542343, 'learning_rate': 1.5463652221280962e-05, 'epoch': 2.0721808667231425, 'step': 1724500}
INFO:transformers.trainer:{'loss': 2.308902529716492, 'learning_rate': 1.545363878324712e-05, 'epoch': 2.072781673005173, 'step': 1725000}
INFO:transformers.trainer:{'loss': 2.2920871859788896, 'learning_rate': 1.544362534521328e-05, 'epoch': 2.0733824792872033, 'step': 1725500}
INFO:transformers.trainer:{'loss': 2.2652750509381296, 'learning_rate': 1.5433611907179436e-05, 'epoch': 2.0739832855692337, 'step': 1726000}
INFO:transformers.trainer:{'loss': 2.3186950090527536, 'learning_rate': 1.5423598469145593e-05, 'epoch': 2.0745840918512646, 'step': 1726500}
INFO:transformers.trainer:{'loss': 2.320535593867302, 'learning_rate': 1.541358503111175e-05, 'epoch': 2.075184898133295, 'step': 1727000}
INFO:transformers.trainer:{'loss': 2.2541920984983443, 'learning_rate': 1.540357159307791e-05, 'epoch': 2.0757857044153254, 'step': 1727500}
INFO:transformers.trainer:{'loss': 2.3137515217661857, 'learning_rate': 1.5393558155044068e-05, 'epoch': 2.076386510697356, 'step': 1728000}
INFO:transformers.trainer:{'loss': 2.3080088220834734, 'learning_rate': 1.538354471701023e-05, 'epoch': 2.076987316979386, 'step': 1728500}
INFO:transformers.trainer:{'loss': 2.3167991889715194, 'learning_rate': 1.5373531278976385e-05, 'epoch': 2.0775881232614166, 'step': 1729000}
INFO:transformers.trainer:{'loss': 2.309703493833542, 'learning_rate': 1.5363517840942546e-05, 'epoch': 2.0781889295434475, 'step': 1729500}
INFO:transformers.trainer:{'loss': 2.3220293757915496, 'learning_rate': 1.5353504402908706e-05, 'epoch': 2.078789735825478, 'step': 1730000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1730000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1730000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1730000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1710000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3401029583215713, 'learning_rate': 1.5343490964874863e-05, 'epoch': 2.0793905421075083, 'step': 1730500}
INFO:transformers.trainer:{'loss': 2.3020809581279753, 'learning_rate': 1.5333477526841024e-05, 'epoch': 2.0799913483895387, 'step': 1731000}
INFO:transformers.trainer:{'loss': 2.3004192942380906, 'learning_rate': 1.532346408880718e-05, 'epoch': 2.080592154671569, 'step': 1731500}
INFO:transformers.trainer:{'loss': 2.3280949434041975, 'learning_rate': 1.5313450650773338e-05, 'epoch': 2.0811929609536, 'step': 1732000}
INFO:transformers.trainer:{'loss': 2.285168267607689, 'learning_rate': 1.5303437212739495e-05, 'epoch': 2.0817937672356304, 'step': 1732500}
INFO:transformers.trainer:{'loss': 2.3722264713048933, 'learning_rate': 1.5293423774705656e-05, 'epoch': 2.0823945735176608, 'step': 1733000}
INFO:transformers.trainer:{'loss': 2.2935160621404647, 'learning_rate': 1.5283410336671813e-05, 'epoch': 2.082995379799691, 'step': 1733500}
INFO:transformers.trainer:{'loss': 2.2426224838495257, 'learning_rate': 1.5273396898637973e-05, 'epoch': 2.0835961860817216, 'step': 1734000}
INFO:transformers.trainer:{'loss': 2.334780506968498, 'learning_rate': 1.526338346060413e-05, 'epoch': 2.084196992363752, 'step': 1734500}
INFO:transformers.trainer:{'loss': 2.3332146204710007, 'learning_rate': 1.525337002257029e-05, 'epoch': 2.084797798645783, 'step': 1735000}
INFO:transformers.trainer:{'loss': 2.3254715086221696, 'learning_rate': 1.5243356584536448e-05, 'epoch': 2.0853986049278133, 'step': 1735500}
INFO:transformers.trainer:{'loss': 2.2999854485988616, 'learning_rate': 1.5233343146502606e-05, 'epoch': 2.0859994112098437, 'step': 1736000}
INFO:transformers.trainer:{'loss': 2.310722538113594, 'learning_rate': 1.5223329708468767e-05, 'epoch': 2.086600217491874, 'step': 1736500}
INFO:transformers.trainer:{'loss': 2.3117816244363785, 'learning_rate': 1.5213316270434924e-05, 'epoch': 2.0872010237739045, 'step': 1737000}
INFO:transformers.trainer:{'loss': 2.3352786309719087, 'learning_rate': 1.5203302832401084e-05, 'epoch': 2.087801830055935, 'step': 1737500}
INFO:transformers.trainer:{'loss': 2.293837892651558, 'learning_rate': 1.5193289394367241e-05, 'epoch': 2.0884026363379657, 'step': 1738000}
INFO:transformers.trainer:{'loss': 2.346055579662323, 'learning_rate': 1.51832759563334e-05, 'epoch': 2.089003442619996, 'step': 1738500}
INFO:transformers.trainer:{'loss': 2.259348549425602, 'learning_rate': 1.5173262518299557e-05, 'epoch': 2.0896042489020266, 'step': 1739000}
INFO:transformers.trainer:{'loss': 2.3194810967445374, 'learning_rate': 1.5163249080265718e-05, 'epoch': 2.090205055184057, 'step': 1739500}
INFO:transformers.trainer:{'loss': 2.3262196896076204, 'learning_rate': 1.5153235642231875e-05, 'epoch': 2.0908058614660874, 'step': 1740000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1740000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1740000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1740000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1720000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2797381389141083, 'learning_rate': 1.5143222204198035e-05, 'epoch': 2.091406667748118, 'step': 1740500}
INFO:transformers.trainer:{'loss': 2.279582023382187, 'learning_rate': 1.5133208766164192e-05, 'epoch': 2.0920074740301486, 'step': 1741000}
INFO:transformers.trainer:{'loss': 2.3285418684482573, 'learning_rate': 1.5123195328130351e-05, 'epoch': 2.092608280312179, 'step': 1741500}
INFO:transformers.trainer:{'loss': 2.257707143187523, 'learning_rate': 1.5113181890096508e-05, 'epoch': 2.0932090865942095, 'step': 1742000}
INFO:transformers.trainer:{'loss': 2.3554181056022645, 'learning_rate': 1.5103168452062669e-05, 'epoch': 2.09380989287624, 'step': 1742500}
INFO:transformers.trainer:{'loss': 2.2873410027027132, 'learning_rate': 1.5093155014028829e-05, 'epoch': 2.0944106991582703, 'step': 1743000}
INFO:transformers.trainer:{'loss': 2.2663529632091524, 'learning_rate': 1.5083141575994986e-05, 'epoch': 2.0950115054403007, 'step': 1743500}
INFO:transformers.trainer:{'loss': 2.297164998412132, 'learning_rate': 1.5073128137961145e-05, 'epoch': 2.0956123117223315, 'step': 1744000}
INFO:transformers.trainer:{'loss': 2.323273141145706, 'learning_rate': 1.5063114699927302e-05, 'epoch': 2.096213118004362, 'step': 1744500}
INFO:transformers.trainer:{'loss': 2.3041536338627338, 'learning_rate': 1.5053101261893462e-05, 'epoch': 2.0968139242863923, 'step': 1745000}
INFO:transformers.trainer:{'loss': 2.306734050631523, 'learning_rate': 1.504308782385962e-05, 'epoch': 2.0974147305684228, 'step': 1745500}
INFO:transformers.trainer:{'loss': 2.3294589775800705, 'learning_rate': 1.503307438582578e-05, 'epoch': 2.098015536850453, 'step': 1746000}
INFO:transformers.trainer:{'loss': 2.326482750058174, 'learning_rate': 1.5023060947791937e-05, 'epoch': 2.098616343132484, 'step': 1746500}
INFO:transformers.trainer:{'loss': 2.2663710614442825, 'learning_rate': 1.5013047509758096e-05, 'epoch': 2.0992171494145144, 'step': 1747000}
INFO:transformers.trainer:{'loss': 2.285184250950813, 'learning_rate': 1.5003034071724253e-05, 'epoch': 2.099817955696545, 'step': 1747500}
INFO:transformers.trainer:{'loss': 2.3393348199129105, 'learning_rate': 1.4993020633690413e-05, 'epoch': 2.1004187619785752, 'step': 1748000}
INFO:transformers.trainer:{'loss': 2.3279683301448824, 'learning_rate': 1.4983007195656574e-05, 'epoch': 2.1010195682606057, 'step': 1748500}
INFO:transformers.trainer:{'loss': 2.3031521121263503, 'learning_rate': 1.497299375762273e-05, 'epoch': 2.101620374542636, 'step': 1749000}
INFO:transformers.trainer:{'loss': 2.320316922903061, 'learning_rate': 1.496298031958889e-05, 'epoch': 2.102221180824667, 'step': 1749500}
INFO:transformers.trainer:{'loss': 2.302210481882095, 'learning_rate': 1.4952966881555047e-05, 'epoch': 2.1028219871066973, 'step': 1750000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1750000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1750000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1750000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1730000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.32371766102314, 'learning_rate': 1.4942953443521207e-05, 'epoch': 2.1034227933887277, 'step': 1750500}
INFO:transformers.trainer:{'loss': 2.311238889455795, 'learning_rate': 1.4932940005487364e-05, 'epoch': 2.104023599670758, 'step': 1751000}
INFO:transformers.trainer:{'loss': 2.309852859735489, 'learning_rate': 1.4922926567453525e-05, 'epoch': 2.1046244059527885, 'step': 1751500}
INFO:transformers.trainer:{'loss': 2.2889734798669816, 'learning_rate': 1.4912913129419682e-05, 'epoch': 2.105225212234819, 'step': 1752000}
INFO:transformers.trainer:{'loss': 2.353857745528221, 'learning_rate': 1.490289969138584e-05, 'epoch': 2.10582601851685, 'step': 1752500}
INFO:transformers.trainer:{'loss': 2.332561149120331, 'learning_rate': 1.4892886253351997e-05, 'epoch': 2.10642682479888, 'step': 1753000}
INFO:transformers.trainer:{'loss': 2.298240094423294, 'learning_rate': 1.4882872815318158e-05, 'epoch': 2.1070276310809106, 'step': 1753500}
INFO:transformers.trainer:{'loss': 2.305395588636398, 'learning_rate': 1.4872859377284315e-05, 'epoch': 2.107628437362941, 'step': 1754000}
INFO:transformers.trainer:{'loss': 2.278603043437004, 'learning_rate': 1.4862845939250475e-05, 'epoch': 2.1082292436449714, 'step': 1754500}
INFO:transformers.trainer:{'loss': 2.2345032529830933, 'learning_rate': 1.4852832501216634e-05, 'epoch': 2.108830049927002, 'step': 1755000}
INFO:transformers.trainer:{'loss': 2.302916548728943, 'learning_rate': 1.4842819063182791e-05, 'epoch': 2.1094308562090327, 'step': 1755500}
INFO:transformers.trainer:{'loss': 2.2943913110494614, 'learning_rate': 1.4832805625148952e-05, 'epoch': 2.110031662491063, 'step': 1756000}
INFO:transformers.trainer:{'loss': 2.2937353780269625, 'learning_rate': 1.4822792187115109e-05, 'epoch': 2.1106324687730935, 'step': 1756500}
INFO:transformers.trainer:{'loss': 2.347420942902565, 'learning_rate': 1.481277874908127e-05, 'epoch': 2.111233275055124, 'step': 1757000}
INFO:transformers.trainer:{'loss': 2.292108740389347, 'learning_rate': 1.4802765311047426e-05, 'epoch': 2.1118340813371543, 'step': 1757500}
INFO:transformers.trainer:{'loss': 2.336863331437111, 'learning_rate': 1.4792751873013585e-05, 'epoch': 2.1124348876191847, 'step': 1758000}
INFO:transformers.trainer:{'loss': 2.3536677079200743, 'learning_rate': 1.4782738434979742e-05, 'epoch': 2.1130356939012156, 'step': 1758500}
INFO:transformers.trainer:{'loss': 2.316963916420937, 'learning_rate': 1.4772724996945903e-05, 'epoch': 2.113636500183246, 'step': 1759000}
INFO:transformers.trainer:{'loss': 2.319547300338745, 'learning_rate': 1.476271155891206e-05, 'epoch': 2.1142373064652764, 'step': 1759500}
INFO:transformers.trainer:{'loss': 2.2936607565283778, 'learning_rate': 1.475269812087822e-05, 'epoch': 2.114838112747307, 'step': 1760000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1760000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1760000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1760000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1740000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.337523254752159, 'learning_rate': 1.4742684682844377e-05, 'epoch': 2.1154389190293372, 'step': 1760500}
INFO:transformers.trainer:{'loss': 2.2767559094429015, 'learning_rate': 1.4732671244810536e-05, 'epoch': 2.116039725311368, 'step': 1761000}
INFO:transformers.trainer:{'loss': 2.3079281796216966, 'learning_rate': 1.4722657806776696e-05, 'epoch': 2.1166405315933985, 'step': 1761500}
INFO:transformers.trainer:{'loss': 2.3186561609506606, 'learning_rate': 1.4712644368742853e-05, 'epoch': 2.117241337875429, 'step': 1762000}
INFO:transformers.trainer:{'loss': 2.272694252848625, 'learning_rate': 1.4702630930709014e-05, 'epoch': 2.1178421441574593, 'step': 1762500}
INFO:transformers.trainer:{'loss': 2.2853353484869, 'learning_rate': 1.4692617492675171e-05, 'epoch': 2.1184429504394897, 'step': 1763000}
INFO:transformers.trainer:{'loss': 2.336463019013405, 'learning_rate': 1.468260405464133e-05, 'epoch': 2.11904375672152, 'step': 1763500}
INFO:transformers.trainer:{'loss': 2.235735913693905, 'learning_rate': 1.4672590616607487e-05, 'epoch': 2.119644563003551, 'step': 1764000}
INFO:transformers.trainer:{'loss': 2.3334171866178512, 'learning_rate': 1.4662577178573647e-05, 'epoch': 2.1202453692855814, 'step': 1764500}
INFO:transformers.trainer:{'loss': 2.265479238629341, 'learning_rate': 1.4652563740539804e-05, 'epoch': 2.120846175567612, 'step': 1765000}
INFO:transformers.trainer:{'loss': 2.3242176424264906, 'learning_rate': 1.4642550302505965e-05, 'epoch': 2.121446981849642, 'step': 1765500}
INFO:transformers.trainer:{'loss': 2.320549842953682, 'learning_rate': 1.4632536864472122e-05, 'epoch': 2.1220477881316726, 'step': 1766000}
INFO:transformers.trainer:{'loss': 2.3489812735319138, 'learning_rate': 1.462252342643828e-05, 'epoch': 2.122648594413703, 'step': 1766500}
INFO:transformers.trainer:{'loss': 2.32134651529789, 'learning_rate': 1.4612509988404438e-05, 'epoch': 2.123249400695734, 'step': 1767000}
INFO:transformers.trainer:{'loss': 2.2689871641397477, 'learning_rate': 1.4602496550370598e-05, 'epoch': 2.1238502069777643, 'step': 1767500}
INFO:transformers.trainer:{'loss': 2.24439328122139, 'learning_rate': 1.4592483112336758e-05, 'epoch': 2.1244510132597947, 'step': 1768000}
INFO:transformers.trainer:{'loss': 2.2665250977277758, 'learning_rate': 1.4582469674302916e-05, 'epoch': 2.125051819541825, 'step': 1768500}
INFO:transformers.trainer:{'loss': 2.2896225014925005, 'learning_rate': 1.4572456236269074e-05, 'epoch': 2.1256526258238555, 'step': 1769000}
INFO:transformers.trainer:{'loss': 2.327307977437973, 'learning_rate': 1.4562442798235231e-05, 'epoch': 2.126253432105886, 'step': 1769500}
INFO:transformers.trainer:{'loss': 2.258954024553299, 'learning_rate': 1.4552429360201392e-05, 'epoch': 2.1268542383879168, 'step': 1770000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1770000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1770000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1770000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1750000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.310633162975311, 'learning_rate': 1.4542415922167549e-05, 'epoch': 2.127455044669947, 'step': 1770500}
INFO:transformers.trainer:{'loss': 2.2755262821912767, 'learning_rate': 1.453240248413371e-05, 'epoch': 2.1280558509519776, 'step': 1771000}
INFO:transformers.trainer:{'loss': 2.31721397793293, 'learning_rate': 1.4522389046099866e-05, 'epoch': 2.128656657234008, 'step': 1771500}
INFO:transformers.trainer:{'loss': 2.245236774027348, 'learning_rate': 1.4512375608066025e-05, 'epoch': 2.1292574635160384, 'step': 1772000}
INFO:transformers.trainer:{'loss': 2.2888789068460467, 'learning_rate': 1.4502362170032182e-05, 'epoch': 2.129858269798069, 'step': 1772500}
INFO:transformers.trainer:{'loss': 2.2913029371500016, 'learning_rate': 1.4492348731998343e-05, 'epoch': 2.1304590760800997, 'step': 1773000}
INFO:transformers.trainer:{'loss': 2.2591205441951754, 'learning_rate': 1.4482335293964503e-05, 'epoch': 2.13105988236213, 'step': 1773500}
INFO:transformers.trainer:{'loss': 2.3236760950684547, 'learning_rate': 1.447232185593066e-05, 'epoch': 2.1316606886441605, 'step': 1774000}
INFO:transformers.trainer:{'loss': 2.279938999533653, 'learning_rate': 1.4462308417896819e-05, 'epoch': 2.132261494926191, 'step': 1774500}
INFO:transformers.trainer:{'loss': 2.304646335363388, 'learning_rate': 1.4452294979862976e-05, 'epoch': 2.1328623012082213, 'step': 1775000}
INFO:transformers.trainer:{'loss': 2.2856008740663527, 'learning_rate': 1.4442281541829136e-05, 'epoch': 2.133463107490252, 'step': 1775500}
INFO:transformers.trainer:{'loss': 2.310958322763443, 'learning_rate': 1.4432268103795294e-05, 'epoch': 2.1340639137722826, 'step': 1776000}
INFO:transformers.trainer:{'loss': 2.2409279227256773, 'learning_rate': 1.4422254665761454e-05, 'epoch': 2.134664720054313, 'step': 1776500}
INFO:transformers.trainer:{'loss': 2.305425574064255, 'learning_rate': 1.4412241227727611e-05, 'epoch': 2.1352655263363434, 'step': 1777000}
INFO:transformers.trainer:{'loss': 2.308581356525421, 'learning_rate': 1.440222778969377e-05, 'epoch': 2.1358663326183738, 'step': 1777500}
INFO:transformers.trainer:{'loss': 2.2909130955338477, 'learning_rate': 1.4392214351659927e-05, 'epoch': 2.136467138900404, 'step': 1778000}
INFO:transformers.trainer:{'loss': 2.282787120223045, 'learning_rate': 1.4382200913626087e-05, 'epoch': 2.1370679451824346, 'step': 1778500}
INFO:transformers.trainer:{'loss': 2.2762598267197607, 'learning_rate': 1.4372187475592244e-05, 'epoch': 2.1376687514644654, 'step': 1779000}
INFO:transformers.trainer:{'loss': 2.3388735296726226, 'learning_rate': 1.4362174037558405e-05, 'epoch': 2.138269557746496, 'step': 1779500}
INFO:transformers.trainer:{'loss': 2.2795079587697984, 'learning_rate': 1.4352160599524564e-05, 'epoch': 2.1388703640285263, 'step': 1780000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1780000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1780000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1780000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1760000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2563019587993622, 'learning_rate': 1.434214716149072e-05, 'epoch': 2.1394711703105567, 'step': 1780500}
INFO:transformers.trainer:{'loss': 2.312177882671356, 'learning_rate': 1.4332133723456881e-05, 'epoch': 2.140071976592587, 'step': 1781000}
INFO:transformers.trainer:{'loss': 2.2892952538728712, 'learning_rate': 1.4322120285423038e-05, 'epoch': 2.140672782874618, 'step': 1781500}
INFO:transformers.trainer:{'loss': 2.3632689843177794, 'learning_rate': 1.4312106847389199e-05, 'epoch': 2.1412735891566483, 'step': 1782000}
INFO:transformers.trainer:{'loss': 2.318605444312096, 'learning_rate': 1.4302093409355356e-05, 'epoch': 2.1418743954386787, 'step': 1782500}
INFO:transformers.trainer:{'loss': 2.2703137955665587, 'learning_rate': 1.4292079971321514e-05, 'epoch': 2.142475201720709, 'step': 1783000}
INFO:transformers.trainer:{'loss': 2.3212200334072115, 'learning_rate': 1.4282066533287672e-05, 'epoch': 2.1430760080027396, 'step': 1783500}
INFO:transformers.trainer:{'loss': 2.3034188631772996, 'learning_rate': 1.4272053095253832e-05, 'epoch': 2.14367681428477, 'step': 1784000}
INFO:transformers.trainer:{'loss': 2.2992263950705527, 'learning_rate': 1.4262039657219989e-05, 'epoch': 2.144277620566801, 'step': 1784500}
INFO:transformers.trainer:{'loss': 2.290206476151943, 'learning_rate': 1.425202621918615e-05, 'epoch': 2.1448784268488312, 'step': 1785000}
INFO:transformers.trainer:{'loss': 2.344205396652222, 'learning_rate': 1.4242012781152307e-05, 'epoch': 2.1454792331308616, 'step': 1785500}
INFO:transformers.trainer:{'loss': 2.2831533188819884, 'learning_rate': 1.4231999343118465e-05, 'epoch': 2.146080039412892, 'step': 1786000}
INFO:transformers.trainer:{'loss': 2.3152847496271134, 'learning_rate': 1.4221985905084626e-05, 'epoch': 2.1466808456949225, 'step': 1786500}
INFO:transformers.trainer:{'loss': 2.352639342844486, 'learning_rate': 1.4211972467050783e-05, 'epoch': 2.147281651976953, 'step': 1787000}
INFO:transformers.trainer:{'loss': 2.342985762119293, 'learning_rate': 1.4201959029016942e-05, 'epoch': 2.1478824582589837, 'step': 1787500}
INFO:transformers.trainer:{'loss': 2.2551270298957826, 'learning_rate': 1.41919455909831e-05, 'epoch': 2.148483264541014, 'step': 1788000}
INFO:transformers.trainer:{'loss': 2.310838339328766, 'learning_rate': 1.4181932152949259e-05, 'epoch': 2.1490840708230445, 'step': 1788500}
INFO:transformers.trainer:{'loss': 2.2228740421533586, 'learning_rate': 1.4171918714915416e-05, 'epoch': 2.149684877105075, 'step': 1789000}
INFO:transformers.trainer:{'loss': 2.24881447160244, 'learning_rate': 1.4161905276881577e-05, 'epoch': 2.1502856833871054, 'step': 1789500}
INFO:transformers.trainer:{'loss': 2.3423470883369446, 'learning_rate': 1.4151891838847734e-05, 'epoch': 2.150886489669136, 'step': 1790000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1790000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1790000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1790000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1770000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.276310416698456, 'learning_rate': 1.4141878400813892e-05, 'epoch': 2.1514872959511666, 'step': 1790500}
INFO:transformers.trainer:{'loss': 2.2412378319501878, 'learning_rate': 1.4131864962780051e-05, 'epoch': 2.152088102233197, 'step': 1791000}
INFO:transformers.trainer:{'loss': 2.3007604262828827, 'learning_rate': 1.412185152474621e-05, 'epoch': 2.1526889085152274, 'step': 1791500}
INFO:transformers.trainer:{'loss': 2.323199691653252, 'learning_rate': 1.4111838086712367e-05, 'epoch': 2.153289714797258, 'step': 1792000}
INFO:transformers.trainer:{'loss': 2.3216001212596895, 'learning_rate': 1.4101824648678527e-05, 'epoch': 2.1538905210792882, 'step': 1792500}
INFO:transformers.trainer:{'loss': 2.2289071916341783, 'learning_rate': 1.4091811210644686e-05, 'epoch': 2.154491327361319, 'step': 1793000}
INFO:transformers.trainer:{'loss': 2.270764074683189, 'learning_rate': 1.4081797772610843e-05, 'epoch': 2.1550921336433495, 'step': 1793500}
INFO:transformers.trainer:{'loss': 2.3207416187524794, 'learning_rate': 1.4071784334577004e-05, 'epoch': 2.15569293992538, 'step': 1794000}
INFO:transformers.trainer:{'loss': 2.340067472219467, 'learning_rate': 1.406177089654316e-05, 'epoch': 2.1562937462074103, 'step': 1794500}
INFO:transformers.trainer:{'loss': 2.318678103208542, 'learning_rate': 1.4051757458509321e-05, 'epoch': 2.1568945524894407, 'step': 1795000}
INFO:transformers.trainer:{'loss': 2.316037192106247, 'learning_rate': 1.4041744020475478e-05, 'epoch': 2.157495358771471, 'step': 1795500}
INFO:transformers.trainer:{'loss': 2.329167235612869, 'learning_rate': 1.4031730582441637e-05, 'epoch': 2.158096165053502, 'step': 1796000}
INFO:transformers.trainer:{'loss': 2.2836317673325537, 'learning_rate': 1.4021717144407794e-05, 'epoch': 2.1586969713355324, 'step': 1796500}
INFO:transformers.trainer:{'loss': 2.3011749678850175, 'learning_rate': 1.4011703706373955e-05, 'epoch': 2.159297777617563, 'step': 1797000}
INFO:transformers.trainer:{'loss': 2.2550367759466172, 'learning_rate': 1.4001690268340112e-05, 'epoch': 2.159898583899593, 'step': 1797500}
INFO:transformers.trainer:{'loss': 2.3138395801782607, 'learning_rate': 1.3991676830306272e-05, 'epoch': 2.1604993901816236, 'step': 1798000}
INFO:transformers.trainer:{'loss': 2.31400588285923, 'learning_rate': 1.3981663392272431e-05, 'epoch': 2.161100196463654, 'step': 1798500}
INFO:transformers.trainer:{'loss': 2.3597895025014877, 'learning_rate': 1.3971649954238588e-05, 'epoch': 2.161701002745685, 'step': 1799000}
INFO:transformers.trainer:{'loss': 2.318838688850403, 'learning_rate': 1.3961636516204748e-05, 'epoch': 2.1623018090277153, 'step': 1799500}
INFO:transformers.trainer:{'loss': 2.2328852977752685, 'learning_rate': 1.3951623078170905e-05, 'epoch': 2.1629026153097457, 'step': 1800000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1800000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1800000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1800000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1780000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.310020983815193, 'learning_rate': 1.3941609640137066e-05, 'epoch': 2.163503421591776, 'step': 1800500}
INFO:transformers.trainer:{'loss': 2.3308626698851587, 'learning_rate': 1.3931596202103223e-05, 'epoch': 2.1641042278738065, 'step': 1801000}
INFO:transformers.trainer:{'loss': 2.3428361257016657, 'learning_rate': 1.3921582764069382e-05, 'epoch': 2.164705034155837, 'step': 1801500}
INFO:transformers.trainer:{'loss': 2.3113989412784575, 'learning_rate': 1.3911569326035539e-05, 'epoch': 2.165305840437868, 'step': 1802000}
INFO:transformers.trainer:{'loss': 2.306012885212898, 'learning_rate': 1.39015558880017e-05, 'epoch': 2.165906646719898, 'step': 1802500}
INFO:transformers.trainer:{'loss': 2.3006340379714967, 'learning_rate': 1.3891542449967856e-05, 'epoch': 2.1665074530019286, 'step': 1803000}
INFO:transformers.trainer:{'loss': 2.299442194223404, 'learning_rate': 1.3881529011934017e-05, 'epoch': 2.167108259283959, 'step': 1803500}
INFO:transformers.trainer:{'loss': 2.27567514526844, 'learning_rate': 1.3871515573900174e-05, 'epoch': 2.1677090655659894, 'step': 1804000}
INFO:transformers.trainer:{'loss': 2.322838566184044, 'learning_rate': 1.3861502135866333e-05, 'epoch': 2.1683098718480203, 'step': 1804500}
INFO:transformers.trainer:{'loss': 2.266523014187813, 'learning_rate': 1.3851488697832493e-05, 'epoch': 2.1689106781300507, 'step': 1805000}
INFO:transformers.trainer:{'loss': 2.298026592731476, 'learning_rate': 1.384147525979865e-05, 'epoch': 2.169511484412081, 'step': 1805500}
INFO:transformers.trainer:{'loss': 2.267286073446274, 'learning_rate': 1.383146182176481e-05, 'epoch': 2.1701122906941115, 'step': 1806000}
INFO:transformers.trainer:{'loss': 2.35326656627655, 'learning_rate': 1.3821448383730968e-05, 'epoch': 2.170713096976142, 'step': 1806500}
INFO:transformers.trainer:{'loss': 2.2524122446775436, 'learning_rate': 1.3811434945697126e-05, 'epoch': 2.1713139032581723, 'step': 1807000}
INFO:transformers.trainer:{'loss': 2.2664971832036973, 'learning_rate': 1.3801421507663283e-05, 'epoch': 2.171914709540203, 'step': 1807500}
INFO:transformers.trainer:{'loss': 2.283577538728714, 'learning_rate': 1.3791408069629444e-05, 'epoch': 2.1725155158222336, 'step': 1808000}
INFO:transformers.trainer:{'loss': 2.2687434363365173, 'learning_rate': 1.3781394631595601e-05, 'epoch': 2.173116322104264, 'step': 1808500}
INFO:transformers.trainer:{'loss': 2.2831124379634855, 'learning_rate': 1.3771381193561761e-05, 'epoch': 2.1737171283862944, 'step': 1809000}
INFO:transformers.trainer:{'loss': 2.260781284332275, 'learning_rate': 1.3761367755527918e-05, 'epoch': 2.174317934668325, 'step': 1809500}
INFO:transformers.trainer:{'loss': 2.2735765562057497, 'learning_rate': 1.3751354317494077e-05, 'epoch': 2.174918740950355, 'step': 1810000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1810000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1810000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1810000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1790000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.295291180431843, 'learning_rate': 1.3741340879460234e-05, 'epoch': 2.175519547232386, 'step': 1810500}
INFO:transformers.trainer:{'loss': 2.2337478045225145, 'learning_rate': 1.3731327441426395e-05, 'epoch': 2.1761203535144165, 'step': 1811000}
INFO:transformers.trainer:{'loss': 2.317510074019432, 'learning_rate': 1.3721314003392555e-05, 'epoch': 2.176721159796447, 'step': 1811500}
INFO:transformers.trainer:{'loss': 2.2725757229924204, 'learning_rate': 1.3711300565358712e-05, 'epoch': 2.1773219660784773, 'step': 1812000}
INFO:transformers.trainer:{'loss': 2.2602431243658065, 'learning_rate': 1.3701287127324871e-05, 'epoch': 2.1779227723605077, 'step': 1812500}
INFO:transformers.trainer:{'loss': 2.2383494580984116, 'learning_rate': 1.3691273689291028e-05, 'epoch': 2.178523578642538, 'step': 1813000}
INFO:transformers.trainer:{'loss': 2.2950563571453095, 'learning_rate': 1.3681260251257189e-05, 'epoch': 2.179124384924569, 'step': 1813500}
INFO:transformers.trainer:{'loss': 2.2820740315914154, 'learning_rate': 1.3671246813223346e-05, 'epoch': 2.1797251912065994, 'step': 1814000}
INFO:transformers.trainer:{'loss': 2.2648850058317183, 'learning_rate': 1.3661233375189506e-05, 'epoch': 2.1803259974886298, 'step': 1814500}
INFO:transformers.trainer:{'loss': 2.311966404080391, 'learning_rate': 1.3651219937155663e-05, 'epoch': 2.18092680377066, 'step': 1815000}
INFO:transformers.trainer:{'loss': 2.299399846673012, 'learning_rate': 1.3641206499121822e-05, 'epoch': 2.1815276100526906, 'step': 1815500}
INFO:transformers.trainer:{'loss': 2.3015813114643096, 'learning_rate': 1.3631193061087979e-05, 'epoch': 2.182128416334721, 'step': 1816000}
INFO:transformers.trainer:{'loss': 2.257859216570854, 'learning_rate': 1.362117962305414e-05, 'epoch': 2.182729222616752, 'step': 1816500}
INFO:transformers.trainer:{'loss': 2.270711003422737, 'learning_rate': 1.36111661850203e-05, 'epoch': 2.1833300288987822, 'step': 1817000}
INFO:transformers.trainer:{'loss': 2.387384553551674, 'learning_rate': 1.3601152746986457e-05, 'epoch': 2.1839308351808127, 'step': 1817500}
INFO:transformers.trainer:{'loss': 2.3320677587985994, 'learning_rate': 1.3591139308952616e-05, 'epoch': 2.184531641462843, 'step': 1818000}
INFO:transformers.trainer:{'loss': 2.33472958111763, 'learning_rate': 1.3581125870918773e-05, 'epoch': 2.1851324477448735, 'step': 1818500}
INFO:transformers.trainer:{'loss': 2.339817521929741, 'learning_rate': 1.3571112432884933e-05, 'epoch': 2.1857332540269043, 'step': 1819000}
INFO:transformers.trainer:{'loss': 2.2987417232990266, 'learning_rate': 1.356109899485109e-05, 'epoch': 2.1863340603089347, 'step': 1819500}
INFO:transformers.trainer:{'loss': 2.269854447364807, 'learning_rate': 1.355108555681725e-05, 'epoch': 2.186934866590965, 'step': 1820000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1820000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1820000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1820000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1800000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3006727797985076, 'learning_rate': 1.3541072118783408e-05, 'epoch': 2.1875356728729956, 'step': 1820500}
INFO:transformers.trainer:{'loss': 2.279218520760536, 'learning_rate': 1.3531058680749567e-05, 'epoch': 2.188136479155026, 'step': 1821000}
INFO:transformers.trainer:{'loss': 2.2403285055160524, 'learning_rate': 1.3521045242715724e-05, 'epoch': 2.1887372854370564, 'step': 1821500}
INFO:transformers.trainer:{'loss': 2.3033810937404633, 'learning_rate': 1.3511031804681884e-05, 'epoch': 2.189338091719087, 'step': 1822000}
INFO:transformers.trainer:{'loss': 2.3112126294374464, 'learning_rate': 1.3501018366648041e-05, 'epoch': 2.1899388980011176, 'step': 1822500}
INFO:transformers.trainer:{'loss': 2.275536234498024, 'learning_rate': 1.3491004928614202e-05, 'epoch': 2.190539704283148, 'step': 1823000}
INFO:transformers.trainer:{'loss': 2.301549864768982, 'learning_rate': 1.348099149058036e-05, 'epoch': 2.1911405105651784, 'step': 1823500}
INFO:transformers.trainer:{'loss': 2.2665027429461477, 'learning_rate': 1.3470978052546517e-05, 'epoch': 2.191741316847209, 'step': 1824000}
INFO:transformers.trainer:{'loss': 2.344700319647789, 'learning_rate': 1.3460964614512678e-05, 'epoch': 2.1923421231292393, 'step': 1824500}
INFO:transformers.trainer:{'loss': 2.280418774008751, 'learning_rate': 1.3450951176478835e-05, 'epoch': 2.19294292941127, 'step': 1825000}
INFO:transformers.trainer:{'loss': 2.285893828511238, 'learning_rate': 1.3440937738444995e-05, 'epoch': 2.1935437356933005, 'step': 1825500}
INFO:transformers.trainer:{'loss': 2.32227243322134, 'learning_rate': 1.3430924300411152e-05, 'epoch': 2.194144541975331, 'step': 1826000}
INFO:transformers.trainer:{'loss': 2.301750298976898, 'learning_rate': 1.3420910862377311e-05, 'epoch': 2.1947453482573613, 'step': 1826500}
INFO:transformers.trainer:{'loss': 2.283043618738651, 'learning_rate': 1.3410897424343468e-05, 'epoch': 2.1953461545393917, 'step': 1827000}
INFO:transformers.trainer:{'loss': 2.2576253883838655, 'learning_rate': 1.3400883986309629e-05, 'epoch': 2.195946960821422, 'step': 1827500}
INFO:transformers.trainer:{'loss': 2.263833958148956, 'learning_rate': 1.3390870548275786e-05, 'epoch': 2.196547767103453, 'step': 1828000}
INFO:transformers.trainer:{'loss': 2.300719294667244, 'learning_rate': 1.3380857110241946e-05, 'epoch': 2.1971485733854834, 'step': 1828500}
INFO:transformers.trainer:{'loss': 2.2913703664541245, 'learning_rate': 1.3370843672208103e-05, 'epoch': 2.197749379667514, 'step': 1829000}
INFO:transformers.trainer:{'loss': 2.3559671535491944, 'learning_rate': 1.3360830234174262e-05, 'epoch': 2.1983501859495442, 'step': 1829500}
INFO:transformers.trainer:{'loss': 2.2782944453954697, 'learning_rate': 1.3350816796140422e-05, 'epoch': 2.1989509922315746, 'step': 1830000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1830000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1830000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1830000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1810000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.299978448212147, 'learning_rate': 1.334080335810658e-05, 'epoch': 2.199551798513605, 'step': 1830500}
INFO:transformers.trainer:{'loss': 2.250248740553856, 'learning_rate': 1.333078992007274e-05, 'epoch': 2.200152604795636, 'step': 1831000}
INFO:transformers.trainer:{'loss': 2.31267885696888, 'learning_rate': 1.3320776482038897e-05, 'epoch': 2.2007534110776663, 'step': 1831500}
INFO:transformers.trainer:{'loss': 2.261019088625908, 'learning_rate': 1.3310763044005056e-05, 'epoch': 2.2013542173596967, 'step': 1832000}
INFO:transformers.trainer:{'loss': 2.2642658821344375, 'learning_rate': 1.3300749605971213e-05, 'epoch': 2.201955023641727, 'step': 1832500}
INFO:transformers.trainer:{'loss': 2.2858469153642655, 'learning_rate': 1.3290736167937373e-05, 'epoch': 2.2025558299237575, 'step': 1833000}
INFO:transformers.trainer:{'loss': 2.358996439099312, 'learning_rate': 1.328072272990353e-05, 'epoch': 2.2031566362057884, 'step': 1833500}
INFO:transformers.trainer:{'loss': 2.298993023276329, 'learning_rate': 1.3270709291869691e-05, 'epoch': 2.203757442487819, 'step': 1834000}
INFO:transformers.trainer:{'loss': 2.2631231697797776, 'learning_rate': 1.3260695853835848e-05, 'epoch': 2.204358248769849, 'step': 1834500}
INFO:transformers.trainer:{'loss': 2.264662466406822, 'learning_rate': 1.3250682415802007e-05, 'epoch': 2.2049590550518796, 'step': 1835000}
INFO:transformers.trainer:{'loss': 2.2835807728767397, 'learning_rate': 1.3240668977768164e-05, 'epoch': 2.20555986133391, 'step': 1835500}
INFO:transformers.trainer:{'loss': 2.319211595416069, 'learning_rate': 1.3230655539734324e-05, 'epoch': 2.2061606676159404, 'step': 1836000}
INFO:transformers.trainer:{'loss': 2.281217419743538, 'learning_rate': 1.3220642101700485e-05, 'epoch': 2.2067614738979713, 'step': 1836500}
INFO:transformers.trainer:{'loss': 2.296969851732254, 'learning_rate': 1.3210628663666642e-05, 'epoch': 2.2073622801800017, 'step': 1837000}
INFO:transformers.trainer:{'loss': 2.2722245827913286, 'learning_rate': 1.32006152256328e-05, 'epoch': 2.207963086462032, 'step': 1837500}
INFO:transformers.trainer:{'loss': 2.291184706568718, 'learning_rate': 1.3190601787598958e-05, 'epoch': 2.2085638927440625, 'step': 1838000}
INFO:transformers.trainer:{'loss': 2.2807307353019715, 'learning_rate': 1.3180588349565118e-05, 'epoch': 2.209164699026093, 'step': 1838500}
INFO:transformers.trainer:{'loss': 2.255688698887825, 'learning_rate': 1.3170574911531275e-05, 'epoch': 2.2097655053081233, 'step': 1839000}
INFO:transformers.trainer:{'loss': 2.283049179196358, 'learning_rate': 1.3160561473497435e-05, 'epoch': 2.210366311590154, 'step': 1839500}
INFO:transformers.trainer:{'loss': 2.2778050500154494, 'learning_rate': 1.3150548035463593e-05, 'epoch': 2.2109671178721846, 'step': 1840000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1840000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1840000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1840000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1820000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2870813940763473, 'learning_rate': 1.3140534597429751e-05, 'epoch': 2.211567924154215, 'step': 1840500}
INFO:transformers.trainer:{'loss': 2.302754063844681, 'learning_rate': 1.3130521159395908e-05, 'epoch': 2.2121687304362454, 'step': 1841000}
INFO:transformers.trainer:{'loss': 2.2727559674978255, 'learning_rate': 1.3120507721362069e-05, 'epoch': 2.212769536718276, 'step': 1841500}
INFO:transformers.trainer:{'loss': 2.2680458260774614, 'learning_rate': 1.311049428332823e-05, 'epoch': 2.213370343000306, 'step': 1842000}
INFO:transformers.trainer:{'loss': 2.3244059165716173, 'learning_rate': 1.3100480845294386e-05, 'epoch': 2.213971149282337, 'step': 1842500}
INFO:transformers.trainer:{'loss': 2.2900026988983155, 'learning_rate': 1.3090467407260545e-05, 'epoch': 2.2145719555643675, 'step': 1843000}
INFO:transformers.trainer:{'loss': 2.306563230037689, 'learning_rate': 1.3080453969226702e-05, 'epoch': 2.215172761846398, 'step': 1843500}
INFO:transformers.trainer:{'loss': 2.2581011686325074, 'learning_rate': 1.3070440531192863e-05, 'epoch': 2.2157735681284283, 'step': 1844000}
INFO:transformers.trainer:{'loss': 2.2603397483825685, 'learning_rate': 1.306042709315902e-05, 'epoch': 2.2163743744104587, 'step': 1844500}
INFO:transformers.trainer:{'loss': 2.3164118554592132, 'learning_rate': 1.305041365512518e-05, 'epoch': 2.216975180692489, 'step': 1845000}
INFO:transformers.trainer:{'loss': 2.2971140911579133, 'learning_rate': 1.3040400217091337e-05, 'epoch': 2.21757598697452, 'step': 1845500}
INFO:transformers.trainer:{'loss': 2.2915291887521745, 'learning_rate': 1.3030386779057496e-05, 'epoch': 2.2181767932565504, 'step': 1846000}
INFO:transformers.trainer:{'loss': 2.3167895466089248, 'learning_rate': 1.3020373341023653e-05, 'epoch': 2.218777599538581, 'step': 1846500}
INFO:transformers.trainer:{'loss': 2.2965449684858323, 'learning_rate': 1.3010359902989813e-05, 'epoch': 2.219378405820611, 'step': 1847000}
INFO:transformers.trainer:{'loss': 2.2275660976171494, 'learning_rate': 1.300034646495597e-05, 'epoch': 2.2199792121026416, 'step': 1847500}
INFO:transformers.trainer:{'loss': 2.278432460308075, 'learning_rate': 1.2990333026922131e-05, 'epoch': 2.2205800183846724, 'step': 1848000}
INFO:transformers.trainer:{'loss': 2.2334761292934417, 'learning_rate': 1.298031958888829e-05, 'epoch': 2.221180824666703, 'step': 1848500}
INFO:transformers.trainer:{'loss': 2.2554142918586733, 'learning_rate': 1.2970306150854447e-05, 'epoch': 2.2217816309487333, 'step': 1849000}
INFO:transformers.trainer:{'loss': 2.310883815050125, 'learning_rate': 1.2960292712820607e-05, 'epoch': 2.2223824372307637, 'step': 1849500}
INFO:transformers.trainer:{'loss': 2.2892340480089186, 'learning_rate': 1.2950279274786764e-05, 'epoch': 2.222983243512794, 'step': 1850000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1850000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1850000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1850000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1830000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2570135630369186, 'learning_rate': 1.2940265836752923e-05, 'epoch': 2.2235840497948245, 'step': 1850500}
INFO:transformers.trainer:{'loss': 2.2687418521642684, 'learning_rate': 1.293025239871908e-05, 'epoch': 2.2241848560768553, 'step': 1851000}
INFO:transformers.trainer:{'loss': 2.280078877091408, 'learning_rate': 1.292023896068524e-05, 'epoch': 2.2247856623588858, 'step': 1851500}
INFO:transformers.trainer:{'loss': 2.3111435573101042, 'learning_rate': 1.2910225522651398e-05, 'epoch': 2.225386468640916, 'step': 1852000}
INFO:transformers.trainer:{'loss': 2.30070234310627, 'learning_rate': 1.2900212084617558e-05, 'epoch': 2.2259872749229466, 'step': 1852500}
INFO:transformers.trainer:{'loss': 2.3162780958414078, 'learning_rate': 1.2890198646583715e-05, 'epoch': 2.226588081204977, 'step': 1853000}
INFO:transformers.trainer:{'loss': 2.28820003426075, 'learning_rate': 1.2880185208549874e-05, 'epoch': 2.2271888874870074, 'step': 1853500}
INFO:transformers.trainer:{'loss': 2.2768217022418975, 'learning_rate': 1.2870171770516031e-05, 'epoch': 2.2277896937690382, 'step': 1854000}
INFO:transformers.trainer:{'loss': 2.2587217433452604, 'learning_rate': 1.2860158332482191e-05, 'epoch': 2.2283905000510686, 'step': 1854500}
INFO:transformers.trainer:{'loss': 2.250971781730652, 'learning_rate': 1.2850144894448352e-05, 'epoch': 2.228991306333099, 'step': 1855000}
INFO:transformers.trainer:{'loss': 2.250609045624733, 'learning_rate': 1.2840131456414509e-05, 'epoch': 2.2295921126151295, 'step': 1855500}
INFO:transformers.trainer:{'loss': 2.224335074186325, 'learning_rate': 1.2830118018380668e-05, 'epoch': 2.23019291889716, 'step': 1856000}
INFO:transformers.trainer:{'loss': 2.354041735649109, 'learning_rate': 1.2820104580346825e-05, 'epoch': 2.2307937251791903, 'step': 1856500}
INFO:transformers.trainer:{'loss': 2.251626243710518, 'learning_rate': 1.2810091142312985e-05, 'epoch': 2.231394531461221, 'step': 1857000}
INFO:transformers.trainer:{'loss': 2.28604132938385, 'learning_rate': 1.2800077704279142e-05, 'epoch': 2.2319953377432515, 'step': 1857500}
INFO:transformers.trainer:{'loss': 2.3126606645584107, 'learning_rate': 1.2790064266245303e-05, 'epoch': 2.232596144025282, 'step': 1858000}
INFO:transformers.trainer:{'loss': 2.2796044245958327, 'learning_rate': 1.278005082821146e-05, 'epoch': 2.2331969503073124, 'step': 1858500}
INFO:transformers.trainer:{'loss': 2.270713747859001, 'learning_rate': 1.2770037390177619e-05, 'epoch': 2.2337977565893428, 'step': 1859000}
INFO:transformers.trainer:{'loss': 2.3362302399873736, 'learning_rate': 1.2760023952143776e-05, 'epoch': 2.234398562871373, 'step': 1859500}
INFO:transformers.trainer:{'loss': 2.239302444934845, 'learning_rate': 1.2750010514109936e-05, 'epoch': 2.234999369153404, 'step': 1860000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1860000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1860000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1860000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1840000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.239057958126068, 'learning_rate': 1.2739997076076093e-05, 'epoch': 2.2356001754354344, 'step': 1860500}
INFO:transformers.trainer:{'loss': 2.251049964785576, 'learning_rate': 1.2729983638042254e-05, 'epoch': 2.236200981717465, 'step': 1861000}
INFO:transformers.trainer:{'loss': 2.329910449743271, 'learning_rate': 1.2719970200008412e-05, 'epoch': 2.2368017879994953, 'step': 1861500}
INFO:transformers.trainer:{'loss': 2.236946309566498, 'learning_rate': 1.270995676197457e-05, 'epoch': 2.2374025942815257, 'step': 1862000}
INFO:transformers.trainer:{'loss': 2.306810091495514, 'learning_rate': 1.269994332394073e-05, 'epoch': 2.2380034005635565, 'step': 1862500}
INFO:transformers.trainer:{'loss': 2.278429356575012, 'learning_rate': 1.2689929885906887e-05, 'epoch': 2.238604206845587, 'step': 1863000}
INFO:transformers.trainer:{'loss': 2.3214899035692214, 'learning_rate': 1.2679916447873047e-05, 'epoch': 2.2392050131276173, 'step': 1863500}
INFO:transformers.trainer:{'loss': 2.241472347021103, 'learning_rate': 1.2669903009839204e-05, 'epoch': 2.2398058194096477, 'step': 1864000}
INFO:transformers.trainer:{'loss': 2.297492675423622, 'learning_rate': 1.2659889571805363e-05, 'epoch': 2.240406625691678, 'step': 1864500}
INFO:transformers.trainer:{'loss': 2.339639132976532, 'learning_rate': 1.264987613377152e-05, 'epoch': 2.2410074319737086, 'step': 1865000}
INFO:transformers.trainer:{'loss': 2.2616902746558187, 'learning_rate': 1.263986269573768e-05, 'epoch': 2.2416082382557394, 'step': 1865500}
INFO:transformers.trainer:{'loss': 2.3017558312416075, 'learning_rate': 1.2629849257703838e-05, 'epoch': 2.24220904453777, 'step': 1866000}
INFO:transformers.trainer:{'loss': 2.326388995051384, 'learning_rate': 1.2619835819669998e-05, 'epoch': 2.2428098508198, 'step': 1866500}
INFO:transformers.trainer:{'loss': 2.2897629896998404, 'learning_rate': 1.2609822381636157e-05, 'epoch': 2.2434106571018306, 'step': 1867000}
INFO:transformers.trainer:{'loss': 2.2780646077394486, 'learning_rate': 1.2599808943602314e-05, 'epoch': 2.244011463383861, 'step': 1867500}
INFO:transformers.trainer:{'loss': 2.2635624879598617, 'learning_rate': 1.2589795505568475e-05, 'epoch': 2.2446122696658914, 'step': 1868000}
INFO:transformers.trainer:{'loss': 2.2647846573591233, 'learning_rate': 1.2579782067534632e-05, 'epoch': 2.2452130759479223, 'step': 1868500}
INFO:transformers.trainer:{'loss': 2.262768862962723, 'learning_rate': 1.2569768629500792e-05, 'epoch': 2.2458138822299527, 'step': 1869000}
INFO:transformers.trainer:{'loss': 2.2964233821630478, 'learning_rate': 1.2559755191466949e-05, 'epoch': 2.246414688511983, 'step': 1869500}
INFO:transformers.trainer:{'loss': 2.297175503253937, 'learning_rate': 1.2549741753433108e-05, 'epoch': 2.2470154947940135, 'step': 1870000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1870000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1870000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1870000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1850000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2573344513177873, 'learning_rate': 1.2539728315399265e-05, 'epoch': 2.247616301076044, 'step': 1870500}
INFO:transformers.trainer:{'loss': 2.2534477726221085, 'learning_rate': 1.2529714877365425e-05, 'epoch': 2.2482171073580743, 'step': 1871000}
INFO:transformers.trainer:{'loss': 2.3036610451936723, 'learning_rate': 1.2519701439331582e-05, 'epoch': 2.248817913640105, 'step': 1871500}
INFO:transformers.trainer:{'loss': 2.300497577190399, 'learning_rate': 1.2509688001297743e-05, 'epoch': 2.2494187199221356, 'step': 1872000}
INFO:transformers.trainer:{'loss': 2.272488384604454, 'learning_rate': 1.2499674563263902e-05, 'epoch': 2.250019526204166, 'step': 1872500}
INFO:transformers.trainer:{'loss': 2.325247477054596, 'learning_rate': 1.2489661125230059e-05, 'epoch': 2.2506203324861964, 'step': 1873000}
INFO:transformers.trainer:{'loss': 2.3085394330024718, 'learning_rate': 1.2479647687196218e-05, 'epoch': 2.251221138768227, 'step': 1873500}
INFO:transformers.trainer:{'loss': 2.2889386667013167, 'learning_rate': 1.2469634249162376e-05, 'epoch': 2.2518219450502572, 'step': 1874000}
INFO:transformers.trainer:{'loss': 2.295989261031151, 'learning_rate': 1.2459620811128535e-05, 'epoch': 2.252422751332288, 'step': 1874500}
INFO:transformers.trainer:{'loss': 2.2710792452096937, 'learning_rate': 1.2449607373094694e-05, 'epoch': 2.2530235576143185, 'step': 1875000}
INFO:transformers.trainer:{'loss': 2.3088588683605193, 'learning_rate': 1.2439593935060853e-05, 'epoch': 2.253624363896349, 'step': 1875500}
INFO:transformers.trainer:{'loss': 2.280132112503052, 'learning_rate': 1.242958049702701e-05, 'epoch': 2.2542251701783793, 'step': 1876000}
INFO:transformers.trainer:{'loss': 2.265618485689163, 'learning_rate': 1.241956705899317e-05, 'epoch': 2.2548259764604097, 'step': 1876500}
INFO:transformers.trainer:{'loss': 2.304735514879227, 'learning_rate': 1.2409553620959329e-05, 'epoch': 2.2554267827424406, 'step': 1877000}
INFO:transformers.trainer:{'loss': 2.2549685335755347, 'learning_rate': 1.2399540182925488e-05, 'epoch': 2.256027589024471, 'step': 1877500}
INFO:transformers.trainer:{'loss': 2.260427523136139, 'learning_rate': 1.2389526744891646e-05, 'epoch': 2.2566283953065014, 'step': 1878000}
INFO:transformers.trainer:{'loss': 2.2623231698274613, 'learning_rate': 1.2379513306857803e-05, 'epoch': 2.257229201588532, 'step': 1878500}
INFO:transformers.trainer:{'loss': 2.34521979701519, 'learning_rate': 1.2369499868823962e-05, 'epoch': 2.257830007870562, 'step': 1879000}
INFO:transformers.trainer:{'loss': 2.2498873361349108, 'learning_rate': 1.2359486430790121e-05, 'epoch': 2.2584308141525926, 'step': 1879500}
INFO:transformers.trainer:{'loss': 2.3029349193572997, 'learning_rate': 1.234947299275628e-05, 'epoch': 2.259031620434623, 'step': 1880000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1880000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1880000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1880000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1860000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2972274892926214, 'learning_rate': 1.2339459554722438e-05, 'epoch': 2.259632426716654, 'step': 1880500}
INFO:transformers.trainer:{'loss': 2.2952225986719133, 'learning_rate': 1.2329446116688595e-05, 'epoch': 2.2602332329986843, 'step': 1881000}
INFO:transformers.trainer:{'loss': 2.283871238589287, 'learning_rate': 1.2319432678654754e-05, 'epoch': 2.2608340392807147, 'step': 1881500}
INFO:transformers.trainer:{'loss': 2.293405029416084, 'learning_rate': 1.2309419240620913e-05, 'epoch': 2.261434845562745, 'step': 1882000}
INFO:transformers.trainer:{'loss': 2.317509983539581, 'learning_rate': 1.2299405802587072e-05, 'epoch': 2.2620356518447755, 'step': 1882500}
INFO:transformers.trainer:{'loss': 2.2615865044593813, 'learning_rate': 1.2289392364553232e-05, 'epoch': 2.2626364581268064, 'step': 1883000}
INFO:transformers.trainer:{'loss': 2.296975427865982, 'learning_rate': 1.227937892651939e-05, 'epoch': 2.2632372644088368, 'step': 1883500}
INFO:transformers.trainer:{'loss': 2.255572278380394, 'learning_rate': 1.2269365488485548e-05, 'epoch': 2.263838070690867, 'step': 1884000}
INFO:transformers.trainer:{'loss': 2.2776314000487328, 'learning_rate': 1.2259352050451707e-05, 'epoch': 2.2644388769728976, 'step': 1884500}
INFO:transformers.trainer:{'loss': 2.2438795092105868, 'learning_rate': 1.2249338612417866e-05, 'epoch': 2.265039683254928, 'step': 1885000}
INFO:transformers.trainer:{'loss': 2.2952782099246978, 'learning_rate': 1.2239325174384024e-05, 'epoch': 2.265640489536959, 'step': 1885500}
INFO:transformers.trainer:{'loss': 2.2992013095617296, 'learning_rate': 1.2229311736350183e-05, 'epoch': 2.2662412958189893, 'step': 1886000}
INFO:transformers.trainer:{'loss': 2.2718988960385325, 'learning_rate': 1.221929829831634e-05, 'epoch': 2.2668421021010197, 'step': 1886500}
INFO:transformers.trainer:{'loss': 2.295595056414604, 'learning_rate': 1.2209284860282499e-05, 'epoch': 2.26744290838305, 'step': 1887000}
INFO:transformers.trainer:{'loss': 2.2860119100809095, 'learning_rate': 1.2199271422248658e-05, 'epoch': 2.2680437146650805, 'step': 1887500}
INFO:transformers.trainer:{'loss': 2.3430318698883057, 'learning_rate': 1.2189257984214816e-05, 'epoch': 2.268644520947111, 'step': 1888000}
INFO:transformers.trainer:{'loss': 2.343883767426014, 'learning_rate': 1.2179244546180975e-05, 'epoch': 2.2692453272291413, 'step': 1888500}
INFO:transformers.trainer:{'loss': 2.2507330790162086, 'learning_rate': 1.2169231108147134e-05, 'epoch': 2.269846133511172, 'step': 1889000}
INFO:transformers.trainer:{'loss': 2.286953646540642, 'learning_rate': 1.2159217670113293e-05, 'epoch': 2.2704469397932026, 'step': 1889500}
INFO:transformers.trainer:{'loss': 2.2288768010139464, 'learning_rate': 1.2149204232079451e-05, 'epoch': 2.271047746075233, 'step': 1890000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1890000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1890000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1890000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1870000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2791218156814574, 'learning_rate': 1.213919079404561e-05, 'epoch': 2.2716485523572634, 'step': 1890500}
INFO:transformers.trainer:{'loss': 2.2409941779375075, 'learning_rate': 1.2129177356011769e-05, 'epoch': 2.272249358639294, 'step': 1891000}
INFO:transformers.trainer:{'loss': 2.272222054243088, 'learning_rate': 1.2119163917977928e-05, 'epoch': 2.2728501649213246, 'step': 1891500}
INFO:transformers.trainer:{'loss': 2.2347866463661195, 'learning_rate': 1.2109150479944085e-05, 'epoch': 2.273450971203355, 'step': 1892000}
INFO:transformers.trainer:{'loss': 2.223309988617897, 'learning_rate': 1.2099137041910244e-05, 'epoch': 2.2740517774853855, 'step': 1892500}
INFO:transformers.trainer:{'loss': 2.2643575018048288, 'learning_rate': 1.2089123603876402e-05, 'epoch': 2.274652583767416, 'step': 1893000}
INFO:transformers.trainer:{'loss': 2.2780820744037626, 'learning_rate': 1.2079110165842561e-05, 'epoch': 2.2752533900494463, 'step': 1893500}
INFO:transformers.trainer:{'loss': 2.2722044892311097, 'learning_rate': 1.206909672780872e-05, 'epoch': 2.2758541963314767, 'step': 1894000}
INFO:transformers.trainer:{'loss': 2.285524942278862, 'learning_rate': 1.2059083289774879e-05, 'epoch': 2.276455002613507, 'step': 1894500}
INFO:transformers.trainer:{'loss': 2.242065904378891, 'learning_rate': 1.2049069851741036e-05, 'epoch': 2.277055808895538, 'step': 1895000}
INFO:transformers.trainer:{'loss': 2.274563602566719, 'learning_rate': 1.2039056413707196e-05, 'epoch': 2.2776566151775683, 'step': 1895500}
INFO:transformers.trainer:{'loss': 2.301027981758118, 'learning_rate': 1.2029042975673355e-05, 'epoch': 2.2782574214595988, 'step': 1896000}
INFO:transformers.trainer:{'loss': 2.2944106245040894, 'learning_rate': 1.2019029537639514e-05, 'epoch': 2.278858227741629, 'step': 1896500}
INFO:transformers.trainer:{'loss': 2.2373221098184586, 'learning_rate': 1.2009016099605672e-05, 'epoch': 2.2794590340236596, 'step': 1897000}
INFO:transformers.trainer:{'loss': 2.2593690121769905, 'learning_rate': 1.199900266157183e-05, 'epoch': 2.2800598403056904, 'step': 1897500}
INFO:transformers.trainer:{'loss': 2.2689796184301376, 'learning_rate': 1.1988989223537988e-05, 'epoch': 2.280660646587721, 'step': 1898000}
INFO:transformers.trainer:{'loss': 2.330709796786308, 'learning_rate': 1.1978975785504147e-05, 'epoch': 2.2812614528697512, 'step': 1898500}
INFO:transformers.trainer:{'loss': 2.2801560623645782, 'learning_rate': 1.1968962347470306e-05, 'epoch': 2.2818622591517816, 'step': 1899000}
INFO:transformers.trainer:{'loss': 2.3009828413724898, 'learning_rate': 1.1958948909436464e-05, 'epoch': 2.282463065433812, 'step': 1899500}
INFO:transformers.trainer:{'loss': 2.2207469408512117, 'learning_rate': 1.1948935471402623e-05, 'epoch': 2.283063871715843, 'step': 1900000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1900000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1900000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1900000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1880000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.279454980373383, 'learning_rate': 1.193892203336878e-05, 'epoch': 2.2836646779978733, 'step': 1900500}
INFO:transformers.trainer:{'loss': 2.1946999057531356, 'learning_rate': 1.1928908595334939e-05, 'epoch': 2.2842654842799037, 'step': 1901000}
INFO:transformers.trainer:{'loss': 2.2896857731342317, 'learning_rate': 1.19188951573011e-05, 'epoch': 2.284866290561934, 'step': 1901500}
INFO:transformers.trainer:{'loss': 2.265535211086273, 'learning_rate': 1.1908881719267258e-05, 'epoch': 2.2854670968439645, 'step': 1902000}
INFO:transformers.trainer:{'loss': 2.2914634996652605, 'learning_rate': 1.1898868281233417e-05, 'epoch': 2.286067903125995, 'step': 1902500}
INFO:transformers.trainer:{'loss': 2.2095771288871764, 'learning_rate': 1.1888854843199574e-05, 'epoch': 2.2866687094080254, 'step': 1903000}
INFO:transformers.trainer:{'loss': 2.2858340871334075, 'learning_rate': 1.1878841405165733e-05, 'epoch': 2.287269515690056, 'step': 1903500}
INFO:transformers.trainer:{'loss': 2.2925244669914244, 'learning_rate': 1.1868827967131892e-05, 'epoch': 2.2878703219720866, 'step': 1904000}
INFO:transformers.trainer:{'loss': 2.2969241324067116, 'learning_rate': 1.185881452909805e-05, 'epoch': 2.288471128254117, 'step': 1904500}
INFO:transformers.trainer:{'loss': 2.3514477523565294, 'learning_rate': 1.1848801091064209e-05, 'epoch': 2.2890719345361474, 'step': 1905000}
INFO:transformers.trainer:{'loss': 2.262852793455124, 'learning_rate': 1.1838787653030368e-05, 'epoch': 2.289672740818178, 'step': 1905500}
INFO:transformers.trainer:{'loss': 2.2922881002426148, 'learning_rate': 1.1828774214996525e-05, 'epoch': 2.2902735471002087, 'step': 1906000}
INFO:transformers.trainer:{'loss': 2.236353454709053, 'learning_rate': 1.1818760776962684e-05, 'epoch': 2.290874353382239, 'step': 1906500}
INFO:transformers.trainer:{'loss': 2.2482083567380906, 'learning_rate': 1.1808747338928842e-05, 'epoch': 2.2914751596642695, 'step': 1907000}
INFO:transformers.trainer:{'loss': 2.2582330256700516, 'learning_rate': 1.1798733900895001e-05, 'epoch': 2.2920759659463, 'step': 1907500}
INFO:transformers.trainer:{'loss': 2.3128216737508773, 'learning_rate': 1.1788720462861162e-05, 'epoch': 2.2926767722283303, 'step': 1908000}
INFO:transformers.trainer:{'loss': 2.28609362244606, 'learning_rate': 1.1778707024827319e-05, 'epoch': 2.2932775785103607, 'step': 1908500}
INFO:transformers.trainer:{'loss': 2.322661777496338, 'learning_rate': 1.1768693586793477e-05, 'epoch': 2.293878384792391, 'step': 1909000}
INFO:transformers.trainer:{'loss': 2.31247166967392, 'learning_rate': 1.1758680148759636e-05, 'epoch': 2.294479191074422, 'step': 1909500}
INFO:transformers.trainer:{'loss': 2.2282735775709153, 'learning_rate': 1.1748666710725795e-05, 'epoch': 2.2950799973564524, 'step': 1910000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1910000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1910000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1910000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1890000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.274462370157242, 'learning_rate': 1.1738653272691954e-05, 'epoch': 2.295680803638483, 'step': 1910500}
INFO:transformers.trainer:{'loss': 2.263282332062721, 'learning_rate': 1.172863983465811e-05, 'epoch': 2.2962816099205132, 'step': 1911000}
INFO:transformers.trainer:{'loss': 2.298145296573639, 'learning_rate': 1.171862639662427e-05, 'epoch': 2.2968824162025436, 'step': 1911500}
INFO:transformers.trainer:{'loss': 2.2752694219350813, 'learning_rate': 1.1708612958590428e-05, 'epoch': 2.2974832224845745, 'step': 1912000}
INFO:transformers.trainer:{'loss': 2.291781739473343, 'learning_rate': 1.1698599520556587e-05, 'epoch': 2.298084028766605, 'step': 1912500}
INFO:transformers.trainer:{'loss': 2.2028035039901734, 'learning_rate': 1.1688586082522746e-05, 'epoch': 2.2986848350486353, 'step': 1913000}
INFO:transformers.trainer:{'loss': 2.2831354639530184, 'learning_rate': 1.1678572644488905e-05, 'epoch': 2.2992856413306657, 'step': 1913500}
INFO:transformers.trainer:{'loss': 2.2721386392116547, 'learning_rate': 1.1668559206455063e-05, 'epoch': 2.299886447612696, 'step': 1914000}
INFO:transformers.trainer:{'loss': 2.28817911362648, 'learning_rate': 1.1658545768421222e-05, 'epoch': 2.300487253894727, 'step': 1914500}
INFO:transformers.trainer:{'loss': 2.290984948992729, 'learning_rate': 1.1648532330387381e-05, 'epoch': 2.3010880601767574, 'step': 1915000}
INFO:transformers.trainer:{'loss': 2.275562122344971, 'learning_rate': 1.163851889235354e-05, 'epoch': 2.301688866458788, 'step': 1915500}
INFO:transformers.trainer:{'loss': 2.294392814397812, 'learning_rate': 1.1628505454319698e-05, 'epoch': 2.302289672740818, 'step': 1916000}
INFO:transformers.trainer:{'loss': 2.2480842037200928, 'learning_rate': 1.1618492016285855e-05, 'epoch': 2.3028904790228486, 'step': 1916500}
INFO:transformers.trainer:{'loss': 2.2744783924818037, 'learning_rate': 1.1608478578252014e-05, 'epoch': 2.303491285304879, 'step': 1917000}
INFO:transformers.trainer:{'loss': 2.286831817626953, 'learning_rate': 1.1598465140218173e-05, 'epoch': 2.3040920915869094, 'step': 1917500}
INFO:transformers.trainer:{'loss': 2.281996519327164, 'learning_rate': 1.1588451702184332e-05, 'epoch': 2.3046928978689403, 'step': 1918000}
INFO:transformers.trainer:{'loss': 2.222854887008667, 'learning_rate': 1.157843826415049e-05, 'epoch': 2.3052937041509707, 'step': 1918500}
INFO:transformers.trainer:{'loss': 2.2791347806453706, 'learning_rate': 1.156842482611665e-05, 'epoch': 2.305894510433001, 'step': 1919000}
INFO:transformers.trainer:{'loss': 2.256881548881531, 'learning_rate': 1.1558411388082806e-05, 'epoch': 2.3064953167150315, 'step': 1919500}
INFO:transformers.trainer:{'loss': 2.2603045276403426, 'learning_rate': 1.1548397950048965e-05, 'epoch': 2.307096122997062, 'step': 1920000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1920000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1920000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1920000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1900000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2483297530412676, 'learning_rate': 1.1538384512015126e-05, 'epoch': 2.3076969292790928, 'step': 1920500}
INFO:transformers.trainer:{'loss': 2.2808390058279038, 'learning_rate': 1.1528371073981284e-05, 'epoch': 2.308297735561123, 'step': 1921000}
INFO:transformers.trainer:{'loss': 2.264548907637596, 'learning_rate': 1.1518357635947443e-05, 'epoch': 2.3088985418431536, 'step': 1921500}
INFO:transformers.trainer:{'loss': 2.343840953230858, 'learning_rate': 1.15083441979136e-05, 'epoch': 2.309499348125184, 'step': 1922000}
INFO:transformers.trainer:{'loss': 2.303586182832718, 'learning_rate': 1.1498330759879759e-05, 'epoch': 2.3101001544072144, 'step': 1922500}
INFO:transformers.trainer:{'loss': 2.27540384221077, 'learning_rate': 1.1488317321845918e-05, 'epoch': 2.310700960689245, 'step': 1923000}
INFO:transformers.trainer:{'loss': 2.257940751791, 'learning_rate': 1.1478303883812076e-05, 'epoch': 2.311301766971275, 'step': 1923500}
INFO:transformers.trainer:{'loss': 2.2902903813123703, 'learning_rate': 1.1468290445778235e-05, 'epoch': 2.311902573253306, 'step': 1924000}
INFO:transformers.trainer:{'loss': 2.3152252476215365, 'learning_rate': 1.1458277007744394e-05, 'epoch': 2.3125033795353365, 'step': 1924500}
INFO:transformers.trainer:{'loss': 2.253192138016224, 'learning_rate': 1.1448263569710551e-05, 'epoch': 2.313104185817367, 'step': 1925000}
INFO:transformers.trainer:{'loss': 2.2551001843214036, 'learning_rate': 1.143825013167671e-05, 'epoch': 2.3137049920993973, 'step': 1925500}
INFO:transformers.trainer:{'loss': 2.258227941632271, 'learning_rate': 1.1428236693642868e-05, 'epoch': 2.3143057983814277, 'step': 1926000}
INFO:transformers.trainer:{'loss': 2.258353424072266, 'learning_rate': 1.1418223255609029e-05, 'epoch': 2.3149066046634585, 'step': 1926500}
INFO:transformers.trainer:{'loss': 2.305526412129402, 'learning_rate': 1.1408209817575188e-05, 'epoch': 2.315507410945489, 'step': 1927000}
INFO:transformers.trainer:{'loss': 2.21808332157135, 'learning_rate': 1.1398196379541345e-05, 'epoch': 2.3161082172275194, 'step': 1927500}
INFO:transformers.trainer:{'loss': 2.2158038118481636, 'learning_rate': 1.1388182941507504e-05, 'epoch': 2.3167090235095498, 'step': 1928000}
INFO:transformers.trainer:{'loss': 2.296966133236885, 'learning_rate': 1.1378169503473662e-05, 'epoch': 2.31730982979158, 'step': 1928500}
INFO:transformers.trainer:{'loss': 2.266199104428291, 'learning_rate': 1.1368156065439821e-05, 'epoch': 2.317910636073611, 'step': 1929000}
INFO:transformers.trainer:{'loss': 2.302547014713287, 'learning_rate': 1.135814262740598e-05, 'epoch': 2.3185114423556414, 'step': 1929500}
INFO:transformers.trainer:{'loss': 2.2834939566254615, 'learning_rate': 1.1348129189372139e-05, 'epoch': 2.319112248637672, 'step': 1930000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1930000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1930000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1930000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1910000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.271736204624176, 'learning_rate': 1.1338115751338296e-05, 'epoch': 2.3197130549197023, 'step': 1930500}
INFO:transformers.trainer:{'loss': 2.2501427235007285, 'learning_rate': 1.1328102313304454e-05, 'epoch': 2.3203138612017327, 'step': 1931000}
INFO:transformers.trainer:{'loss': 2.2497125418186186, 'learning_rate': 1.1318088875270613e-05, 'epoch': 2.320914667483763, 'step': 1931500}
INFO:transformers.trainer:{'loss': 2.3265942685604095, 'learning_rate': 1.1308075437236772e-05, 'epoch': 2.3215154737657935, 'step': 1932000}
INFO:transformers.trainer:{'loss': 2.258126529097557, 'learning_rate': 1.129806199920293e-05, 'epoch': 2.3221162800478243, 'step': 1932500}
INFO:transformers.trainer:{'loss': 2.2745601897239687, 'learning_rate': 1.128804856116909e-05, 'epoch': 2.3227170863298547, 'step': 1933000}
INFO:transformers.trainer:{'loss': 2.289268960237503, 'learning_rate': 1.1278035123135248e-05, 'epoch': 2.323317892611885, 'step': 1933500}
INFO:transformers.trainer:{'loss': 2.245574851691723, 'learning_rate': 1.1268021685101407e-05, 'epoch': 2.3239186988939156, 'step': 1934000}
INFO:transformers.trainer:{'loss': 2.256536257863045, 'learning_rate': 1.1258008247067566e-05, 'epoch': 2.324519505175946, 'step': 1934500}
INFO:transformers.trainer:{'loss': 2.2560394122600553, 'learning_rate': 1.1247994809033724e-05, 'epoch': 2.325120311457977, 'step': 1935000}
INFO:transformers.trainer:{'loss': 2.2627210743427275, 'learning_rate': 1.1237981370999883e-05, 'epoch': 2.3257211177400072, 'step': 1935500}
INFO:transformers.trainer:{'loss': 2.222216826558113, 'learning_rate': 1.122796793296604e-05, 'epoch': 2.3263219240220376, 'step': 1936000}
INFO:transformers.trainer:{'loss': 2.329741423726082, 'learning_rate': 1.1217954494932199e-05, 'epoch': 2.326922730304068, 'step': 1936500}
INFO:transformers.trainer:{'loss': 2.2995436767339705, 'learning_rate': 1.1207941056898358e-05, 'epoch': 2.3275235365860985, 'step': 1937000}
INFO:transformers.trainer:{'loss': 2.2620384404659273, 'learning_rate': 1.1197927618864517e-05, 'epoch': 2.328124342868129, 'step': 1937500}
INFO:transformers.trainer:{'loss': 2.313617768406868, 'learning_rate': 1.1187914180830675e-05, 'epoch': 2.3287251491501593, 'step': 1938000}
INFO:transformers.trainer:{'loss': 2.3021139714717864, 'learning_rate': 1.1177900742796834e-05, 'epoch': 2.32932595543219, 'step': 1938500}
INFO:transformers.trainer:{'loss': 2.2651436170339583, 'learning_rate': 1.1167887304762993e-05, 'epoch': 2.3299267617142205, 'step': 1939000}
INFO:transformers.trainer:{'loss': 2.243065675020218, 'learning_rate': 1.1157873866729152e-05, 'epoch': 2.330527567996251, 'step': 1939500}
INFO:transformers.trainer:{'loss': 2.233079888224602, 'learning_rate': 1.114786042869531e-05, 'epoch': 2.3311283742782813, 'step': 1940000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1940000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1940000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1940000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1920000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.3113114579916, 'learning_rate': 1.1137846990661469e-05, 'epoch': 2.3317291805603118, 'step': 1940500}
INFO:transformers.trainer:{'loss': 2.270919780254364, 'learning_rate': 1.1127833552627626e-05, 'epoch': 2.3323299868423426, 'step': 1941000}
INFO:transformers.trainer:{'loss': 2.259011355996132, 'learning_rate': 1.1117820114593785e-05, 'epoch': 2.332930793124373, 'step': 1941500}
INFO:transformers.trainer:{'loss': 2.2786985223293303, 'learning_rate': 1.1107806676559944e-05, 'epoch': 2.3335315994064034, 'step': 1942000}
INFO:transformers.trainer:{'loss': 2.241297932982445, 'learning_rate': 1.1097793238526102e-05, 'epoch': 2.334132405688434, 'step': 1942500}
INFO:transformers.trainer:{'loss': 2.2864766192436217, 'learning_rate': 1.1087779800492261e-05, 'epoch': 2.3347332119704642, 'step': 1943000}
INFO:transformers.trainer:{'loss': 2.3027801463603974, 'learning_rate': 1.107776636245842e-05, 'epoch': 2.335334018252495, 'step': 1943500}
INFO:transformers.trainer:{'loss': 2.2516766818761824, 'learning_rate': 1.1067752924424577e-05, 'epoch': 2.3359348245345255, 'step': 1944000}
INFO:transformers.trainer:{'loss': 2.2925020641088487, 'learning_rate': 1.1057739486390736e-05, 'epoch': 2.336535630816556, 'step': 1944500}
INFO:transformers.trainer:{'loss': 2.2707338688373566, 'learning_rate': 1.1047726048356896e-05, 'epoch': 2.3371364370985863, 'step': 1945000}
INFO:transformers.trainer:{'loss': 2.1826995525956154, 'learning_rate': 1.1037712610323055e-05, 'epoch': 2.3377372433806167, 'step': 1945500}
INFO:transformers.trainer:{'loss': 2.296482622385025, 'learning_rate': 1.1027699172289214e-05, 'epoch': 2.338338049662647, 'step': 1946000}
INFO:transformers.trainer:{'loss': 2.2747250763177873, 'learning_rate': 1.101768573425537e-05, 'epoch': 2.3389388559446775, 'step': 1946500}
INFO:transformers.trainer:{'loss': 2.2816377292871475, 'learning_rate': 1.100767229622153e-05, 'epoch': 2.3395396622267084, 'step': 1947000}
INFO:transformers.trainer:{'loss': 2.2660801492929457, 'learning_rate': 1.0997658858187688e-05, 'epoch': 2.340140468508739, 'step': 1947500}
INFO:transformers.trainer:{'loss': 2.245914357185364, 'learning_rate': 1.0987645420153847e-05, 'epoch': 2.340741274790769, 'step': 1948000}
INFO:transformers.trainer:{'loss': 2.2178683136105537, 'learning_rate': 1.0977631982120006e-05, 'epoch': 2.3413420810727996, 'step': 1948500}
INFO:transformers.trainer:{'loss': 2.2776123896837235, 'learning_rate': 1.0967618544086165e-05, 'epoch': 2.34194288735483, 'step': 1949000}
INFO:transformers.trainer:{'loss': 2.278289860725403, 'learning_rate': 1.0957605106052322e-05, 'epoch': 2.342543693636861, 'step': 1949500}
INFO:transformers.trainer:{'loss': 2.2652356409430503, 'learning_rate': 1.094759166801848e-05, 'epoch': 2.3431444999188913, 'step': 1950000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1950000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1950000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1950000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1930000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.244722434878349, 'learning_rate': 1.093757822998464e-05, 'epoch': 2.3437453062009217, 'step': 1950500}
INFO:transformers.trainer:{'loss': 2.2658814190626146, 'learning_rate': 1.0927564791950798e-05, 'epoch': 2.344346112482952, 'step': 1951000}
INFO:transformers.trainer:{'loss': 2.264370063185692, 'learning_rate': 1.0917551353916958e-05, 'epoch': 2.3449469187649825, 'step': 1951500}
INFO:transformers.trainer:{'loss': 2.2918797962069513, 'learning_rate': 1.0907537915883115e-05, 'epoch': 2.345547725047013, 'step': 1952000}
INFO:transformers.trainer:{'loss': 2.278115242123604, 'learning_rate': 1.0897524477849274e-05, 'epoch': 2.3461485313290433, 'step': 1952500}
INFO:transformers.trainer:{'loss': 2.236862063586712, 'learning_rate': 1.0887511039815433e-05, 'epoch': 2.346749337611074, 'step': 1953000}
INFO:transformers.trainer:{'loss': 2.19854124546051, 'learning_rate': 1.0877497601781592e-05, 'epoch': 2.3473501438931046, 'step': 1953500}
INFO:transformers.trainer:{'loss': 2.287741080522537, 'learning_rate': 1.086748416374775e-05, 'epoch': 2.347950950175135, 'step': 1954000}
INFO:transformers.trainer:{'loss': 2.257078766703606, 'learning_rate': 1.085747072571391e-05, 'epoch': 2.3485517564571654, 'step': 1954500}
INFO:transformers.trainer:{'loss': 2.2395162110328672, 'learning_rate': 1.0847457287680066e-05, 'epoch': 2.349152562739196, 'step': 1955000}
INFO:transformers.trainer:{'loss': 2.279277643203735, 'learning_rate': 1.0837443849646225e-05, 'epoch': 2.3497533690212267, 'step': 1955500}
INFO:transformers.trainer:{'loss': 2.286359690785408, 'learning_rate': 1.0827430411612384e-05, 'epoch': 2.350354175303257, 'step': 1956000}
INFO:transformers.trainer:{'loss': 2.2492818857431414, 'learning_rate': 1.0817416973578543e-05, 'epoch': 2.3509549815852875, 'step': 1956500}
INFO:transformers.trainer:{'loss': 2.2700853135585786, 'learning_rate': 1.0807403535544701e-05, 'epoch': 2.351555787867318, 'step': 1957000}
INFO:transformers.trainer:{'loss': 2.2495652700662614, 'learning_rate': 1.079739009751086e-05, 'epoch': 2.3521565941493483, 'step': 1957500}
INFO:transformers.trainer:{'loss': 2.2823450832366943, 'learning_rate': 1.0787376659477019e-05, 'epoch': 2.352757400431379, 'step': 1958000}
INFO:transformers.trainer:{'loss': 2.219411832809448, 'learning_rate': 1.0777363221443178e-05, 'epoch': 2.3533582067134096, 'step': 1958500}
INFO:transformers.trainer:{'loss': 2.265765268921852, 'learning_rate': 1.0767349783409336e-05, 'epoch': 2.35395901299544, 'step': 1959000}
INFO:transformers.trainer:{'loss': 2.2651437498927116, 'learning_rate': 1.0757336345375495e-05, 'epoch': 2.3545598192774704, 'step': 1959500}
INFO:transformers.trainer:{'loss': 2.293768233835697, 'learning_rate': 1.0747322907341654e-05, 'epoch': 2.355160625559501, 'step': 1960000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1960000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1960000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1960000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1940000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.256554997444153, 'learning_rate': 1.0737309469307811e-05, 'epoch': 2.355761431841531, 'step': 1960500}
INFO:transformers.trainer:{'loss': 2.2580981055498124, 'learning_rate': 1.072729603127397e-05, 'epoch': 2.3563622381235616, 'step': 1961000}
INFO:transformers.trainer:{'loss': 2.266145498752594, 'learning_rate': 1.0717282593240128e-05, 'epoch': 2.3569630444055925, 'step': 1961500}
INFO:transformers.trainer:{'loss': 2.2994448539614676, 'learning_rate': 1.0707269155206287e-05, 'epoch': 2.357563850687623, 'step': 1962000}
INFO:transformers.trainer:{'loss': 2.2724243218898774, 'learning_rate': 1.0697255717172446e-05, 'epoch': 2.3581646569696533, 'step': 1962500}
INFO:transformers.trainer:{'loss': 2.227667967915535, 'learning_rate': 1.0687242279138605e-05, 'epoch': 2.3587654632516837, 'step': 1963000}
INFO:transformers.trainer:{'loss': 2.303434944987297, 'learning_rate': 1.0677228841104762e-05, 'epoch': 2.359366269533714, 'step': 1963500}
INFO:transformers.trainer:{'loss': 2.259348290681839, 'learning_rate': 1.0667215403070922e-05, 'epoch': 2.359967075815745, 'step': 1964000}
INFO:transformers.trainer:{'loss': 2.1801568081378937, 'learning_rate': 1.0657201965037081e-05, 'epoch': 2.3605678820977753, 'step': 1964500}
INFO:transformers.trainer:{'loss': 2.2579928894042967, 'learning_rate': 1.064718852700324e-05, 'epoch': 2.3611686883798058, 'step': 1965000}
INFO:transformers.trainer:{'loss': 2.2362922758460044, 'learning_rate': 1.0637175088969399e-05, 'epoch': 2.361769494661836, 'step': 1965500}
INFO:transformers.trainer:{'loss': 2.2868573474884033, 'learning_rate': 1.0627161650935556e-05, 'epoch': 2.3623703009438666, 'step': 1966000}
INFO:transformers.trainer:{'loss': 2.2383762021064757, 'learning_rate': 1.0617148212901714e-05, 'epoch': 2.362971107225897, 'step': 1966500}
INFO:transformers.trainer:{'loss': 2.2922138756513597, 'learning_rate': 1.0607134774867873e-05, 'epoch': 2.3635719135079274, 'step': 1967000}
INFO:transformers.trainer:{'loss': 2.2332915723323823, 'learning_rate': 1.0597121336834032e-05, 'epoch': 2.3641727197899582, 'step': 1967500}
INFO:transformers.trainer:{'loss': 2.2485397859811784, 'learning_rate': 1.058710789880019e-05, 'epoch': 2.3647735260719887, 'step': 1968000}
INFO:transformers.trainer:{'loss': 2.233987715959549, 'learning_rate': 1.057709446076635e-05, 'epoch': 2.365374332354019, 'step': 1968500}
INFO:transformers.trainer:{'loss': 2.26747184240818, 'learning_rate': 1.0567081022732506e-05, 'epoch': 2.3659751386360495, 'step': 1969000}
INFO:transformers.trainer:{'loss': 2.248691182613373, 'learning_rate': 1.0557067584698665e-05, 'epoch': 2.36657594491808, 'step': 1969500}
INFO:transformers.trainer:{'loss': 2.3223044105768205, 'learning_rate': 1.0547054146664826e-05, 'epoch': 2.3671767512001107, 'step': 1970000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1970000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1970000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1970000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1950000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2340920530557633, 'learning_rate': 1.0537040708630984e-05, 'epoch': 2.367777557482141, 'step': 1970500}
INFO:transformers.trainer:{'loss': 2.2827080976963043, 'learning_rate': 1.0527027270597141e-05, 'epoch': 2.3683783637641715, 'step': 1971000}
INFO:transformers.trainer:{'loss': 2.2563133412599563, 'learning_rate': 1.05170138325633e-05, 'epoch': 2.368979170046202, 'step': 1971500}
INFO:transformers.trainer:{'loss': 2.2425783330202105, 'learning_rate': 1.0507000394529459e-05, 'epoch': 2.3695799763282324, 'step': 1972000}
INFO:transformers.trainer:{'loss': 2.2551020933389663, 'learning_rate': 1.0496986956495618e-05, 'epoch': 2.370180782610263, 'step': 1972500}
INFO:transformers.trainer:{'loss': 2.2286824505329133, 'learning_rate': 1.0486973518461777e-05, 'epoch': 2.3707815888922936, 'step': 1973000}
INFO:transformers.trainer:{'loss': 2.210668128490448, 'learning_rate': 1.0476960080427935e-05, 'epoch': 2.371382395174324, 'step': 1973500}
INFO:transformers.trainer:{'loss': 2.242648693501949, 'learning_rate': 1.0466946642394092e-05, 'epoch': 2.3719832014563544, 'step': 1974000}
INFO:transformers.trainer:{'loss': 2.2819043300151827, 'learning_rate': 1.0456933204360251e-05, 'epoch': 2.372584007738385, 'step': 1974500}
INFO:transformers.trainer:{'loss': 2.209430188179016, 'learning_rate': 1.044691976632641e-05, 'epoch': 2.3731848140204153, 'step': 1975000}
INFO:transformers.trainer:{'loss': 2.287604828119278, 'learning_rate': 1.0436906328292569e-05, 'epoch': 2.3737856203024457, 'step': 1975500}
INFO:transformers.trainer:{'loss': 2.2473964252471923, 'learning_rate': 1.0426892890258727e-05, 'epoch': 2.3743864265844765, 'step': 1976000}
INFO:transformers.trainer:{'loss': 2.3111894211769104, 'learning_rate': 1.0416879452224886e-05, 'epoch': 2.374987232866507, 'step': 1976500}
INFO:transformers.trainer:{'loss': 2.3207807735204695, 'learning_rate': 1.0406866014191045e-05, 'epoch': 2.3755880391485373, 'step': 1977000}
INFO:transformers.trainer:{'loss': 2.2759551503658293, 'learning_rate': 1.0396852576157204e-05, 'epoch': 2.3761888454305677, 'step': 1977500}
INFO:transformers.trainer:{'loss': 2.2346333569288253, 'learning_rate': 1.0386839138123362e-05, 'epoch': 2.376789651712598, 'step': 1978000}
INFO:transformers.trainer:{'loss': 2.2747321888208387, 'learning_rate': 1.0376825700089521e-05, 'epoch': 2.377390457994629, 'step': 1978500}
INFO:transformers.trainer:{'loss': 2.2564960125684737, 'learning_rate': 1.036681226205568e-05, 'epoch': 2.3779912642766594, 'step': 1979000}
INFO:transformers.trainer:{'loss': 2.2496256822347642, 'learning_rate': 1.0356798824021837e-05, 'epoch': 2.37859207055869, 'step': 1979500}
INFO:transformers.trainer:{'loss': 2.215903891682625, 'learning_rate': 1.0346785385987996e-05, 'epoch': 2.3791928768407202, 'step': 1980000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1980000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1980000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1980000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1960000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2677708211541177, 'learning_rate': 1.0336771947954155e-05, 'epoch': 2.3797936831227506, 'step': 1980500}
INFO:transformers.trainer:{'loss': 2.2380755574703217, 'learning_rate': 1.0326758509920313e-05, 'epoch': 2.380394489404781, 'step': 1981000}
INFO:transformers.trainer:{'loss': 2.277280470371246, 'learning_rate': 1.0316745071886472e-05, 'epoch': 2.3809952956868115, 'step': 1981500}
INFO:transformers.trainer:{'loss': 2.2977186764478685, 'learning_rate': 1.030673163385263e-05, 'epoch': 2.3815961019688423, 'step': 1982000}
INFO:transformers.trainer:{'loss': 2.2875653522014616, 'learning_rate': 1.029671819581879e-05, 'epoch': 2.3821969082508727, 'step': 1982500}
INFO:transformers.trainer:{'loss': 2.23431008541584, 'learning_rate': 1.0286704757784948e-05, 'epoch': 2.382797714532903, 'step': 1983000}
INFO:transformers.trainer:{'loss': 2.2757960938215254, 'learning_rate': 1.0276691319751107e-05, 'epoch': 2.3833985208149335, 'step': 1983500}
INFO:transformers.trainer:{'loss': 2.2547190252542495, 'learning_rate': 1.0266677881717266e-05, 'epoch': 2.383999327096964, 'step': 1984000}
INFO:transformers.trainer:{'loss': 2.2474321994781494, 'learning_rate': 1.0256664443683425e-05, 'epoch': 2.384600133378995, 'step': 1984500}
INFO:transformers.trainer:{'loss': 2.273945963680744, 'learning_rate': 1.0246651005649582e-05, 'epoch': 2.385200939661025, 'step': 1985000}
INFO:transformers.trainer:{'loss': 2.271052896142006, 'learning_rate': 1.023663756761574e-05, 'epoch': 2.3858017459430556, 'step': 1985500}
INFO:transformers.trainer:{'loss': 2.267313479781151, 'learning_rate': 1.02266241295819e-05, 'epoch': 2.386402552225086, 'step': 1986000}
INFO:transformers.trainer:{'loss': 2.249669569134712, 'learning_rate': 1.0216610691548058e-05, 'epoch': 2.3870033585071164, 'step': 1986500}
INFO:transformers.trainer:{'loss': 2.287347798705101, 'learning_rate': 1.0206597253514217e-05, 'epoch': 2.3876041647891473, 'step': 1987000}
INFO:transformers.trainer:{'loss': 2.2998591524362566, 'learning_rate': 1.0196583815480375e-05, 'epoch': 2.3882049710711777, 'step': 1987500}
INFO:transformers.trainer:{'loss': 2.299866985797882, 'learning_rate': 1.0186570377446533e-05, 'epoch': 2.388805777353208, 'step': 1988000}
INFO:transformers.trainer:{'loss': 2.2351437602043154, 'learning_rate': 1.0176556939412691e-05, 'epoch': 2.3894065836352385, 'step': 1988500}
INFO:transformers.trainer:{'loss': 2.198031491518021, 'learning_rate': 1.0166543501378852e-05, 'epoch': 2.390007389917269, 'step': 1989000}
INFO:transformers.trainer:{'loss': 2.284784809947014, 'learning_rate': 1.015653006334501e-05, 'epoch': 2.3906081961992993, 'step': 1989500}
INFO:transformers.trainer:{'loss': 2.2169776087403297, 'learning_rate': 1.014651662531117e-05, 'epoch': 2.3912090024813297, 'step': 1990000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-1990000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-1990000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-1990000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1970000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2695946825742723, 'learning_rate': 1.0136503187277326e-05, 'epoch': 2.3918098087633606, 'step': 1990500}
INFO:transformers.trainer:{'loss': 2.2547408748865125, 'learning_rate': 1.0126489749243485e-05, 'epoch': 2.392410615045391, 'step': 1991000}
INFO:transformers.trainer:{'loss': 2.272816536664963, 'learning_rate': 1.0116476311209644e-05, 'epoch': 2.3930114213274214, 'step': 1991500}
INFO:transformers.trainer:{'loss': 2.2396906032562254, 'learning_rate': 1.0106462873175803e-05, 'epoch': 2.393612227609452, 'step': 1992000}
INFO:transformers.trainer:{'loss': 2.214171347618103, 'learning_rate': 1.0096449435141961e-05, 'epoch': 2.394213033891482, 'step': 1992500}
INFO:transformers.trainer:{'loss': 2.1889736728072164, 'learning_rate': 1.008643599710812e-05, 'epoch': 2.394813840173513, 'step': 1993000}
INFO:transformers.trainer:{'loss': 2.2751551011800766, 'learning_rate': 1.0076422559074277e-05, 'epoch': 2.3954146464555435, 'step': 1993500}
INFO:transformers.trainer:{'loss': 2.264059780240059, 'learning_rate': 1.0066409121040436e-05, 'epoch': 2.396015452737574, 'step': 1994000}
INFO:transformers.trainer:{'loss': 2.2530929856300355, 'learning_rate': 1.0056395683006595e-05, 'epoch': 2.3966162590196043, 'step': 1994500}
INFO:transformers.trainer:{'loss': 2.290185282588005, 'learning_rate': 1.0046382244972755e-05, 'epoch': 2.3972170653016347, 'step': 1995000}
INFO:transformers.trainer:{'loss': 2.243757483482361, 'learning_rate': 1.0036368806938914e-05, 'epoch': 2.397817871583665, 'step': 1995500}
INFO:transformers.trainer:{'loss': 2.241143226504326, 'learning_rate': 1.0026355368905071e-05, 'epoch': 2.3984186778656955, 'step': 1996000}
INFO:transformers.trainer:{'loss': 2.234877960383892, 'learning_rate': 1.001634193087123e-05, 'epoch': 2.3990194841477264, 'step': 1996500}
INFO:transformers.trainer:{'loss': 2.1996257119178773, 'learning_rate': 1.0006328492837388e-05, 'epoch': 2.3996202904297568, 'step': 1997000}
INFO:transformers.trainer:{'loss': 2.235218482017517, 'learning_rate': 9.996315054803547e-06, 'epoch': 2.400221096711787, 'step': 1997500}
INFO:transformers.trainer:{'loss': 2.249495495080948, 'learning_rate': 9.986301616769706e-06, 'epoch': 2.4008219029938176, 'step': 1998000}
INFO:transformers.trainer:{'loss': 2.252802022099495, 'learning_rate': 9.976288178735865e-06, 'epoch': 2.401422709275848, 'step': 1998500}
INFO:transformers.trainer:{'loss': 2.2877779141664507, 'learning_rate': 9.966274740702022e-06, 'epoch': 2.402023515557879, 'step': 1999000}
INFO:transformers.trainer:{'loss': 2.233268732666969, 'learning_rate': 9.95626130266818e-06, 'epoch': 2.4026243218399093, 'step': 1999500}
INFO:transformers.trainer:{'loss': 2.294723965167999, 'learning_rate': 9.94624786463434e-06, 'epoch': 2.4032251281219397, 'step': 2000000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2000000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2000000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2000000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1980000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.24879030919075, 'learning_rate': 9.936234426600498e-06, 'epoch': 2.40382593440397, 'step': 2000500}
INFO:transformers.trainer:{'loss': 2.2787380385398865, 'learning_rate': 9.926220988566657e-06, 'epoch': 2.4044267406860005, 'step': 2001000}
INFO:transformers.trainer:{'loss': 2.3141591178178786, 'learning_rate': 9.916207550532816e-06, 'epoch': 2.4050275469680313, 'step': 2001500}
INFO:transformers.trainer:{'loss': 2.2725834243297576, 'learning_rate': 9.906194112498974e-06, 'epoch': 2.4056283532500617, 'step': 2002000}
INFO:transformers.trainer:{'loss': 2.3234322101473808, 'learning_rate': 9.896180674465133e-06, 'epoch': 2.406229159532092, 'step': 2002500}
INFO:transformers.trainer:{'loss': 2.2932799882888792, 'learning_rate': 9.886167236431292e-06, 'epoch': 2.4068299658141226, 'step': 2003000}
INFO:transformers.trainer:{'loss': 2.2523966348171234, 'learning_rate': 9.87615379839745e-06, 'epoch': 2.407430772096153, 'step': 2003500}
INFO:transformers.trainer:{'loss': 2.236821226775646, 'learning_rate': 9.866140360363608e-06, 'epoch': 2.4080315783781834, 'step': 2004000}
INFO:transformers.trainer:{'loss': 2.284164857983589, 'learning_rate': 9.856126922329766e-06, 'epoch': 2.408632384660214, 'step': 2004500}
INFO:transformers.trainer:{'loss': 2.2336496910452843, 'learning_rate': 9.846113484295925e-06, 'epoch': 2.4092331909422446, 'step': 2005000}
INFO:transformers.trainer:{'loss': 2.268364994287491, 'learning_rate': 9.836100046262084e-06, 'epoch': 2.409833997224275, 'step': 2005500}
INFO:transformers.trainer:{'loss': 2.2936727343797685, 'learning_rate': 9.826086608228243e-06, 'epoch': 2.4104348035063055, 'step': 2006000}
INFO:transformers.trainer:{'loss': 2.2503339805603026, 'learning_rate': 9.816073170194401e-06, 'epoch': 2.411035609788336, 'step': 2006500}
INFO:transformers.trainer:{'loss': 2.244860903263092, 'learning_rate': 9.806059732160559e-06, 'epoch': 2.4116364160703663, 'step': 2007000}
INFO:transformers.trainer:{'loss': 2.2087810035943987, 'learning_rate': 9.796046294126719e-06, 'epoch': 2.412237222352397, 'step': 2007500}
INFO:transformers.trainer:{'loss': 2.20130668759346, 'learning_rate': 9.786032856092878e-06, 'epoch': 2.4128380286344275, 'step': 2008000}
INFO:transformers.trainer:{'loss': 2.259184156060219, 'learning_rate': 9.776019418059037e-06, 'epoch': 2.413438834916458, 'step': 2008500}
INFO:transformers.trainer:{'loss': 2.2626336430311205, 'learning_rate': 9.766005980025195e-06, 'epoch': 2.4140396411984884, 'step': 2009000}
INFO:transformers.trainer:{'loss': 2.247177769064903, 'learning_rate': 9.755992541991352e-06, 'epoch': 2.4146404474805188, 'step': 2009500}
INFO:transformers.trainer:{'loss': 2.2328929438591003, 'learning_rate': 9.745979103957511e-06, 'epoch': 2.415241253762549, 'step': 2010000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2010000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2010000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2010000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-1990000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2932492212057114, 'learning_rate': 9.73596566592367e-06, 'epoch': 2.41584206004458, 'step': 2010500}
INFO:transformers.trainer:{'loss': 2.265226518392563, 'learning_rate': 9.725952227889829e-06, 'epoch': 2.4164428663266104, 'step': 2011000}
INFO:transformers.trainer:{'loss': 2.2202020314931867, 'learning_rate': 9.715938789855987e-06, 'epoch': 2.417043672608641, 'step': 2011500}
INFO:transformers.trainer:{'loss': 2.2412692929506304, 'learning_rate': 9.705925351822146e-06, 'epoch': 2.4176444788906712, 'step': 2012000}
INFO:transformers.trainer:{'loss': 2.211137045502663, 'learning_rate': 9.695911913788303e-06, 'epoch': 2.4182452851727017, 'step': 2012500}
INFO:transformers.trainer:{'loss': 2.277345316231251, 'learning_rate': 9.685898475754462e-06, 'epoch': 2.418846091454732, 'step': 2013000}
INFO:transformers.trainer:{'loss': 2.2705675551891327, 'learning_rate': 9.67588503772062e-06, 'epoch': 2.419446897736763, 'step': 2013500}
INFO:transformers.trainer:{'loss': 2.270149718761444, 'learning_rate': 9.665871599686781e-06, 'epoch': 2.4200477040187933, 'step': 2014000}
INFO:transformers.trainer:{'loss': 2.2351331053972245, 'learning_rate': 9.65585816165294e-06, 'epoch': 2.4206485103008237, 'step': 2014500}
INFO:transformers.trainer:{'loss': 2.272646253705025, 'learning_rate': 9.645844723619097e-06, 'epoch': 2.421249316582854, 'step': 2015000}
INFO:transformers.trainer:{'loss': 2.2209992225170136, 'learning_rate': 9.635831285585256e-06, 'epoch': 2.4218501228648845, 'step': 2015500}
INFO:transformers.trainer:{'loss': 2.2165077050924302, 'learning_rate': 9.625817847551414e-06, 'epoch': 2.4224509291469154, 'step': 2016000}
INFO:transformers.trainer:{'loss': 2.2043843994140624, 'learning_rate': 9.615804409517573e-06, 'epoch': 2.423051735428946, 'step': 2016500}
INFO:transformers.trainer:{'loss': 2.237872909784317, 'learning_rate': 9.605790971483732e-06, 'epoch': 2.423652541710976, 'step': 2017000}
INFO:transformers.trainer:{'loss': 2.269092410326004, 'learning_rate': 9.59577753344989e-06, 'epoch': 2.4242533479930066, 'step': 2017500}
INFO:transformers.trainer:{'loss': 2.279851742506027, 'learning_rate': 9.585764095416048e-06, 'epoch': 2.424854154275037, 'step': 2018000}
INFO:transformers.trainer:{'loss': 2.2617309423685072, 'learning_rate': 9.575750657382207e-06, 'epoch': 2.4254549605570674, 'step': 2018500}
INFO:transformers.trainer:{'loss': 2.3309585126638415, 'learning_rate': 9.565737219348365e-06, 'epoch': 2.426055766839098, 'step': 2019000}
INFO:transformers.trainer:{'loss': 2.258155346632004, 'learning_rate': 9.555723781314524e-06, 'epoch': 2.4266565731211287, 'step': 2019500}
INFO:transformers.trainer:{'loss': 2.2670360567569734, 'learning_rate': 9.545710343280685e-06, 'epoch': 2.427257379403159, 'step': 2020000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2020000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2020000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2020000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2000000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.240965126037598, 'learning_rate': 9.535696905246842e-06, 'epoch': 2.4278581856851895, 'step': 2020500}
INFO:transformers.trainer:{'loss': 2.274588902235031, 'learning_rate': 9.525683467213e-06, 'epoch': 2.42845899196722, 'step': 2021000}
INFO:transformers.trainer:{'loss': 2.225658766388893, 'learning_rate': 9.515670029179159e-06, 'epoch': 2.4290597982492503, 'step': 2021500}
INFO:transformers.trainer:{'loss': 2.281976645231247, 'learning_rate': 9.505656591145318e-06, 'epoch': 2.429660604531281, 'step': 2022000}
INFO:transformers.trainer:{'loss': 2.2590546305179595, 'learning_rate': 9.495643153111477e-06, 'epoch': 2.4302614108133116, 'step': 2022500}
INFO:transformers.trainer:{'loss': 2.2576464565992356, 'learning_rate': 9.485629715077635e-06, 'epoch': 2.430862217095342, 'step': 2023000}
INFO:transformers.trainer:{'loss': 2.2376030792593955, 'learning_rate': 9.475616277043792e-06, 'epoch': 2.4314630233773724, 'step': 2023500}
INFO:transformers.trainer:{'loss': 2.197663059592247, 'learning_rate': 9.465602839009951e-06, 'epoch': 2.432063829659403, 'step': 2024000}
INFO:transformers.trainer:{'loss': 2.278241825222969, 'learning_rate': 9.45558940097611e-06, 'epoch': 2.4326646359414332, 'step': 2024500}
INFO:transformers.trainer:{'loss': 2.235926853656769, 'learning_rate': 9.445575962942269e-06, 'epoch': 2.433265442223464, 'step': 2025000}
INFO:transformers.trainer:{'loss': 2.2734916458129883, 'learning_rate': 9.435562524908428e-06, 'epoch': 2.4338662485054945, 'step': 2025500}
INFO:transformers.trainer:{'loss': 2.2275335789918898, 'learning_rate': 9.425549086874586e-06, 'epoch': 2.434467054787525, 'step': 2026000}
INFO:transformers.trainer:{'loss': 2.2741995742321013, 'learning_rate': 9.415535648840745e-06, 'epoch': 2.4350678610695553, 'step': 2026500}
INFO:transformers.trainer:{'loss': 2.220558695435524, 'learning_rate': 9.405522210806904e-06, 'epoch': 2.4356686673515857, 'step': 2027000}
INFO:transformers.trainer:{'loss': 2.262530813097954, 'learning_rate': 9.395508772773063e-06, 'epoch': 2.436269473633616, 'step': 2027500}
INFO:transformers.trainer:{'loss': 2.208757717251778, 'learning_rate': 9.385495334739221e-06, 'epoch': 2.436870279915647, 'step': 2028000}
INFO:transformers.trainer:{'loss': 2.2611779666543006, 'learning_rate': 9.37548189670538e-06, 'epoch': 2.4374710861976774, 'step': 2028500}
INFO:transformers.trainer:{'loss': 2.2050293394327163, 'learning_rate': 9.365468458671537e-06, 'epoch': 2.438071892479708, 'step': 2029000}
INFO:transformers.trainer:{'loss': 2.288196712613106, 'learning_rate': 9.355455020637696e-06, 'epoch': 2.438672698761738, 'step': 2029500}
INFO:transformers.trainer:{'loss': 2.2264668768644333, 'learning_rate': 9.345441582603855e-06, 'epoch': 2.4392735050437686, 'step': 2030000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2030000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2030000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2030000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2010000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2662125984430315, 'learning_rate': 9.335428144570013e-06, 'epoch': 2.4398743113257995, 'step': 2030500}
INFO:transformers.trainer:{'loss': 2.2842957125902177, 'learning_rate': 9.325414706536172e-06, 'epoch': 2.44047511760783, 'step': 2031000}
INFO:transformers.trainer:{'loss': 2.211159489095211, 'learning_rate': 9.315401268502331e-06, 'epoch': 2.4410759238898603, 'step': 2031500}
INFO:transformers.trainer:{'loss': 2.2644291576147078, 'learning_rate': 9.305387830468488e-06, 'epoch': 2.4416767301718907, 'step': 2032000}
INFO:transformers.trainer:{'loss': 2.26751951110363, 'learning_rate': 9.295374392434648e-06, 'epoch': 2.442277536453921, 'step': 2032500}
INFO:transformers.trainer:{'loss': 2.270108696103096, 'learning_rate': 9.285360954400807e-06, 'epoch': 2.4428783427359515, 'step': 2033000}
INFO:transformers.trainer:{'loss': 2.255176856637001, 'learning_rate': 9.275347516366966e-06, 'epoch': 2.443479149017982, 'step': 2033500}
INFO:transformers.trainer:{'loss': 2.2547161191105842, 'learning_rate': 9.265334078333123e-06, 'epoch': 2.4440799553000128, 'step': 2034000}
INFO:transformers.trainer:{'loss': 2.2151530995368955, 'learning_rate': 9.255320640299282e-06, 'epoch': 2.444680761582043, 'step': 2034500}
INFO:transformers.trainer:{'loss': 2.257932699680328, 'learning_rate': 9.24530720226544e-06, 'epoch': 2.4452815678640736, 'step': 2035000}
INFO:transformers.trainer:{'loss': 2.2101154091358186, 'learning_rate': 9.2352937642316e-06, 'epoch': 2.445882374146104, 'step': 2035500}
INFO:transformers.trainer:{'loss': 2.260826850295067, 'learning_rate': 9.225280326197758e-06, 'epoch': 2.4464831804281344, 'step': 2036000}
INFO:transformers.trainer:{'loss': 2.277354215145111, 'learning_rate': 9.215266888163917e-06, 'epoch': 2.4470839867101652, 'step': 2036500}
INFO:transformers.trainer:{'loss': 2.270966880321503, 'learning_rate': 9.205253450130074e-06, 'epoch': 2.4476847929921957, 'step': 2037000}
INFO:transformers.trainer:{'loss': 2.2457215113639832, 'learning_rate': 9.195240012096233e-06, 'epoch': 2.448285599274226, 'step': 2037500}
INFO:transformers.trainer:{'loss': 2.258740995168686, 'learning_rate': 9.185226574062391e-06, 'epoch': 2.4488864055562565, 'step': 2038000}
INFO:transformers.trainer:{'loss': 2.2387285207509993, 'learning_rate': 9.17521313602855e-06, 'epoch': 2.449487211838287, 'step': 2038500}
INFO:transformers.trainer:{'loss': 2.204110895752907, 'learning_rate': 9.16519969799471e-06, 'epoch': 2.4500880181203173, 'step': 2039000}
INFO:transformers.trainer:{'loss': 2.2935028413534164, 'learning_rate': 9.155186259960868e-06, 'epoch': 2.450688824402348, 'step': 2039500}
INFO:transformers.trainer:{'loss': 2.2402306532859804, 'learning_rate': 9.145172821927026e-06, 'epoch': 2.4512896306843786, 'step': 2040000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2040000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2040000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2040000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2020000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.250725704550743, 'learning_rate': 9.135159383893185e-06, 'epoch': 2.451890436966409, 'step': 2040500}
INFO:transformers.trainer:{'loss': 2.224550513625145, 'learning_rate': 9.125145945859344e-06, 'epoch': 2.4524912432484394, 'step': 2041000}
INFO:transformers.trainer:{'loss': 2.165725286066532, 'learning_rate': 9.115132507825503e-06, 'epoch': 2.4530920495304698, 'step': 2041500}
INFO:transformers.trainer:{'loss': 2.233159553050995, 'learning_rate': 9.105119069791661e-06, 'epoch': 2.4536928558125, 'step': 2042000}
INFO:transformers.trainer:{'loss': 2.2292421902418136, 'learning_rate': 9.095105631757819e-06, 'epoch': 2.454293662094531, 'step': 2042500}
INFO:transformers.trainer:{'loss': 2.251572337329388, 'learning_rate': 9.085092193723977e-06, 'epoch': 2.4548944683765614, 'step': 2043000}
INFO:transformers.trainer:{'loss': 2.2812954239845276, 'learning_rate': 9.075078755690136e-06, 'epoch': 2.455495274658592, 'step': 2043500}
INFO:transformers.trainer:{'loss': 2.2308562277555466, 'learning_rate': 9.065065317656295e-06, 'epoch': 2.4560960809406223, 'step': 2044000}
INFO:transformers.trainer:{'loss': 2.2467659759521483, 'learning_rate': 9.055051879622454e-06, 'epoch': 2.4566968872226527, 'step': 2044500}
INFO:transformers.trainer:{'loss': 2.1986093794107435, 'learning_rate': 9.045038441588612e-06, 'epoch': 2.4572976935046835, 'step': 2045000}
INFO:transformers.trainer:{'loss': 2.223935443162918, 'learning_rate': 9.035025003554771e-06, 'epoch': 2.457898499786714, 'step': 2045500}
INFO:transformers.trainer:{'loss': 2.186844576239586, 'learning_rate': 9.02501156552093e-06, 'epoch': 2.4584993060687443, 'step': 2046000}
INFO:transformers.trainer:{'loss': 2.2794177223443985, 'learning_rate': 9.014998127487089e-06, 'epoch': 2.4591001123507747, 'step': 2046500}
INFO:transformers.trainer:{'loss': 2.202663333773613, 'learning_rate': 9.004984689453247e-06, 'epoch': 2.459700918632805, 'step': 2047000}
INFO:transformers.trainer:{'loss': 2.231353158712387, 'learning_rate': 8.994971251419406e-06, 'epoch': 2.4603017249148356, 'step': 2047500}
INFO:transformers.trainer:{'loss': 2.2678209455013274, 'learning_rate': 8.984957813385563e-06, 'epoch': 2.460902531196866, 'step': 2048000}
INFO:transformers.trainer:{'loss': 2.2394242202043535, 'learning_rate': 8.974944375351722e-06, 'epoch': 2.461503337478897, 'step': 2048500}
INFO:transformers.trainer:{'loss': 2.27083303129673, 'learning_rate': 8.96493093731788e-06, 'epoch': 2.4621041437609272, 'step': 2049000}
INFO:transformers.trainer:{'loss': 2.2486229273080824, 'learning_rate': 8.95491749928404e-06, 'epoch': 2.4627049500429576, 'step': 2049500}
INFO:transformers.trainer:{'loss': 2.234815136730671, 'learning_rate': 8.944904061250198e-06, 'epoch': 2.463305756324988, 'step': 2050000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2050000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2050000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2050000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2030000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2189183258414267, 'learning_rate': 8.934890623216357e-06, 'epoch': 2.4639065626070185, 'step': 2050500}
INFO:transformers.trainer:{'loss': 2.2705139320492744, 'learning_rate': 8.924877185182514e-06, 'epoch': 2.4645073688890493, 'step': 2051000}
INFO:transformers.trainer:{'loss': 2.23583080971241, 'learning_rate': 8.914863747148674e-06, 'epoch': 2.4651081751710797, 'step': 2051500}
INFO:transformers.trainer:{'loss': 2.243056405425072, 'learning_rate': 8.904850309114833e-06, 'epoch': 2.46570898145311, 'step': 2052000}
INFO:transformers.trainer:{'loss': 2.254935138106346, 'learning_rate': 8.894836871080992e-06, 'epoch': 2.4663097877351405, 'step': 2052500}
INFO:transformers.trainer:{'loss': 2.1689886998534202, 'learning_rate': 8.88482343304715e-06, 'epoch': 2.466910594017171, 'step': 2053000}
INFO:transformers.trainer:{'loss': 2.2800800366401672, 'learning_rate': 8.874809995013308e-06, 'epoch': 2.4675114002992014, 'step': 2053500}
INFO:transformers.trainer:{'loss': 2.2247387417554854, 'learning_rate': 8.864796556979467e-06, 'epoch': 2.468112206581232, 'step': 2054000}
INFO:transformers.trainer:{'loss': 2.2505559710264205, 'learning_rate': 8.854783118945625e-06, 'epoch': 2.4687130128632626, 'step': 2054500}
INFO:transformers.trainer:{'loss': 2.2230124726295473, 'learning_rate': 8.844769680911784e-06, 'epoch': 2.469313819145293, 'step': 2055000}
INFO:transformers.trainer:{'loss': 2.2805441771745683, 'learning_rate': 8.834756242877943e-06, 'epoch': 2.4699146254273234, 'step': 2055500}
INFO:transformers.trainer:{'loss': 2.26260893368721, 'learning_rate': 8.824742804844102e-06, 'epoch': 2.470515431709354, 'step': 2056000}
INFO:transformers.trainer:{'loss': 2.255300394654274, 'learning_rate': 8.814729366810259e-06, 'epoch': 2.4711162379913842, 'step': 2056500}
INFO:transformers.trainer:{'loss': 2.261453562736511, 'learning_rate': 8.804715928776417e-06, 'epoch': 2.471717044273415, 'step': 2057000}
INFO:transformers.trainer:{'loss': 2.2071537911891936, 'learning_rate': 8.794702490742578e-06, 'epoch': 2.4723178505554455, 'step': 2057500}
INFO:transformers.trainer:{'loss': 2.227848662853241, 'learning_rate': 8.784689052708737e-06, 'epoch': 2.472918656837476, 'step': 2058000}
INFO:transformers.trainer:{'loss': 2.2576346417665483, 'learning_rate': 8.774675614674895e-06, 'epoch': 2.4735194631195063, 'step': 2058500}
INFO:transformers.trainer:{'loss': 2.2347759802341463, 'learning_rate': 8.764662176641052e-06, 'epoch': 2.4741202694015367, 'step': 2059000}
INFO:transformers.trainer:{'loss': 2.2771083936095238, 'learning_rate': 8.754648738607211e-06, 'epoch': 2.4747210756835676, 'step': 2059500}
INFO:transformers.trainer:{'loss': 2.2130926011800764, 'learning_rate': 8.74463530057337e-06, 'epoch': 2.475321881965598, 'step': 2060000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2060000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2060000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2060000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2040000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.283075371861458, 'learning_rate': 8.734621862539529e-06, 'epoch': 2.4759226882476284, 'step': 2060500}
INFO:transformers.trainer:{'loss': 2.28642503452301, 'learning_rate': 8.724608424505687e-06, 'epoch': 2.476523494529659, 'step': 2061000}
INFO:transformers.trainer:{'loss': 2.2133775059580802, 'learning_rate': 8.714594986471846e-06, 'epoch': 2.477124300811689, 'step': 2061500}
INFO:transformers.trainer:{'loss': 2.2456155796051025, 'learning_rate': 8.704581548438003e-06, 'epoch': 2.4777251070937196, 'step': 2062000}
INFO:transformers.trainer:{'loss': 2.2284919718503953, 'learning_rate': 8.694568110404162e-06, 'epoch': 2.47832591337575, 'step': 2062500}
INFO:transformers.trainer:{'loss': 2.2863045409917833, 'learning_rate': 8.68455467237032e-06, 'epoch': 2.478926719657781, 'step': 2063000}
INFO:transformers.trainer:{'loss': 2.2650590953826906, 'learning_rate': 8.67454123433648e-06, 'epoch': 2.4795275259398113, 'step': 2063500}
INFO:transformers.trainer:{'loss': 2.2297158448696135, 'learning_rate': 8.664527796302638e-06, 'epoch': 2.4801283322218417, 'step': 2064000}
INFO:transformers.trainer:{'loss': 2.191465578556061, 'learning_rate': 8.654514358268797e-06, 'epoch': 2.480729138503872, 'step': 2064500}
INFO:transformers.trainer:{'loss': 2.256780878305435, 'learning_rate': 8.644500920234956e-06, 'epoch': 2.4813299447859025, 'step': 2065000}
INFO:transformers.trainer:{'loss': 2.2093194888830183, 'learning_rate': 8.634487482201115e-06, 'epoch': 2.4819307510679334, 'step': 2065500}
INFO:transformers.trainer:{'loss': 2.2692008694410326, 'learning_rate': 8.624474044167273e-06, 'epoch': 2.482531557349964, 'step': 2066000}
INFO:transformers.trainer:{'loss': 2.2630158640146254, 'learning_rate': 8.614460606133432e-06, 'epoch': 2.483132363631994, 'step': 2066500}
INFO:transformers.trainer:{'loss': 2.183827139914036, 'learning_rate': 8.60444716809959e-06, 'epoch': 2.4837331699140246, 'step': 2067000}
INFO:transformers.trainer:{'loss': 2.251036947131157, 'learning_rate': 8.594433730065748e-06, 'epoch': 2.484333976196055, 'step': 2067500}
INFO:transformers.trainer:{'loss': 2.2407147451639173, 'learning_rate': 8.584420292031907e-06, 'epoch': 2.4849347824780854, 'step': 2068000}
INFO:transformers.trainer:{'loss': 2.2261414337158203, 'learning_rate': 8.574406853998065e-06, 'epoch': 2.4855355887601163, 'step': 2068500}
INFO:transformers.trainer:{'loss': 2.255112789750099, 'learning_rate': 8.564393415964224e-06, 'epoch': 2.4861363950421467, 'step': 2069000}
INFO:transformers.trainer:{'loss': 2.2165330445766447, 'learning_rate': 8.554379977930383e-06, 'epoch': 2.486737201324177, 'step': 2069500}
INFO:transformers.trainer:{'loss': 2.195860423207283, 'learning_rate': 8.544366539896542e-06, 'epoch': 2.4873380076062075, 'step': 2070000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2070000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2070000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2070000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2050000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2034643565416334, 'learning_rate': 8.5343531018627e-06, 'epoch': 2.487938813888238, 'step': 2070500}
INFO:transformers.trainer:{'loss': 2.2767407368421555, 'learning_rate': 8.52433966382886e-06, 'epoch': 2.4885396201702683, 'step': 2071000}
INFO:transformers.trainer:{'loss': 2.185426076054573, 'learning_rate': 8.514326225795018e-06, 'epoch': 2.489140426452299, 'step': 2071500}
INFO:transformers.trainer:{'loss': 2.2596090244054796, 'learning_rate': 8.504312787761177e-06, 'epoch': 2.4897412327343296, 'step': 2072000}
INFO:transformers.trainer:{'loss': 2.1947963571548463, 'learning_rate': 8.494299349727334e-06, 'epoch': 2.49034203901636, 'step': 2072500}
INFO:transformers.trainer:{'loss': 2.261865941345692, 'learning_rate': 8.484285911693493e-06, 'epoch': 2.4909428452983904, 'step': 2073000}
INFO:transformers.trainer:{'loss': 2.1822062695622444, 'learning_rate': 8.474272473659651e-06, 'epoch': 2.491543651580421, 'step': 2073500}
INFO:transformers.trainer:{'loss': 2.259320451259613, 'learning_rate': 8.46425903562581e-06, 'epoch': 2.4921444578624516, 'step': 2074000}
INFO:transformers.trainer:{'loss': 2.1822503629922867, 'learning_rate': 8.454245597591969e-06, 'epoch': 2.492745264144482, 'step': 2074500}
INFO:transformers.trainer:{'loss': 2.2444796073436737, 'learning_rate': 8.444232159558128e-06, 'epoch': 2.4933460704265125, 'step': 2075000}
INFO:transformers.trainer:{'loss': 2.207717123866081, 'learning_rate': 8.434218721524285e-06, 'epoch': 2.493946876708543, 'step': 2075500}
INFO:transformers.trainer:{'loss': 2.2710408926010133, 'learning_rate': 8.424205283490443e-06, 'epoch': 2.4945476829905733, 'step': 2076000}
INFO:transformers.trainer:{'loss': 2.228050659418106, 'learning_rate': 8.414191845456604e-06, 'epoch': 2.4951484892726037, 'step': 2076500}
INFO:transformers.trainer:{'loss': 2.2175193169116976, 'learning_rate': 8.404178407422763e-06, 'epoch': 2.495749295554634, 'step': 2077000}
INFO:transformers.trainer:{'loss': 2.223985658288002, 'learning_rate': 8.394164969388921e-06, 'epoch': 2.496350101836665, 'step': 2077500}
INFO:transformers.trainer:{'loss': 2.193793881893158, 'learning_rate': 8.384151531355079e-06, 'epoch': 2.4969509081186954, 'step': 2078000}
INFO:transformers.trainer:{'loss': 2.2920034910440443, 'learning_rate': 8.374138093321237e-06, 'epoch': 2.4975517144007258, 'step': 2078500}
INFO:transformers.trainer:{'loss': 2.2288072543144226, 'learning_rate': 8.364124655287396e-06, 'epoch': 2.498152520682756, 'step': 2079000}
INFO:transformers.trainer:{'loss': 2.269003144145012, 'learning_rate': 8.354111217253555e-06, 'epoch': 2.4987533269647866, 'step': 2079500}
INFO:transformers.trainer:{'loss': 2.298215615272522, 'learning_rate': 8.344097779219714e-06, 'epoch': 2.4993541332468174, 'step': 2080000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2080000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2080000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2080000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2060000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2310357224941253, 'learning_rate': 8.334084341185872e-06, 'epoch': 2.499954939528848, 'step': 2080500}
INFO:transformers.trainer:{'loss': 2.2369383742809297, 'learning_rate': 8.32407090315203e-06, 'epoch': 2.5005557458108782, 'step': 2081000}
INFO:transformers.trainer:{'loss': 2.245728136301041, 'learning_rate': 8.314057465118188e-06, 'epoch': 2.5011565520929087, 'step': 2081500}
INFO:transformers.trainer:{'loss': 2.234485138654709, 'learning_rate': 8.304044027084347e-06, 'epoch': 2.501757358374939, 'step': 2082000}
INFO:transformers.trainer:{'loss': 2.2032776268720626, 'learning_rate': 8.294030589050507e-06, 'epoch': 2.50235816465697, 'step': 2082500}
INFO:transformers.trainer:{'loss': 2.290257947564125, 'learning_rate': 8.284017151016666e-06, 'epoch': 2.502958970939, 'step': 2083000}
INFO:transformers.trainer:{'loss': 2.20846610891819, 'learning_rate': 8.274003712982823e-06, 'epoch': 2.5035597772210307, 'step': 2083500}
INFO:transformers.trainer:{'loss': 2.261832827925682, 'learning_rate': 8.263990274948982e-06, 'epoch': 2.504160583503061, 'step': 2084000}
INFO:transformers.trainer:{'loss': 2.2411596685647965, 'learning_rate': 8.25397683691514e-06, 'epoch': 2.5047613897850916, 'step': 2084500}
INFO:transformers.trainer:{'loss': 2.20642148065567, 'learning_rate': 8.2439633988813e-06, 'epoch': 2.505362196067122, 'step': 2085000}
INFO:transformers.trainer:{'loss': 2.304607683420181, 'learning_rate': 8.233949960847458e-06, 'epoch': 2.5059630023491524, 'step': 2085500}
INFO:transformers.trainer:{'loss': 2.231212212562561, 'learning_rate': 8.223936522813617e-06, 'epoch': 2.506563808631183, 'step': 2086000}
INFO:transformers.trainer:{'loss': 2.2624421050548555, 'learning_rate': 8.213923084779774e-06, 'epoch': 2.5071646149132136, 'step': 2086500}
INFO:transformers.trainer:{'loss': 2.2275209770202635, 'learning_rate': 8.203909646745933e-06, 'epoch': 2.507765421195244, 'step': 2087000}
INFO:transformers.trainer:{'loss': 2.255769176721573, 'learning_rate': 8.193896208712092e-06, 'epoch': 2.5083662274772744, 'step': 2087500}
INFO:transformers.trainer:{'loss': 2.1956442576646804, 'learning_rate': 8.18388277067825e-06, 'epoch': 2.508967033759305, 'step': 2088000}
INFO:transformers.trainer:{'loss': 2.202455466091633, 'learning_rate': 8.173869332644409e-06, 'epoch': 2.5095678400413357, 'step': 2088500}
INFO:transformers.trainer:{'loss': 2.277243568778038, 'learning_rate': 8.163855894610568e-06, 'epoch': 2.510168646323366, 'step': 2089000}
INFO:transformers.trainer:{'loss': 2.227945386648178, 'learning_rate': 8.153842456576727e-06, 'epoch': 2.5107694526053965, 'step': 2089500}
INFO:transformers.trainer:{'loss': 2.2018606580495836, 'learning_rate': 8.143829018542885e-06, 'epoch': 2.511370258887427, 'step': 2090000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2090000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2090000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2090000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2070000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.257886248588562, 'learning_rate': 8.133815580509044e-06, 'epoch': 2.5119710651694573, 'step': 2090500}
INFO:transformers.trainer:{'loss': 2.2132590259313583, 'learning_rate': 8.123802142475203e-06, 'epoch': 2.5125718714514877, 'step': 2091000}
INFO:transformers.trainer:{'loss': 2.2429987855553626, 'learning_rate': 8.113788704441362e-06, 'epoch': 2.513172677733518, 'step': 2091500}
INFO:transformers.trainer:{'loss': 2.214348349571228, 'learning_rate': 8.103775266407519e-06, 'epoch': 2.513773484015549, 'step': 2092000}
INFO:transformers.trainer:{'loss': 2.2174212218523026, 'learning_rate': 8.093761828373677e-06, 'epoch': 2.5143742902975794, 'step': 2092500}
INFO:transformers.trainer:{'loss': 2.208209927082062, 'learning_rate': 8.083748390339836e-06, 'epoch': 2.51497509657961, 'step': 2093000}
INFO:transformers.trainer:{'loss': 2.192021818935871, 'learning_rate': 8.073734952305995e-06, 'epoch': 2.5155759028616402, 'step': 2093500}
INFO:transformers.trainer:{'loss': 2.259239945054054, 'learning_rate': 8.063721514272154e-06, 'epoch': 2.5161767091436706, 'step': 2094000}
INFO:transformers.trainer:{'loss': 2.204429957270622, 'learning_rate': 8.05370807623831e-06, 'epoch': 2.5167775154257015, 'step': 2094500}
INFO:transformers.trainer:{'loss': 2.2359109194278717, 'learning_rate': 8.043694638204471e-06, 'epoch': 2.517378321707732, 'step': 2095000}
INFO:transformers.trainer:{'loss': 2.2694128676652907, 'learning_rate': 8.03368120017063e-06, 'epoch': 2.5179791279897623, 'step': 2095500}
INFO:transformers.trainer:{'loss': 2.2244488211870195, 'learning_rate': 8.023667762136789e-06, 'epoch': 2.5185799342717927, 'step': 2096000}
INFO:transformers.trainer:{'loss': 2.2330158885717393, 'learning_rate': 8.013654324102947e-06, 'epoch': 2.519180740553823, 'step': 2096500}
INFO:transformers.trainer:{'loss': 2.2445764075517656, 'learning_rate': 8.003640886069105e-06, 'epoch': 2.519781546835854, 'step': 2097000}
INFO:transformers.trainer:{'loss': 2.2225277667045593, 'learning_rate': 7.993627448035263e-06, 'epoch': 2.520382353117884, 'step': 2097500}
INFO:transformers.trainer:{'loss': 2.2539500961899757, 'learning_rate': 7.983614010001422e-06, 'epoch': 2.520983159399915, 'step': 2098000}
INFO:transformers.trainer:{'loss': 2.26328515034914, 'learning_rate': 7.97360057196758e-06, 'epoch': 2.521583965681945, 'step': 2098500}
INFO:transformers.trainer:{'loss': 2.2258345158696176, 'learning_rate': 7.96358713393374e-06, 'epoch': 2.5221847719639756, 'step': 2099000}
INFO:transformers.trainer:{'loss': 2.2193403652906416, 'learning_rate': 7.953573695899898e-06, 'epoch': 2.522785578246006, 'step': 2099500}
INFO:transformers.trainer:{'loss': 2.2469986464977265, 'learning_rate': 7.943560257866055e-06, 'epoch': 2.5233863845280364, 'step': 2100000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2100000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2100000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2100000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2080000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2394312331676485, 'learning_rate': 7.933546819832214e-06, 'epoch': 2.5239871908100673, 'step': 2100500}
INFO:transformers.trainer:{'loss': 2.231065706014633, 'learning_rate': 7.923533381798373e-06, 'epoch': 2.5245879970920977, 'step': 2101000}
INFO:transformers.trainer:{'loss': 2.2013769765496254, 'learning_rate': 7.913519943764533e-06, 'epoch': 2.525188803374128, 'step': 2101500}
INFO:transformers.trainer:{'loss': 2.246199885725975, 'learning_rate': 7.903506505730692e-06, 'epoch': 2.5257896096561585, 'step': 2102000}
INFO:transformers.trainer:{'loss': 2.2209753197431565, 'learning_rate': 7.89349306769685e-06, 'epoch': 2.526390415938189, 'step': 2102500}
INFO:transformers.trainer:{'loss': 2.243058507561684, 'learning_rate': 7.883479629663008e-06, 'epoch': 2.5269912222202198, 'step': 2103000}
INFO:transformers.trainer:{'loss': 2.2155278215408325, 'learning_rate': 7.873466191629167e-06, 'epoch': 2.52759202850225, 'step': 2103500}
INFO:transformers.trainer:{'loss': 2.215836305141449, 'learning_rate': 7.863452753595325e-06, 'epoch': 2.5281928347842806, 'step': 2104000}
INFO:transformers.trainer:{'loss': 2.2200705697536467, 'learning_rate': 7.853439315561484e-06, 'epoch': 2.528793641066311, 'step': 2104500}
INFO:transformers.trainer:{'loss': 2.222102053523064, 'learning_rate': 7.843425877527643e-06, 'epoch': 2.5293944473483414, 'step': 2105000}
INFO:transformers.trainer:{'loss': 2.211292300462723, 'learning_rate': 7.8334124394938e-06, 'epoch': 2.529995253630372, 'step': 2105500}
INFO:transformers.trainer:{'loss': 2.2600082701444624, 'learning_rate': 7.823399001459959e-06, 'epoch': 2.530596059912402, 'step': 2106000}
INFO:transformers.trainer:{'loss': 2.2000365756750107, 'learning_rate': 7.813385563426118e-06, 'epoch': 2.531196866194433, 'step': 2106500}
INFO:transformers.trainer:{'loss': 2.2518074843883515, 'learning_rate': 7.803372125392276e-06, 'epoch': 2.5317976724764635, 'step': 2107000}
INFO:transformers.trainer:{'loss': 2.1709989270567895, 'learning_rate': 7.793358687358437e-06, 'epoch': 2.532398478758494, 'step': 2107500}
INFO:transformers.trainer:{'loss': 2.243032965898514, 'learning_rate': 7.783345249324594e-06, 'epoch': 2.5329992850405243, 'step': 2108000}
INFO:transformers.trainer:{'loss': 2.2703926169872286, 'learning_rate': 7.773331811290753e-06, 'epoch': 2.5336000913225547, 'step': 2108500}
INFO:transformers.trainer:{'loss': 2.26128989315033, 'learning_rate': 7.763318373256911e-06, 'epoch': 2.5342008976045856, 'step': 2109000}
INFO:transformers.trainer:{'loss': 2.2248412997722626, 'learning_rate': 7.75330493522307e-06, 'epoch': 2.534801703886616, 'step': 2109500}
INFO:transformers.trainer:{'loss': 2.198539776086807, 'learning_rate': 7.743291497189229e-06, 'epoch': 2.5354025101686464, 'step': 2110000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2110000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2110000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2110000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2090000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.20860024869442, 'learning_rate': 7.733278059155388e-06, 'epoch': 2.536003316450677, 'step': 2110500}
INFO:transformers.trainer:{'loss': 2.165366789698601, 'learning_rate': 7.723264621121545e-06, 'epoch': 2.536604122732707, 'step': 2111000}
INFO:transformers.trainer:{'loss': 2.204824614405632, 'learning_rate': 7.713251183087703e-06, 'epoch': 2.537204929014738, 'step': 2111500}
INFO:transformers.trainer:{'loss': 2.2203764840364455, 'learning_rate': 7.703237745053862e-06, 'epoch': 2.537805735296768, 'step': 2112000}
INFO:transformers.trainer:{'loss': 2.2190093505978585, 'learning_rate': 7.693224307020021e-06, 'epoch': 2.538406541578799, 'step': 2112500}
INFO:transformers.trainer:{'loss': 2.2124018017053606, 'learning_rate': 7.68321086898618e-06, 'epoch': 2.5390073478608293, 'step': 2113000}
INFO:transformers.trainer:{'loss': 2.262835944056511, 'learning_rate': 7.673197430952338e-06, 'epoch': 2.5396081541428597, 'step': 2113500}
INFO:transformers.trainer:{'loss': 2.1998385105729104, 'learning_rate': 7.663183992918497e-06, 'epoch': 2.54020896042489, 'step': 2114000}
INFO:transformers.trainer:{'loss': 2.1873767273426057, 'learning_rate': 7.653170554884656e-06, 'epoch': 2.5408097667069205, 'step': 2114500}
INFO:transformers.trainer:{'loss': 2.2318624370098115, 'learning_rate': 7.643157116850815e-06, 'epoch': 2.5414105729889513, 'step': 2115000}
INFO:transformers.trainer:{'loss': 2.3007181144952775, 'learning_rate': 7.633143678816974e-06, 'epoch': 2.5420113792709818, 'step': 2115500}
INFO:transformers.trainer:{'loss': 2.250614574432373, 'learning_rate': 7.6231302407831314e-06, 'epoch': 2.542612185553012, 'step': 2116000}
INFO:transformers.trainer:{'loss': 2.210919330239296, 'learning_rate': 7.61311680274929e-06, 'epoch': 2.5432129918350426, 'step': 2116500}
INFO:transformers.trainer:{'loss': 2.2021721839904784, 'learning_rate': 7.603103364715448e-06, 'epoch': 2.543813798117073, 'step': 2117000}
INFO:transformers.trainer:{'loss': 2.2485439254045487, 'learning_rate': 7.593089926681607e-06, 'epoch': 2.544414604399104, 'step': 2117500}
INFO:transformers.trainer:{'loss': 2.2289165441989898, 'learning_rate': 7.583076488647766e-06, 'epoch': 2.5450154106811342, 'step': 2118000}
INFO:transformers.trainer:{'loss': 2.202833436846733, 'learning_rate': 7.5730630506139235e-06, 'epoch': 2.5456162169631646, 'step': 2118500}
INFO:transformers.trainer:{'loss': 2.2422740947008135, 'learning_rate': 7.563049612580082e-06, 'epoch': 2.546217023245195, 'step': 2119000}
INFO:transformers.trainer:{'loss': 2.2690334316492082, 'learning_rate': 7.553036174546241e-06, 'epoch': 2.5468178295272255, 'step': 2119500}
INFO:transformers.trainer:{'loss': 2.2221374460458754, 'learning_rate': 7.543022736512401e-06, 'epoch': 2.547418635809256, 'step': 2120000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2120000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2120000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2120000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2100000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.262860473752022, 'learning_rate': 7.533009298478559e-06, 'epoch': 2.5480194420912863, 'step': 2120500}
INFO:transformers.trainer:{'loss': 2.23257175052166, 'learning_rate': 7.522995860444717e-06, 'epoch': 2.548620248373317, 'step': 2121000}
INFO:transformers.trainer:{'loss': 2.2165933809280394, 'learning_rate': 7.512982422410876e-06, 'epoch': 2.5492210546553475, 'step': 2121500}
INFO:transformers.trainer:{'loss': 2.206030357003212, 'learning_rate': 7.502968984377035e-06, 'epoch': 2.549821860937378, 'step': 2122000}
INFO:transformers.trainer:{'loss': 2.229324182868004, 'learning_rate': 7.492955546343193e-06, 'epoch': 2.5504226672194084, 'step': 2122500}
INFO:transformers.trainer:{'loss': 2.1926487078666685, 'learning_rate': 7.4829421083093515e-06, 'epoch': 2.5510234735014388, 'step': 2123000}
INFO:transformers.trainer:{'loss': 2.2171587677001954, 'learning_rate': 7.47292867027551e-06, 'epoch': 2.5516242797834696, 'step': 2123500}
INFO:transformers.trainer:{'loss': 2.2591221286058425, 'learning_rate': 7.462915232241668e-06, 'epoch': 2.5522250860655, 'step': 2124000}
INFO:transformers.trainer:{'loss': 2.2177357699275015, 'learning_rate': 7.452901794207827e-06, 'epoch': 2.5528258923475304, 'step': 2124500}
INFO:transformers.trainer:{'loss': 2.2660591776371004, 'learning_rate': 7.442888356173986e-06, 'epoch': 2.553426698629561, 'step': 2125000}
INFO:transformers.trainer:{'loss': 2.18582616853714, 'learning_rate': 7.432874918140144e-06, 'epoch': 2.5540275049115913, 'step': 2125500}
INFO:transformers.trainer:{'loss': 2.2292292478084565, 'learning_rate': 7.422861480106302e-06, 'epoch': 2.554628311193622, 'step': 2126000}
INFO:transformers.trainer:{'loss': 2.238834731578827, 'learning_rate': 7.412848042072462e-06, 'epoch': 2.555229117475652, 'step': 2126500}
INFO:transformers.trainer:{'loss': 2.251886802792549, 'learning_rate': 7.402834604038621e-06, 'epoch': 2.555829923757683, 'step': 2127000}
INFO:transformers.trainer:{'loss': 2.246394349336624, 'learning_rate': 7.3928211660047795e-06, 'epoch': 2.5564307300397133, 'step': 2127500}
INFO:transformers.trainer:{'loss': 2.2190620868206024, 'learning_rate': 7.382807727970937e-06, 'epoch': 2.5570315363217437, 'step': 2128000}
INFO:transformers.trainer:{'loss': 2.2117654600143433, 'learning_rate': 7.372794289937096e-06, 'epoch': 2.557632342603774, 'step': 2128500}
INFO:transformers.trainer:{'loss': 2.2457504589557646, 'learning_rate': 7.362780851903255e-06, 'epoch': 2.5582331488858046, 'step': 2129000}
INFO:transformers.trainer:{'loss': 2.2886752766370773, 'learning_rate': 7.352767413869413e-06, 'epoch': 2.5588339551678354, 'step': 2129500}
INFO:transformers.trainer:{'loss': 2.2192200890779494, 'learning_rate': 7.3427539758355716e-06, 'epoch': 2.559434761449866, 'step': 2130000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2130000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2130000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2130000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2110000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2701441646814344, 'learning_rate': 7.33274053780173e-06, 'epoch': 2.5600355677318962, 'step': 2130500}
INFO:transformers.trainer:{'loss': 2.166303487062454, 'learning_rate': 7.322727099767888e-06, 'epoch': 2.5606363740139266, 'step': 2131000}
INFO:transformers.trainer:{'loss': 2.2775279363393786, 'learning_rate': 7.312713661734047e-06, 'epoch': 2.561237180295957, 'step': 2131500}
INFO:transformers.trainer:{'loss': 2.2656427698731423, 'learning_rate': 7.302700223700206e-06, 'epoch': 2.561837986577988, 'step': 2132000}
INFO:transformers.trainer:{'loss': 2.2279213894605636, 'learning_rate': 7.292686785666365e-06, 'epoch': 2.5624387928600183, 'step': 2132500}
INFO:transformers.trainer:{'loss': 2.2490091576576234, 'learning_rate': 7.282673347632524e-06, 'epoch': 2.5630395991420487, 'step': 2133000}
INFO:transformers.trainer:{'loss': 2.1831891469955442, 'learning_rate': 7.272659909598682e-06, 'epoch': 2.563640405424079, 'step': 2133500}
INFO:transformers.trainer:{'loss': 2.1818940666913984, 'learning_rate': 7.262646471564841e-06, 'epoch': 2.5642412117061095, 'step': 2134000}
INFO:transformers.trainer:{'loss': 2.247999658584595, 'learning_rate': 7.2526330335309996e-06, 'epoch': 2.56484201798814, 'step': 2134500}
INFO:transformers.trainer:{'loss': 2.240041259765625, 'learning_rate': 7.2426195954971575e-06, 'epoch': 2.5654428242701703, 'step': 2135000}
INFO:transformers.trainer:{'loss': 2.200492555975914, 'learning_rate': 7.232606157463316e-06, 'epoch': 2.566043630552201, 'step': 2135500}
INFO:transformers.trainer:{'loss': 2.2722499144673347, 'learning_rate': 7.222592719429475e-06, 'epoch': 2.5666444368342316, 'step': 2136000}
INFO:transformers.trainer:{'loss': 2.223445234656334, 'learning_rate': 7.212579281395633e-06, 'epoch': 2.567245243116262, 'step': 2136500}
INFO:transformers.trainer:{'loss': 2.1828448284864423, 'learning_rate': 7.202565843361792e-06, 'epoch': 2.5678460493982924, 'step': 2137000}
INFO:transformers.trainer:{'loss': 2.25952705681324, 'learning_rate': 7.19255240532795e-06, 'epoch': 2.568446855680323, 'step': 2137500}
INFO:transformers.trainer:{'loss': 2.2928559893369673, 'learning_rate': 7.182538967294108e-06, 'epoch': 2.5690476619623537, 'step': 2138000}
INFO:transformers.trainer:{'loss': 2.211883432507515, 'learning_rate': 7.172525529260267e-06, 'epoch': 2.569648468244384, 'step': 2138500}
INFO:transformers.trainer:{'loss': 2.245361002922058, 'learning_rate': 7.162512091226427e-06, 'epoch': 2.5702492745264145, 'step': 2139000}
INFO:transformers.trainer:{'loss': 2.2382904747724535, 'learning_rate': 7.1524986531925854e-06, 'epoch': 2.570850080808445, 'step': 2139500}
INFO:transformers.trainer:{'loss': 2.18573145031929, 'learning_rate': 7.142485215158743e-06, 'epoch': 2.5714508870904753, 'step': 2140000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2140000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2140000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2140000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2120000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2640922697782515, 'learning_rate': 7.132471777124902e-06, 'epoch': 2.572051693372506, 'step': 2140500}
INFO:transformers.trainer:{'loss': 2.2409374105930326, 'learning_rate': 7.122458339091061e-06, 'epoch': 2.572652499654536, 'step': 2141000}
INFO:transformers.trainer:{'loss': 2.212798614025116, 'learning_rate': 7.112444901057219e-06, 'epoch': 2.573253305936567, 'step': 2141500}
INFO:transformers.trainer:{'loss': 2.198881033360958, 'learning_rate': 7.1024314630233775e-06, 'epoch': 2.5738541122185974, 'step': 2142000}
INFO:transformers.trainer:{'loss': 2.2120765899419785, 'learning_rate': 7.092418024989536e-06, 'epoch': 2.574454918500628, 'step': 2142500}
INFO:transformers.trainer:{'loss': 2.2074409504532815, 'learning_rate': 7.082404586955694e-06, 'epoch': 2.575055724782658, 'step': 2143000}
INFO:transformers.trainer:{'loss': 2.202620290875435, 'learning_rate': 7.072391148921853e-06, 'epoch': 2.5756565310646886, 'step': 2143500}
INFO:transformers.trainer:{'loss': 2.1945406806468966, 'learning_rate': 7.062377710888012e-06, 'epoch': 2.5762573373467195, 'step': 2144000}
INFO:transformers.trainer:{'loss': 2.167528520107269, 'learning_rate': 7.05236427285417e-06, 'epoch': 2.57685814362875, 'step': 2144500}
INFO:transformers.trainer:{'loss': 2.2042081617712976, 'learning_rate': 7.04235083482033e-06, 'epoch': 2.5774589499107803, 'step': 2145000}
INFO:transformers.trainer:{'loss': 2.2403876155614855, 'learning_rate': 7.032337396786488e-06, 'epoch': 2.5780597561928107, 'step': 2145500}
INFO:transformers.trainer:{'loss': 2.2680416471958162, 'learning_rate': 7.022323958752647e-06, 'epoch': 2.578660562474841, 'step': 2146000}
INFO:transformers.trainer:{'loss': 2.205069320678711, 'learning_rate': 7.0123105207188055e-06, 'epoch': 2.579261368756872, 'step': 2146500}
INFO:transformers.trainer:{'loss': 2.231244874238968, 'learning_rate': 7.0022970826849634e-06, 'epoch': 2.5798621750389024, 'step': 2147000}
INFO:transformers.trainer:{'loss': 2.2400216225385665, 'learning_rate': 6.992283644651122e-06, 'epoch': 2.5804629813209328, 'step': 2147500}
INFO:transformers.trainer:{'loss': 2.2307184401750564, 'learning_rate': 6.982270206617281e-06, 'epoch': 2.581063787602963, 'step': 2148000}
INFO:transformers.trainer:{'loss': 2.1908453602790834, 'learning_rate': 6.972256768583439e-06, 'epoch': 2.5816645938849936, 'step': 2148500}
INFO:transformers.trainer:{'loss': 2.1949161505699157, 'learning_rate': 6.962243330549598e-06, 'epoch': 2.582265400167024, 'step': 2149000}
INFO:transformers.trainer:{'loss': 2.222284170508385, 'learning_rate': 6.952229892515756e-06, 'epoch': 2.5828662064490544, 'step': 2149500}
INFO:transformers.trainer:{'loss': 2.206778399348259, 'learning_rate': 6.942216454481914e-06, 'epoch': 2.5834670127310853, 'step': 2150000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2150000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2150000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2150000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2130000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.212001930952072, 'learning_rate': 6.932203016448073e-06, 'epoch': 2.5840678190131157, 'step': 2150500}
INFO:transformers.trainer:{'loss': 2.1757183517217635, 'learning_rate': 6.922189578414232e-06, 'epoch': 2.584668625295146, 'step': 2151000}
INFO:transformers.trainer:{'loss': 2.2247650760412214, 'learning_rate': 6.912176140380391e-06, 'epoch': 2.5852694315771765, 'step': 2151500}
INFO:transformers.trainer:{'loss': 2.2142345632314684, 'learning_rate': 6.90216270234655e-06, 'epoch': 2.585870237859207, 'step': 2152000}
INFO:transformers.trainer:{'loss': 2.212756268262863, 'learning_rate': 6.892149264312708e-06, 'epoch': 2.5864710441412377, 'step': 2152500}
INFO:transformers.trainer:{'loss': 2.2006851576566695, 'learning_rate': 6.882135826278867e-06, 'epoch': 2.587071850423268, 'step': 2153000}
INFO:transformers.trainer:{'loss': 2.2171589242219927, 'learning_rate': 6.872122388245026e-06, 'epoch': 2.5876726567052986, 'step': 2153500}
INFO:transformers.trainer:{'loss': 2.234467181801796, 'learning_rate': 6.8621089502111835e-06, 'epoch': 2.588273462987329, 'step': 2154000}
INFO:transformers.trainer:{'loss': 2.185318826556206, 'learning_rate': 6.852095512177342e-06, 'epoch': 2.5888742692693594, 'step': 2154500}
INFO:transformers.trainer:{'loss': 2.2700886459350587, 'learning_rate': 6.842082074143501e-06, 'epoch': 2.5894750755513902, 'step': 2155000}
INFO:transformers.trainer:{'loss': 2.2194874629080297, 'learning_rate': 6.832068636109659e-06, 'epoch': 2.59007588183342, 'step': 2155500}
INFO:transformers.trainer:{'loss': 2.2613629182577135, 'learning_rate': 6.822055198075818e-06, 'epoch': 2.590676688115451, 'step': 2156000}
INFO:transformers.trainer:{'loss': 2.226222093462944, 'learning_rate': 6.8120417600419764e-06, 'epoch': 2.5912774943974815, 'step': 2156500}
INFO:transformers.trainer:{'loss': 2.20863552737236, 'learning_rate': 6.802028322008134e-06, 'epoch': 2.591878300679512, 'step': 2157000}
INFO:transformers.trainer:{'loss': 2.216681218624115, 'learning_rate': 6.792014883974295e-06, 'epoch': 2.5924791069615423, 'step': 2157500}
INFO:transformers.trainer:{'loss': 2.2211316454410555, 'learning_rate': 6.782001445940453e-06, 'epoch': 2.5930799132435727, 'step': 2158000}
INFO:transformers.trainer:{'loss': 2.234465648174286, 'learning_rate': 6.7719880079066115e-06, 'epoch': 2.5936807195256035, 'step': 2158500}
INFO:transformers.trainer:{'loss': 2.19229692953825, 'learning_rate': 6.76197456987277e-06, 'epoch': 2.594281525807634, 'step': 2159000}
INFO:transformers.trainer:{'loss': 2.2112855974435806, 'learning_rate': 6.751961131838928e-06, 'epoch': 2.5948823320896643, 'step': 2159500}
INFO:transformers.trainer:{'loss': 2.1934649938344957, 'learning_rate': 6.741947693805087e-06, 'epoch': 2.5954831383716948, 'step': 2160000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2160000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2160000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2160000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2140000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2779872705340387, 'learning_rate': 6.731934255771246e-06, 'epoch': 2.596083944653725, 'step': 2160500}
INFO:transformers.trainer:{'loss': 2.2310730654001234, 'learning_rate': 6.721920817737404e-06, 'epoch': 2.596684750935756, 'step': 2161000}
INFO:transformers.trainer:{'loss': 2.2528701469898222, 'learning_rate': 6.711907379703562e-06, 'epoch': 2.5972855572177864, 'step': 2161500}
INFO:transformers.trainer:{'loss': 2.176495295763016, 'learning_rate': 6.701893941669721e-06, 'epoch': 2.597886363499817, 'step': 2162000}
INFO:transformers.trainer:{'loss': 2.2110359988212585, 'learning_rate': 6.691880503635879e-06, 'epoch': 2.5984871697818472, 'step': 2162500}
INFO:transformers.trainer:{'loss': 2.1994331699609755, 'learning_rate': 6.681867065602038e-06, 'epoch': 2.5990879760638776, 'step': 2163000}
INFO:transformers.trainer:{'loss': 2.216385550737381, 'learning_rate': 6.6718536275681965e-06, 'epoch': 2.599688782345908, 'step': 2163500}
INFO:transformers.trainer:{'loss': 2.2137581450939177, 'learning_rate': 6.661840189534356e-06, 'epoch': 2.6002895886279385, 'step': 2164000}
INFO:transformers.trainer:{'loss': 2.1874404302835466, 'learning_rate': 6.651826751500515e-06, 'epoch': 2.6008903949099693, 'step': 2164500}
INFO:transformers.trainer:{'loss': 2.2235702327489855, 'learning_rate': 6.641813313466673e-06, 'epoch': 2.6014912011919997, 'step': 2165000}
INFO:transformers.trainer:{'loss': 2.2052448011636736, 'learning_rate': 6.6317998754328316e-06, 'epoch': 2.60209200747403, 'step': 2165500}
INFO:transformers.trainer:{'loss': 2.233429048061371, 'learning_rate': 6.62178643739899e-06, 'epoch': 2.6026928137560605, 'step': 2166000}
INFO:transformers.trainer:{'loss': 2.252753435254097, 'learning_rate': 6.611772999365148e-06, 'epoch': 2.603293620038091, 'step': 2166500}
INFO:transformers.trainer:{'loss': 2.1976902792453767, 'learning_rate': 6.601759561331307e-06, 'epoch': 2.603894426320122, 'step': 2167000}
INFO:transformers.trainer:{'loss': 2.171298527956009, 'learning_rate': 6.591746123297466e-06, 'epoch': 2.604495232602152, 'step': 2167500}
INFO:transformers.trainer:{'loss': 2.2552407817840576, 'learning_rate': 6.581732685263624e-06, 'epoch': 2.6050960388841826, 'step': 2168000}
INFO:transformers.trainer:{'loss': 2.2435773396492005, 'learning_rate': 6.571719247229782e-06, 'epoch': 2.605696845166213, 'step': 2168500}
INFO:transformers.trainer:{'loss': 2.195908478319645, 'learning_rate': 6.56170580919594e-06, 'epoch': 2.6062976514482434, 'step': 2169000}
INFO:transformers.trainer:{'loss': 2.252104173064232, 'learning_rate': 6.551692371162099e-06, 'epoch': 2.6068984577302743, 'step': 2169500}
INFO:transformers.trainer:{'loss': 2.2851798862218855, 'learning_rate': 6.541678933128259e-06, 'epoch': 2.6074992640123043, 'step': 2170000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2170000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2170000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2170000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2150000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.18965439081192, 'learning_rate': 6.5316654950944174e-06, 'epoch': 2.608100070294335, 'step': 2170500}
INFO:transformers.trainer:{'loss': 2.2142036551237108, 'learning_rate': 6.521652057060576e-06, 'epoch': 2.6087008765763655, 'step': 2171000}
INFO:transformers.trainer:{'loss': 2.221210877418518, 'learning_rate': 6.511638619026734e-06, 'epoch': 2.609301682858396, 'step': 2171500}
INFO:transformers.trainer:{'loss': 2.2201686972379684, 'learning_rate': 6.501625180992893e-06, 'epoch': 2.6099024891404263, 'step': 2172000}
INFO:transformers.trainer:{'loss': 2.2638083741664885, 'learning_rate': 6.491611742959052e-06, 'epoch': 2.6105032954224567, 'step': 2172500}
INFO:transformers.trainer:{'loss': 2.2370590969324113, 'learning_rate': 6.4815983049252095e-06, 'epoch': 2.6111041017044876, 'step': 2173000}
INFO:transformers.trainer:{'loss': 2.2346446996927263, 'learning_rate': 6.471584866891368e-06, 'epoch': 2.611704907986518, 'step': 2173500}
INFO:transformers.trainer:{'loss': 2.2891158341169358, 'learning_rate': 6.461571428857527e-06, 'epoch': 2.6123057142685484, 'step': 2174000}
INFO:transformers.trainer:{'loss': 2.23884061729908, 'learning_rate': 6.451557990823685e-06, 'epoch': 2.612906520550579, 'step': 2174500}
INFO:transformers.trainer:{'loss': 2.221172179222107, 'learning_rate': 6.441544552789844e-06, 'epoch': 2.6135073268326092, 'step': 2175000}
INFO:transformers.trainer:{'loss': 2.2309783474206926, 'learning_rate': 6.4315311147560025e-06, 'epoch': 2.61410813311464, 'step': 2175500}
INFO:transformers.trainer:{'loss': 2.2032081773281096, 'learning_rate': 6.42151767672216e-06, 'epoch': 2.6147089393966705, 'step': 2176000}
INFO:transformers.trainer:{'loss': 2.199008923649788, 'learning_rate': 6.411504238688321e-06, 'epoch': 2.615309745678701, 'step': 2176500}
INFO:transformers.trainer:{'loss': 2.255893191933632, 'learning_rate': 6.401490800654479e-06, 'epoch': 2.6159105519607313, 'step': 2177000}
INFO:transformers.trainer:{'loss': 2.242934284329414, 'learning_rate': 6.3914773626206375e-06, 'epoch': 2.6165113582427617, 'step': 2177500}
INFO:transformers.trainer:{'loss': 2.2167998732924463, 'learning_rate': 6.381463924586796e-06, 'epoch': 2.617112164524792, 'step': 2178000}
INFO:transformers.trainer:{'loss': 2.223617107450962, 'learning_rate': 6.371450486552954e-06, 'epoch': 2.6177129708068225, 'step': 2178500}
INFO:transformers.trainer:{'loss': 2.214195291399956, 'learning_rate': 6.361437048519113e-06, 'epoch': 2.6183137770888534, 'step': 2179000}
INFO:transformers.trainer:{'loss': 2.2224567707777023, 'learning_rate': 6.351423610485272e-06, 'epoch': 2.618914583370884, 'step': 2179500}
INFO:transformers.trainer:{'loss': 2.201135620713234, 'learning_rate': 6.34141017245143e-06, 'epoch': 2.619515389652914, 'step': 2180000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2180000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2180000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2180000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2160000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.251001361489296, 'learning_rate': 6.331396734417588e-06, 'epoch': 2.6201161959349446, 'step': 2180500}
INFO:transformers.trainer:{'loss': 2.221563399195671, 'learning_rate': 6.321383296383747e-06, 'epoch': 2.620717002216975, 'step': 2181000}
INFO:transformers.trainer:{'loss': 2.1727990185022352, 'learning_rate': 6.311369858349905e-06, 'epoch': 2.621317808499006, 'step': 2181500}
INFO:transformers.trainer:{'loss': 2.2241218130588534, 'learning_rate': 6.301356420316064e-06, 'epoch': 2.6219186147810363, 'step': 2182000}
INFO:transformers.trainer:{'loss': 2.17324373459816, 'learning_rate': 6.291342982282223e-06, 'epoch': 2.6225194210630667, 'step': 2182500}
INFO:transformers.trainer:{'loss': 2.2055803909301757, 'learning_rate': 6.281329544248382e-06, 'epoch': 2.623120227345097, 'step': 2183000}
INFO:transformers.trainer:{'loss': 2.2579109404087068, 'learning_rate': 6.271316106214541e-06, 'epoch': 2.6237210336271275, 'step': 2183500}
INFO:transformers.trainer:{'loss': 2.2580065861940386, 'learning_rate': 6.261302668180699e-06, 'epoch': 2.6243218399091583, 'step': 2184000}
INFO:transformers.trainer:{'loss': 2.269920798897743, 'learning_rate': 6.251289230146858e-06, 'epoch': 2.6249226461911883, 'step': 2184500}
INFO:transformers.trainer:{'loss': 2.204375009894371, 'learning_rate': 6.241275792113016e-06, 'epoch': 2.625523452473219, 'step': 2185000}
INFO:transformers.trainer:{'loss': 2.217799307703972, 'learning_rate': 6.231262354079174e-06, 'epoch': 2.6261242587552496, 'step': 2185500}
INFO:transformers.trainer:{'loss': 2.2079715276956557, 'learning_rate': 6.221248916045333e-06, 'epoch': 2.62672506503728, 'step': 2186000}
INFO:transformers.trainer:{'loss': 2.242581798672676, 'learning_rate': 6.211235478011492e-06, 'epoch': 2.6273258713193104, 'step': 2186500}
INFO:transformers.trainer:{'loss': 2.17363576233387, 'learning_rate': 6.2012220399776505e-06, 'epoch': 2.627926677601341, 'step': 2187000}
INFO:transformers.trainer:{'loss': 2.1958812248706816, 'learning_rate': 6.191208601943809e-06, 'epoch': 2.6285274838833717, 'step': 2187500}
INFO:transformers.trainer:{'loss': 2.215111819148064, 'learning_rate': 6.181195163909967e-06, 'epoch': 2.629128290165402, 'step': 2188000}
INFO:transformers.trainer:{'loss': 2.201363885819912, 'learning_rate': 6.171181725876126e-06, 'epoch': 2.6297290964474325, 'step': 2188500}
INFO:transformers.trainer:{'loss': 2.1792693614959715, 'learning_rate': 6.161168287842285e-06, 'epoch': 2.630329902729463, 'step': 2189000}
INFO:transformers.trainer:{'loss': 2.1695499611496927, 'learning_rate': 6.151154849808443e-06, 'epoch': 2.6309307090114933, 'step': 2189500}
INFO:transformers.trainer:{'loss': 2.208199055433273, 'learning_rate': 6.141141411774601e-06, 'epoch': 2.631531515293524, 'step': 2190000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2190000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2190000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2190000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2170000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.198717294573784, 'learning_rate': 6.131127973740761e-06, 'epoch': 2.6321323215755545, 'step': 2190500}
INFO:transformers.trainer:{'loss': 2.192541578292847, 'learning_rate': 6.121114535706919e-06, 'epoch': 2.632733127857585, 'step': 2191000}
INFO:transformers.trainer:{'loss': 2.246027610421181, 'learning_rate': 6.111101097673078e-06, 'epoch': 2.6333339341396154, 'step': 2191500}
INFO:transformers.trainer:{'loss': 2.2159869718551635, 'learning_rate': 6.1010876596392364e-06, 'epoch': 2.6339347404216458, 'step': 2192000}
INFO:transformers.trainer:{'loss': 2.215573352217674, 'learning_rate': 6.091074221605394e-06, 'epoch': 2.634535546703676, 'step': 2192500}
INFO:transformers.trainer:{'loss': 2.268019651174545, 'learning_rate': 6.081060783571553e-06, 'epoch': 2.6351363529857066, 'step': 2193000}
INFO:transformers.trainer:{'loss': 2.232378554582596, 'learning_rate': 6.071047345537712e-06, 'epoch': 2.6357371592677374, 'step': 2193500}
INFO:transformers.trainer:{'loss': 2.1848696545362474, 'learning_rate': 6.061033907503871e-06, 'epoch': 2.636337965549768, 'step': 2194000}
INFO:transformers.trainer:{'loss': 2.2207047854661943, 'learning_rate': 6.051020469470029e-06, 'epoch': 2.6369387718317983, 'step': 2194500}
INFO:transformers.trainer:{'loss': 2.1620086438059807, 'learning_rate': 6.041007031436187e-06, 'epoch': 2.6375395781138287, 'step': 2195000}
INFO:transformers.trainer:{'loss': 2.1983925050199034, 'learning_rate': 6.030993593402346e-06, 'epoch': 2.638140384395859, 'step': 2195500}
INFO:transformers.trainer:{'loss': 2.26984980905056, 'learning_rate': 6.020980155368505e-06, 'epoch': 2.63874119067789, 'step': 2196000}
INFO:transformers.trainer:{'loss': 2.149292453944683, 'learning_rate': 6.0109667173346636e-06, 'epoch': 2.6393419969599203, 'step': 2196500}
INFO:transformers.trainer:{'loss': 2.2627439336776733, 'learning_rate': 6.000953279300822e-06, 'epoch': 2.6399428032419507, 'step': 2197000}
INFO:transformers.trainer:{'loss': 2.23458587706089, 'learning_rate': 5.990939841266981e-06, 'epoch': 2.640543609523981, 'step': 2197500}
INFO:transformers.trainer:{'loss': 2.2150026992559435, 'learning_rate': 5.980926403233139e-06, 'epoch': 2.6411444158060116, 'step': 2198000}
INFO:transformers.trainer:{'loss': 2.207730107784271, 'learning_rate': 5.970912965199298e-06, 'epoch': 2.6417452220880424, 'step': 2198500}
INFO:transformers.trainer:{'loss': 2.2165399643182755, 'learning_rate': 5.960899527165456e-06, 'epoch': 2.6423460283700724, 'step': 2199000}
INFO:transformers.trainer:{'loss': 2.24922955262661, 'learning_rate': 5.950886089131615e-06, 'epoch': 2.6429468346521032, 'step': 2199500}
INFO:transformers.trainer:{'loss': 2.2176358919143677, 'learning_rate': 5.940872651097774e-06, 'epoch': 2.6435476409341336, 'step': 2200000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2200000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2200000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2200000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2180000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2402972396016123, 'learning_rate': 5.930859213063932e-06, 'epoch': 2.644148447216164, 'step': 2200500}
INFO:transformers.trainer:{'loss': 2.2061672167778017, 'learning_rate': 5.920845775030091e-06, 'epoch': 2.6447492534981945, 'step': 2201000}
INFO:transformers.trainer:{'loss': 2.2670517901182174, 'learning_rate': 5.9108323369962494e-06, 'epoch': 2.645350059780225, 'step': 2201500}
INFO:transformers.trainer:{'loss': 2.205862083911896, 'learning_rate': 5.900818898962407e-06, 'epoch': 2.6459508660622557, 'step': 2202000}
INFO:transformers.trainer:{'loss': 2.2262138097286224, 'learning_rate': 5.890805460928566e-06, 'epoch': 2.646551672344286, 'step': 2202500}
INFO:transformers.trainer:{'loss': 2.193878086566925, 'learning_rate': 5.880792022894725e-06, 'epoch': 2.6471524786263165, 'step': 2203000}
INFO:transformers.trainer:{'loss': 2.232020098209381, 'learning_rate': 5.870778584860884e-06, 'epoch': 2.647753284908347, 'step': 2203500}
INFO:transformers.trainer:{'loss': 2.187703304171562, 'learning_rate': 5.860765146827042e-06, 'epoch': 2.6483540911903773, 'step': 2204000}
INFO:transformers.trainer:{'loss': 2.237785867571831, 'learning_rate': 5.8507517087932e-06, 'epoch': 2.648954897472408, 'step': 2204500}
INFO:transformers.trainer:{'loss': 2.2430350569486617, 'learning_rate': 5.840738270759359e-06, 'epoch': 2.6495557037544386, 'step': 2205000}
INFO:transformers.trainer:{'loss': 2.1807216931581497, 'learning_rate': 5.830724832725518e-06, 'epoch': 2.650156510036469, 'step': 2205500}
INFO:transformers.trainer:{'loss': 2.2331770182847976, 'learning_rate': 5.820711394691677e-06, 'epoch': 2.6507573163184994, 'step': 2206000}
INFO:transformers.trainer:{'loss': 2.233018810391426, 'learning_rate': 5.810697956657835e-06, 'epoch': 2.65135812260053, 'step': 2206500}
INFO:transformers.trainer:{'loss': 2.220524045109749, 'learning_rate': 5.800684518623994e-06, 'epoch': 2.6519589288825602, 'step': 2207000}
INFO:transformers.trainer:{'loss': 2.206000868678093, 'learning_rate': 5.790671080590152e-06, 'epoch': 2.6525597351645906, 'step': 2207500}
INFO:transformers.trainer:{'loss': 2.1966891729831697, 'learning_rate': 5.780657642556311e-06, 'epoch': 2.6531605414466215, 'step': 2208000}
INFO:transformers.trainer:{'loss': 2.2306956520080568, 'learning_rate': 5.7706442045224695e-06, 'epoch': 2.653761347728652, 'step': 2208500}
INFO:transformers.trainer:{'loss': 2.2232699584960938, 'learning_rate': 5.760630766488628e-06, 'epoch': 2.6543621540106823, 'step': 2209000}
INFO:transformers.trainer:{'loss': 2.1760311216712, 'learning_rate': 5.750617328454787e-06, 'epoch': 2.6549629602927127, 'step': 2209500}
INFO:transformers.trainer:{'loss': 2.2196998066902163, 'learning_rate': 5.740603890420945e-06, 'epoch': 2.655563766574743, 'step': 2210000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2210000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2210000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2210000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2190000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1668091254234314, 'learning_rate': 5.730590452387104e-06, 'epoch': 2.656164572856774, 'step': 2210500}
INFO:transformers.trainer:{'loss': 2.2297792648077013, 'learning_rate': 5.7205770143532625e-06, 'epoch': 2.6567653791388044, 'step': 2211000}
INFO:transformers.trainer:{'loss': 2.2354984776973725, 'learning_rate': 5.71056357631942e-06, 'epoch': 2.657366185420835, 'step': 2211500}
INFO:transformers.trainer:{'loss': 2.1733448207378387, 'learning_rate': 5.70055013828558e-06, 'epoch': 2.657966991702865, 'step': 2212000}
INFO:transformers.trainer:{'loss': 2.1987335230708123, 'learning_rate': 5.690536700251739e-06, 'epoch': 2.6585677979848956, 'step': 2212500}
INFO:transformers.trainer:{'loss': 2.2201031879186632, 'learning_rate': 5.680523262217897e-06, 'epoch': 2.6591686042669265, 'step': 2213000}
INFO:transformers.trainer:{'loss': 2.221456504225731, 'learning_rate': 5.670509824184055e-06, 'epoch': 2.6597694105489564, 'step': 2213500}
INFO:transformers.trainer:{'loss': 2.202586388349533, 'learning_rate': 5.660496386150213e-06, 'epoch': 2.6603702168309873, 'step': 2214000}
INFO:transformers.trainer:{'loss': 2.2192633270025253, 'learning_rate': 5.650482948116372e-06, 'epoch': 2.6609710231130177, 'step': 2214500}
INFO:transformers.trainer:{'loss': 2.2248875778913497, 'learning_rate': 5.640469510082531e-06, 'epoch': 2.661571829395048, 'step': 2215000}
INFO:transformers.trainer:{'loss': 2.2339839677512647, 'learning_rate': 5.63045607204869e-06, 'epoch': 2.6621726356770785, 'step': 2215500}
INFO:transformers.trainer:{'loss': 2.2387125133275987, 'learning_rate': 5.620442634014848e-06, 'epoch': 2.662773441959109, 'step': 2216000}
INFO:transformers.trainer:{'loss': 2.2438277748823166, 'learning_rate': 5.610429195981007e-06, 'epoch': 2.6633742482411398, 'step': 2216500}
INFO:transformers.trainer:{'loss': 2.231272908329964, 'learning_rate': 5.600415757947165e-06, 'epoch': 2.66397505452317, 'step': 2217000}
INFO:transformers.trainer:{'loss': 2.1917779651880265, 'learning_rate': 5.590402319913324e-06, 'epoch': 2.6645758608052006, 'step': 2217500}
INFO:transformers.trainer:{'loss': 2.2009877989292144, 'learning_rate': 5.5803888818794825e-06, 'epoch': 2.665176667087231, 'step': 2218000}
INFO:transformers.trainer:{'loss': 2.227777336835861, 'learning_rate': 5.570375443845641e-06, 'epoch': 2.6657774733692614, 'step': 2218500}
INFO:transformers.trainer:{'loss': 2.213735343337059, 'learning_rate': 5.5603620058118e-06, 'epoch': 2.6663782796512923, 'step': 2219000}
INFO:transformers.trainer:{'loss': 2.1915378762483595, 'learning_rate': 5.550348567777958e-06, 'epoch': 2.6669790859333227, 'step': 2219500}
INFO:transformers.trainer:{'loss': 2.190771415233612, 'learning_rate': 5.540335129744117e-06, 'epoch': 2.667579892215353, 'step': 2220000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2220000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2220000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2220000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2200000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2078796646595, 'learning_rate': 5.5303216917102755e-06, 'epoch': 2.6681806984973835, 'step': 2220500}
INFO:transformers.trainer:{'loss': 2.2427405890226364, 'learning_rate': 5.520308253676433e-06, 'epoch': 2.668781504779414, 'step': 2221000}
INFO:transformers.trainer:{'loss': 2.1968685706853868, 'learning_rate': 5.510294815642593e-06, 'epoch': 2.6693823110614443, 'step': 2221500}
INFO:transformers.trainer:{'loss': 2.193513441205025, 'learning_rate': 5.500281377608752e-06, 'epoch': 2.6699831173434747, 'step': 2222000}
INFO:transformers.trainer:{'loss': 2.2096426292061806, 'learning_rate': 5.49026793957491e-06, 'epoch': 2.6705839236255056, 'step': 2222500}
INFO:transformers.trainer:{'loss': 2.243430007457733, 'learning_rate': 5.4802545015410684e-06, 'epoch': 2.671184729907536, 'step': 2223000}
INFO:transformers.trainer:{'loss': 2.2338969385921956, 'learning_rate': 5.470241063507227e-06, 'epoch': 2.6717855361895664, 'step': 2223500}
INFO:transformers.trainer:{'loss': 2.177530732035637, 'learning_rate': 5.460227625473385e-06, 'epoch': 2.672386342471597, 'step': 2224000}
INFO:transformers.trainer:{'loss': 2.1817194372415543, 'learning_rate': 5.450214187439545e-06, 'epoch': 2.672987148753627, 'step': 2224500}
INFO:transformers.trainer:{'loss': 2.1395129668712616, 'learning_rate': 5.440200749405703e-06, 'epoch': 2.673587955035658, 'step': 2225000}
INFO:transformers.trainer:{'loss': 2.2598458050489425, 'learning_rate': 5.430187311371861e-06, 'epoch': 2.6741887613176885, 'step': 2225500}
INFO:transformers.trainer:{'loss': 2.2105410257577898, 'learning_rate': 5.42017387333802e-06, 'epoch': 2.674789567599719, 'step': 2226000}
INFO:transformers.trainer:{'loss': 2.217564046263695, 'learning_rate': 5.410160435304178e-06, 'epoch': 2.6753903738817493, 'step': 2226500}
INFO:transformers.trainer:{'loss': 2.225454990029335, 'learning_rate': 5.400146997270337e-06, 'epoch': 2.6759911801637797, 'step': 2227000}
INFO:transformers.trainer:{'loss': 2.202747996211052, 'learning_rate': 5.3901335592364956e-06, 'epoch': 2.6765919864458105, 'step': 2227500}
INFO:transformers.trainer:{'loss': 2.182731414079666, 'learning_rate': 5.380120121202654e-06, 'epoch': 2.6771927927278405, 'step': 2228000}
INFO:transformers.trainer:{'loss': 2.2644799455404283, 'learning_rate': 5.370106683168813e-06, 'epoch': 2.6777935990098714, 'step': 2228500}
INFO:transformers.trainer:{'loss': 2.2220182421803476, 'learning_rate': 5.360093245134971e-06, 'epoch': 2.6783944052919018, 'step': 2229000}
INFO:transformers.trainer:{'loss': 2.196341660141945, 'learning_rate': 5.35007980710113e-06, 'epoch': 2.678995211573932, 'step': 2229500}
INFO:transformers.trainer:{'loss': 2.170403706431389, 'learning_rate': 5.3400663690672885e-06, 'epoch': 2.6795960178559626, 'step': 2230000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2230000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2230000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2230000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2210000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1654115761518478, 'learning_rate': 5.330052931033446e-06, 'epoch': 2.680196824137993, 'step': 2230500}
INFO:transformers.trainer:{'loss': 2.1470191096663473, 'learning_rate': 5.320039492999606e-06, 'epoch': 2.680797630420024, 'step': 2231000}
INFO:transformers.trainer:{'loss': 2.217249255299568, 'learning_rate': 5.310026054965765e-06, 'epoch': 2.6813984367020542, 'step': 2231500}
INFO:transformers.trainer:{'loss': 2.2329200595617293, 'learning_rate': 5.300012616931923e-06, 'epoch': 2.6819992429840847, 'step': 2232000}
INFO:transformers.trainer:{'loss': 2.207634660124779, 'learning_rate': 5.2899991788980815e-06, 'epoch': 2.682600049266115, 'step': 2232500}
INFO:transformers.trainer:{'loss': 2.1850905084609984, 'learning_rate': 5.27998574086424e-06, 'epoch': 2.6832008555481455, 'step': 2233000}
INFO:transformers.trainer:{'loss': 2.204519648551941, 'learning_rate': 5.269972302830398e-06, 'epoch': 2.6838016618301763, 'step': 2233500}
INFO:transformers.trainer:{'loss': 2.223431413769722, 'learning_rate': 5.259958864796558e-06, 'epoch': 2.6844024681122067, 'step': 2234000}
INFO:transformers.trainer:{'loss': 2.2253287793397902, 'learning_rate': 5.249945426762716e-06, 'epoch': 2.685003274394237, 'step': 2234500}
INFO:transformers.trainer:{'loss': 2.186839282155037, 'learning_rate': 5.239931988728874e-06, 'epoch': 2.6856040806762675, 'step': 2235000}
INFO:transformers.trainer:{'loss': 2.239935281872749, 'learning_rate': 5.229918550695033e-06, 'epoch': 2.686204886958298, 'step': 2235500}
INFO:transformers.trainer:{'loss': 2.2736071808338165, 'learning_rate': 5.219905112661191e-06, 'epoch': 2.6868056932403284, 'step': 2236000}
INFO:transformers.trainer:{'loss': 2.1948650521337987, 'learning_rate': 5.20989167462735e-06, 'epoch': 2.6874064995223588, 'step': 2236500}
INFO:transformers.trainer:{'loss': 2.206958841204643, 'learning_rate': 5.1998782365935094e-06, 'epoch': 2.6880073058043896, 'step': 2237000}
INFO:transformers.trainer:{'loss': 2.2153831179142, 'learning_rate': 5.189864798559667e-06, 'epoch': 2.68860811208642, 'step': 2237500}
INFO:transformers.trainer:{'loss': 2.18099181830883, 'learning_rate': 5.179851360525826e-06, 'epoch': 2.6892089183684504, 'step': 2238000}
INFO:transformers.trainer:{'loss': 2.2074920068979265, 'learning_rate': 5.169837922491985e-06, 'epoch': 2.689809724650481, 'step': 2238500}
INFO:transformers.trainer:{'loss': 2.1827014664411544, 'learning_rate': 5.159824484458143e-06, 'epoch': 2.6904105309325113, 'step': 2239000}
INFO:transformers.trainer:{'loss': 2.2212704016566276, 'learning_rate': 5.1498110464243015e-06, 'epoch': 2.691011337214542, 'step': 2239500}
INFO:transformers.trainer:{'loss': 2.272371570587158, 'learning_rate': 5.13979760839046e-06, 'epoch': 2.6916121434965725, 'step': 2240000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2240000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2240000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2240000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2220000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2077856627702714, 'learning_rate': 5.129784170356619e-06, 'epoch': 2.692212949778603, 'step': 2240500}
INFO:transformers.trainer:{'loss': 2.191667033314705, 'learning_rate': 5.119770732322778e-06, 'epoch': 2.6928137560606333, 'step': 2241000}
INFO:transformers.trainer:{'loss': 2.264921993613243, 'learning_rate': 5.109757294288936e-06, 'epoch': 2.6934145623426637, 'step': 2241500}
INFO:transformers.trainer:{'loss': 2.2701966681480408, 'learning_rate': 5.0997438562550945e-06, 'epoch': 2.6940153686246946, 'step': 2242000}
INFO:transformers.trainer:{'loss': 2.2399166103601456, 'learning_rate': 5.089730418221253e-06, 'epoch': 2.6946161749067246, 'step': 2242500}
INFO:transformers.trainer:{'loss': 2.192375046312809, 'learning_rate': 5.079716980187411e-06, 'epoch': 2.6952169811887554, 'step': 2243000}
INFO:transformers.trainer:{'loss': 2.196699486732483, 'learning_rate': 5.069703542153571e-06, 'epoch': 2.695817787470786, 'step': 2243500}
INFO:transformers.trainer:{'loss': 2.2329516942501066, 'learning_rate': 5.059690104119729e-06, 'epoch': 2.6964185937528162, 'step': 2244000}
INFO:transformers.trainer:{'loss': 2.218232872605324, 'learning_rate': 5.049676666085887e-06, 'epoch': 2.6970194000348466, 'step': 2244500}
INFO:transformers.trainer:{'loss': 2.1635160492658616, 'learning_rate': 5.039663228052046e-06, 'epoch': 2.697620206316877, 'step': 2245000}
INFO:transformers.trainer:{'loss': 2.2138820707798006, 'learning_rate': 5.029649790018204e-06, 'epoch': 2.698221012598908, 'step': 2245500}
INFO:transformers.trainer:{'loss': 2.2236161277294157, 'learning_rate': 5.019636351984363e-06, 'epoch': 2.6988218188809383, 'step': 2246000}
INFO:transformers.trainer:{'loss': 2.162166668653488, 'learning_rate': 5.0096229139505224e-06, 'epoch': 2.6994226251629687, 'step': 2246500}
INFO:transformers.trainer:{'loss': 2.196880445718765, 'learning_rate': 4.99960947591668e-06, 'epoch': 2.700023431444999, 'step': 2247000}
INFO:transformers.trainer:{'loss': 2.201717139005661, 'learning_rate': 4.989596037882839e-06, 'epoch': 2.7006242377270295, 'step': 2247500}
INFO:transformers.trainer:{'loss': 2.1974131816625597, 'learning_rate': 4.979582599848998e-06, 'epoch': 2.7012250440090604, 'step': 2248000}
INFO:transformers.trainer:{'loss': 2.236519427895546, 'learning_rate': 4.969569161815156e-06, 'epoch': 2.701825850291091, 'step': 2248500}
INFO:transformers.trainer:{'loss': 2.2229084230661393, 'learning_rate': 4.9595557237813145e-06, 'epoch': 2.702426656573121, 'step': 2249000}
INFO:transformers.trainer:{'loss': 2.184279779911041, 'learning_rate': 4.949542285747473e-06, 'epoch': 2.7030274628551516, 'step': 2249500}
INFO:transformers.trainer:{'loss': 2.2207509911060335, 'learning_rate': 4.939528847713632e-06, 'epoch': 2.703628269137182, 'step': 2250000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2250000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2250000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2250000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2230000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2165579573512075, 'learning_rate': 4.929515409679791e-06, 'epoch': 2.7042290754192124, 'step': 2250500}
INFO:transformers.trainer:{'loss': 2.2283921744823454, 'learning_rate': 4.919501971645949e-06, 'epoch': 2.704829881701243, 'step': 2251000}
INFO:transformers.trainer:{'loss': 2.1829454646110533, 'learning_rate': 4.9094885336121075e-06, 'epoch': 2.7054306879832737, 'step': 2251500}
INFO:transformers.trainer:{'loss': 2.1768666048049927, 'learning_rate': 4.899475095578266e-06, 'epoch': 2.706031494265304, 'step': 2252000}
INFO:transformers.trainer:{'loss': 2.206928211569786, 'learning_rate': 4.889461657544424e-06, 'epoch': 2.7066323005473345, 'step': 2252500}
INFO:transformers.trainer:{'loss': 2.159548763155937, 'learning_rate': 4.879448219510584e-06, 'epoch': 2.707233106829365, 'step': 2253000}
INFO:transformers.trainer:{'loss': 2.175465918302536, 'learning_rate': 4.8694347814767425e-06, 'epoch': 2.7078339131113953, 'step': 2253500}
INFO:transformers.trainer:{'loss': 2.1565230993628504, 'learning_rate': 4.8594213434429004e-06, 'epoch': 2.708434719393426, 'step': 2254000}
INFO:transformers.trainer:{'loss': 2.1685676362514497, 'learning_rate': 4.849407905409059e-06, 'epoch': 2.7090355256754566, 'step': 2254500}
INFO:transformers.trainer:{'loss': 2.2326912450790406, 'learning_rate': 4.839394467375218e-06, 'epoch': 2.709636331957487, 'step': 2255000}
INFO:transformers.trainer:{'loss': 2.210370520114899, 'learning_rate': 4.829381029341376e-06, 'epoch': 2.7102371382395174, 'step': 2255500}
INFO:transformers.trainer:{'loss': 2.203547681212425, 'learning_rate': 4.8193675913075355e-06, 'epoch': 2.710837944521548, 'step': 2256000}
INFO:transformers.trainer:{'loss': 2.1744691957235336, 'learning_rate': 4.809354153273693e-06, 'epoch': 2.7114387508035787, 'step': 2256500}
INFO:transformers.trainer:{'loss': 2.230189005970955, 'learning_rate': 4.799340715239852e-06, 'epoch': 2.7120395570856086, 'step': 2257000}
INFO:transformers.trainer:{'loss': 2.196774822473526, 'learning_rate': 4.789327277206011e-06, 'epoch': 2.7126403633676395, 'step': 2257500}
INFO:transformers.trainer:{'loss': 2.1836567596793173, 'learning_rate': 4.779313839172169e-06, 'epoch': 2.71324116964967, 'step': 2258000}
INFO:transformers.trainer:{'loss': 2.2129112910032274, 'learning_rate': 4.7693004011383276e-06, 'epoch': 2.7138419759317003, 'step': 2258500}
INFO:transformers.trainer:{'loss': 2.2001609584093096, 'learning_rate': 4.759286963104486e-06, 'epoch': 2.7144427822137307, 'step': 2259000}
INFO:transformers.trainer:{'loss': 2.2317821180820463, 'learning_rate': 4.749273525070645e-06, 'epoch': 2.715043588495761, 'step': 2259500}
INFO:transformers.trainer:{'loss': 2.207580127358437, 'learning_rate': 4.739260087036804e-06, 'epoch': 2.715644394777792, 'step': 2260000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2260000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2260000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2260000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2240000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.218349965453148, 'learning_rate': 4.729246649002962e-06, 'epoch': 2.7162452010598224, 'step': 2260500}
INFO:transformers.trainer:{'loss': 2.1996122553348543, 'learning_rate': 4.7192332109691205e-06, 'epoch': 2.7168460073418528, 'step': 2261000}
INFO:transformers.trainer:{'loss': 2.25714553630352, 'learning_rate': 4.709219772935279e-06, 'epoch': 2.717446813623883, 'step': 2261500}
INFO:transformers.trainer:{'loss': 2.1418477276563643, 'learning_rate': 4.699206334901438e-06, 'epoch': 2.7180476199059136, 'step': 2262000}
INFO:transformers.trainer:{'loss': 2.192599722862244, 'learning_rate': 4.689192896867597e-06, 'epoch': 2.7186484261879444, 'step': 2262500}
INFO:transformers.trainer:{'loss': 2.183396954536438, 'learning_rate': 4.6791794588337555e-06, 'epoch': 2.719249232469975, 'step': 2263000}
INFO:transformers.trainer:{'loss': 2.269251280426979, 'learning_rate': 4.6691660207999135e-06, 'epoch': 2.7198500387520053, 'step': 2263500}
INFO:transformers.trainer:{'loss': 2.1785231615900993, 'learning_rate': 4.659152582766072e-06, 'epoch': 2.7204508450340357, 'step': 2264000}
INFO:transformers.trainer:{'loss': 2.2083796616792677, 'learning_rate': 4.649139144732231e-06, 'epoch': 2.721051651316066, 'step': 2264500}
INFO:transformers.trainer:{'loss': 2.2425246275067328, 'learning_rate': 4.63912570669839e-06, 'epoch': 2.7216524575980965, 'step': 2265000}
INFO:transformers.trainer:{'loss': 2.186241091012955, 'learning_rate': 4.6291122686645485e-06, 'epoch': 2.722253263880127, 'step': 2265500}
INFO:transformers.trainer:{'loss': 2.1435730826854704, 'learning_rate': 4.619098830630706e-06, 'epoch': 2.7228540701621577, 'step': 2266000}
INFO:transformers.trainer:{'loss': 2.2594660818576813, 'learning_rate': 4.609085392596865e-06, 'epoch': 2.723454876444188, 'step': 2266500}
INFO:transformers.trainer:{'loss': 2.1956987406313417, 'learning_rate': 4.599071954563024e-06, 'epoch': 2.7240556827262186, 'step': 2267000}
INFO:transformers.trainer:{'loss': 2.1760083988904952, 'learning_rate': 4.589058516529182e-06, 'epoch': 2.724656489008249, 'step': 2267500}
INFO:transformers.trainer:{'loss': 2.1600703957080842, 'learning_rate': 4.579045078495341e-06, 'epoch': 2.7252572952902794, 'step': 2268000}
INFO:transformers.trainer:{'loss': 2.248758826971054, 'learning_rate': 4.5690316404615e-06, 'epoch': 2.7258581015723102, 'step': 2268500}
INFO:transformers.trainer:{'loss': 2.190368065595627, 'learning_rate': 4.559018202427658e-06, 'epoch': 2.7264589078543406, 'step': 2269000}
INFO:transformers.trainer:{'loss': 2.236430530846119, 'learning_rate': 4.549004764393817e-06, 'epoch': 2.727059714136371, 'step': 2269500}
INFO:transformers.trainer:{'loss': 2.238234997034073, 'learning_rate': 4.538991326359976e-06, 'epoch': 2.7276605204184015, 'step': 2270000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2270000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2270000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2270000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2250000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1900345636606215, 'learning_rate': 4.5289778883261335e-06, 'epoch': 2.728261326700432, 'step': 2270500}
INFO:transformers.trainer:{'loss': 2.240582973361015, 'learning_rate': 4.518964450292292e-06, 'epoch': 2.7288621329824627, 'step': 2271000}
INFO:transformers.trainer:{'loss': 2.1959486684799194, 'learning_rate': 4.508951012258451e-06, 'epoch': 2.7294629392644927, 'step': 2271500}
INFO:transformers.trainer:{'loss': 2.1714759620577095, 'learning_rate': 4.49893757422461e-06, 'epoch': 2.7300637455465235, 'step': 2272000}
INFO:transformers.trainer:{'loss': 2.183593091905117, 'learning_rate': 4.4889241361907686e-06, 'epoch': 2.730664551828554, 'step': 2272500}
INFO:transformers.trainer:{'loss': 2.1548727926015854, 'learning_rate': 4.4789106981569265e-06, 'epoch': 2.7312653581105844, 'step': 2273000}
INFO:transformers.trainer:{'loss': 2.230300247609615, 'learning_rate': 4.468897260123085e-06, 'epoch': 2.7318661643926148, 'step': 2273500}
INFO:transformers.trainer:{'loss': 2.1981899732351304, 'learning_rate': 4.458883822089244e-06, 'epoch': 2.732466970674645, 'step': 2274000}
INFO:transformers.trainer:{'loss': 2.2105221557617187, 'learning_rate': 4.448870384055403e-06, 'epoch': 2.733067776956676, 'step': 2274500}
INFO:transformers.trainer:{'loss': 2.1963551148176195, 'learning_rate': 4.4388569460215615e-06, 'epoch': 2.7336685832387064, 'step': 2275000}
INFO:transformers.trainer:{'loss': 2.1948639324903487, 'learning_rate': 4.428843507987719e-06, 'epoch': 2.734269389520737, 'step': 2275500}
INFO:transformers.trainer:{'loss': 2.1781706633567812, 'learning_rate': 4.418830069953878e-06, 'epoch': 2.7348701958027672, 'step': 2276000}
INFO:transformers.trainer:{'loss': 2.133409824132919, 'learning_rate': 4.408816631920037e-06, 'epoch': 2.7354710020847977, 'step': 2276500}
INFO:transformers.trainer:{'loss': 2.225986352801323, 'learning_rate': 4.398803193886195e-06, 'epoch': 2.7360718083668285, 'step': 2277000}
INFO:transformers.trainer:{'loss': 2.173972199559212, 'learning_rate': 4.3887897558523544e-06, 'epoch': 2.736672614648859, 'step': 2277500}
INFO:transformers.trainer:{'loss': 2.2089260533899067, 'learning_rate': 4.378776317818513e-06, 'epoch': 2.7372734209308893, 'step': 2278000}
INFO:transformers.trainer:{'loss': 2.184227799654007, 'learning_rate': 4.368762879784671e-06, 'epoch': 2.7378742272129197, 'step': 2278500}
INFO:transformers.trainer:{'loss': 2.1900515393018725, 'learning_rate': 4.35874944175083e-06, 'epoch': 2.73847503349495, 'step': 2279000}
INFO:transformers.trainer:{'loss': 2.1533944479823113, 'learning_rate': 4.348736003716989e-06, 'epoch': 2.7390758397769805, 'step': 2279500}
INFO:transformers.trainer:{'loss': 2.1916943024396898, 'learning_rate': 4.3387225656831465e-06, 'epoch': 2.739676646059011, 'step': 2280000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2280000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2280000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2280000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2260000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.212613231539726, 'learning_rate': 4.328709127649305e-06, 'epoch': 2.740277452341042, 'step': 2280500}
INFO:transformers.trainer:{'loss': 2.2205215529203417, 'learning_rate': 4.318695689615464e-06, 'epoch': 2.740878258623072, 'step': 2281000}
INFO:transformers.trainer:{'loss': 2.228409625351429, 'learning_rate': 4.308682251581623e-06, 'epoch': 2.7414790649051026, 'step': 2281500}
INFO:transformers.trainer:{'loss': 2.1963914014697075, 'learning_rate': 4.298668813547782e-06, 'epoch': 2.742079871187133, 'step': 2282000}
INFO:transformers.trainer:{'loss': 2.2365216146707536, 'learning_rate': 4.2886553755139395e-06, 'epoch': 2.7426806774691634, 'step': 2282500}
INFO:transformers.trainer:{'loss': 2.207632555127144, 'learning_rate': 4.278641937480098e-06, 'epoch': 2.7432814837511943, 'step': 2283000}
INFO:transformers.trainer:{'loss': 2.2701340761184694, 'learning_rate': 4.268628499446257e-06, 'epoch': 2.7438822900332247, 'step': 2283500}
INFO:transformers.trainer:{'loss': 2.1615651876926423, 'learning_rate': 4.258615061412416e-06, 'epoch': 2.744483096315255, 'step': 2284000}
INFO:transformers.trainer:{'loss': 2.2354760249853136, 'learning_rate': 4.2486016233785745e-06, 'epoch': 2.7450839025972855, 'step': 2284500}
INFO:transformers.trainer:{'loss': 2.1540772062540054, 'learning_rate': 4.238588185344733e-06, 'epoch': 2.745684708879316, 'step': 2285000}
INFO:transformers.trainer:{'loss': 2.1541898728609086, 'learning_rate': 4.228574747310891e-06, 'epoch': 2.746285515161347, 'step': 2285500}
INFO:transformers.trainer:{'loss': 2.193474791884422, 'learning_rate': 4.21856130927705e-06, 'epoch': 2.7468863214433767, 'step': 2286000}
INFO:transformers.trainer:{'loss': 2.2235549437999724, 'learning_rate': 4.208547871243209e-06, 'epoch': 2.7474871277254076, 'step': 2286500}
INFO:transformers.trainer:{'loss': 2.220206809401512, 'learning_rate': 4.1985344332093675e-06, 'epoch': 2.748087934007438, 'step': 2287000}
INFO:transformers.trainer:{'loss': 2.228847179055214, 'learning_rate': 4.188520995175526e-06, 'epoch': 2.7486887402894684, 'step': 2287500}
INFO:transformers.trainer:{'loss': 2.200608299970627, 'learning_rate': 4.178507557141684e-06, 'epoch': 2.749289546571499, 'step': 2288000}
INFO:transformers.trainer:{'loss': 2.255176185488701, 'learning_rate': 4.168494119107843e-06, 'epoch': 2.7498903528535292, 'step': 2288500}
INFO:transformers.trainer:{'loss': 2.2108779706954955, 'learning_rate': 4.158480681074002e-06, 'epoch': 2.75049115913556, 'step': 2289000}
INFO:transformers.trainer:{'loss': 2.176823036670685, 'learning_rate': 4.1484672430401596e-06, 'epoch': 2.7510919654175905, 'step': 2289500}
INFO:transformers.trainer:{'loss': 2.201513231754303, 'learning_rate': 4.138453805006319e-06, 'epoch': 2.751692771699621, 'step': 2290000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2290000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2290000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2290000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2270000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2174438642263414, 'learning_rate': 4.128440366972477e-06, 'epoch': 2.7522935779816513, 'step': 2290500}
INFO:transformers.trainer:{'loss': 2.1936888227462767, 'learning_rate': 4.118426928938636e-06, 'epoch': 2.7528943842636817, 'step': 2291000}
INFO:transformers.trainer:{'loss': 2.1809636885523798, 'learning_rate': 4.108413490904795e-06, 'epoch': 2.7534951905457126, 'step': 2291500}
INFO:transformers.trainer:{'loss': 2.1403520101904867, 'learning_rate': 4.0984000528709525e-06, 'epoch': 2.754095996827743, 'step': 2292000}
INFO:transformers.trainer:{'loss': 2.2143795582056045, 'learning_rate': 4.088386614837111e-06, 'epoch': 2.7546968031097734, 'step': 2292500}
INFO:transformers.trainer:{'loss': 2.1972370282411577, 'learning_rate': 4.07837317680327e-06, 'epoch': 2.755297609391804, 'step': 2293000}
INFO:transformers.trainer:{'loss': 2.2088110303878783, 'learning_rate': 4.068359738769429e-06, 'epoch': 2.755898415673834, 'step': 2293500}
INFO:transformers.trainer:{'loss': 2.164375626325607, 'learning_rate': 4.0583463007355875e-06, 'epoch': 2.7564992219558646, 'step': 2294000}
INFO:transformers.trainer:{'loss': 2.217271686077118, 'learning_rate': 4.048332862701746e-06, 'epoch': 2.757100028237895, 'step': 2294500}
INFO:transformers.trainer:{'loss': 2.2100819216966627, 'learning_rate': 4.038319424667904e-06, 'epoch': 2.757700834519926, 'step': 2295000}
INFO:transformers.trainer:{'loss': 2.191993645310402, 'learning_rate': 4.028305986634063e-06, 'epoch': 2.7583016408019563, 'step': 2295500}
INFO:transformers.trainer:{'loss': 2.2207156958580017, 'learning_rate': 4.018292548600222e-06, 'epoch': 2.7589024470839867, 'step': 2296000}
INFO:transformers.trainer:{'loss': 2.1799052604436873, 'learning_rate': 4.0082791105663805e-06, 'epoch': 2.759503253366017, 'step': 2296500}
INFO:transformers.trainer:{'loss': 2.2159715336561203, 'learning_rate': 3.998265672532539e-06, 'epoch': 2.7601040596480475, 'step': 2297000}
INFO:transformers.trainer:{'loss': 2.248488079071045, 'learning_rate': 3.988252234498697e-06, 'epoch': 2.7607048659300784, 'step': 2297500}
INFO:transformers.trainer:{'loss': 2.185955905675888, 'learning_rate': 3.978238796464856e-06, 'epoch': 2.7613056722121088, 'step': 2298000}
INFO:transformers.trainer:{'loss': 2.2355021579265593, 'learning_rate': 3.968225358431015e-06, 'epoch': 2.761906478494139, 'step': 2298500}
INFO:transformers.trainer:{'loss': 2.221487544417381, 'learning_rate': 3.958211920397173e-06, 'epoch': 2.7625072847761696, 'step': 2299000}
INFO:transformers.trainer:{'loss': 2.141762316286564, 'learning_rate': 3.948198482363332e-06, 'epoch': 2.7631080910582, 'step': 2299500}
INFO:transformers.trainer:{'loss': 2.1737351991534233, 'learning_rate': 3.938185044329491e-06, 'epoch': 2.763708897340231, 'step': 2300000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2300000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2300000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2300000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2280000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1700921761989593, 'learning_rate': 3.928171606295649e-06, 'epoch': 2.764309703622261, 'step': 2300500}
INFO:transformers.trainer:{'loss': 2.219415094137192, 'learning_rate': 3.918158168261808e-06, 'epoch': 2.7649105099042917, 'step': 2301000}
INFO:transformers.trainer:{'loss': 2.193187405586243, 'learning_rate': 3.908144730227966e-06, 'epoch': 2.765511316186322, 'step': 2301500}
INFO:transformers.trainer:{'loss': 2.178512080073357, 'learning_rate': 3.898131292194124e-06, 'epoch': 2.7661121224683525, 'step': 2302000}
INFO:transformers.trainer:{'loss': 2.1665889558792113, 'learning_rate': 3.888117854160284e-06, 'epoch': 2.766712928750383, 'step': 2302500}
INFO:transformers.trainer:{'loss': 2.208982492685318, 'learning_rate': 3.878104416126442e-06, 'epoch': 2.7673137350324133, 'step': 2303000}
INFO:transformers.trainer:{'loss': 2.1973328577876092, 'learning_rate': 3.8680909780926006e-06, 'epoch': 2.767914541314444, 'step': 2303500}
INFO:transformers.trainer:{'loss': 2.1412968915104864, 'learning_rate': 3.858077540058759e-06, 'epoch': 2.7685153475964746, 'step': 2304000}
INFO:transformers.trainer:{'loss': 2.2105609152317047, 'learning_rate': 3.848064102024917e-06, 'epoch': 2.769116153878505, 'step': 2304500}
INFO:transformers.trainer:{'loss': 2.2227572102546693, 'learning_rate': 3.838050663991076e-06, 'epoch': 2.7697169601605354, 'step': 2305000}
INFO:transformers.trainer:{'loss': 2.202001857638359, 'learning_rate': 3.828037225957235e-06, 'epoch': 2.7703177664425658, 'step': 2305500}
INFO:transformers.trainer:{'loss': 2.225463183283806, 'learning_rate': 3.8180237879233935e-06, 'epoch': 2.7709185727245966, 'step': 2306000}
INFO:transformers.trainer:{'loss': 2.2273589382171632, 'learning_rate': 3.8080103498895523e-06, 'epoch': 2.771519379006627, 'step': 2306500}
INFO:transformers.trainer:{'loss': 2.178142319083214, 'learning_rate': 3.7979969118557106e-06, 'epoch': 2.7721201852886574, 'step': 2307000}
INFO:transformers.trainer:{'loss': 2.225234206557274, 'learning_rate': 3.787983473821869e-06, 'epoch': 2.772720991570688, 'step': 2307500}
INFO:transformers.trainer:{'loss': 2.2336186555624007, 'learning_rate': 3.7779700357880277e-06, 'epoch': 2.7733217978527183, 'step': 2308000}
INFO:transformers.trainer:{'loss': 2.1791914880275725, 'learning_rate': 3.767956597754186e-06, 'epoch': 2.7739226041347487, 'step': 2308500}
INFO:transformers.trainer:{'loss': 2.2179305676221848, 'learning_rate': 3.757943159720345e-06, 'epoch': 2.774523410416779, 'step': 2309000}
INFO:transformers.trainer:{'loss': 2.1778220627307894, 'learning_rate': 3.7479297216865035e-06, 'epoch': 2.77512421669881, 'step': 2309500}
INFO:transformers.trainer:{'loss': 2.196629665851593, 'learning_rate': 3.7379162836526623e-06, 'epoch': 2.7757250229808403, 'step': 2310000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2310000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2310000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2310000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2290000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.163508166670799, 'learning_rate': 3.7279028456188206e-06, 'epoch': 2.7763258292628707, 'step': 2310500}
INFO:transformers.trainer:{'loss': 2.1827143679857253, 'learning_rate': 3.717889407584979e-06, 'epoch': 2.776926635544901, 'step': 2311000}
INFO:transformers.trainer:{'loss': 2.191345138192177, 'learning_rate': 3.7078759695511377e-06, 'epoch': 2.7775274418269316, 'step': 2311500}
INFO:transformers.trainer:{'loss': 2.2130879294872283, 'learning_rate': 3.6978625315172965e-06, 'epoch': 2.7781282481089624, 'step': 2312000}
INFO:transformers.trainer:{'loss': 2.1971958190202714, 'learning_rate': 3.6878490934834552e-06, 'epoch': 2.778729054390993, 'step': 2312500}
INFO:transformers.trainer:{'loss': 2.19023353433609, 'learning_rate': 3.6778356554496136e-06, 'epoch': 2.7793298606730232, 'step': 2313000}
INFO:transformers.trainer:{'loss': 2.236951417326927, 'learning_rate': 3.667822217415772e-06, 'epoch': 2.7799306669550536, 'step': 2313500}
INFO:transformers.trainer:{'loss': 2.2173497161269187, 'learning_rate': 3.6578087793819307e-06, 'epoch': 2.780531473237084, 'step': 2314000}
INFO:transformers.trainer:{'loss': 2.1803147852420808, 'learning_rate': 3.647795341348089e-06, 'epoch': 2.781132279519115, 'step': 2314500}
INFO:transformers.trainer:{'loss': 2.169716397047043, 'learning_rate': 3.637781903314248e-06, 'epoch': 2.781733085801145, 'step': 2315000}
INFO:transformers.trainer:{'loss': 2.194798768758774, 'learning_rate': 3.6277684652804065e-06, 'epoch': 2.7823338920831757, 'step': 2315500}
INFO:transformers.trainer:{'loss': 2.1777327525615693, 'learning_rate': 3.6177550272465653e-06, 'epoch': 2.782934698365206, 'step': 2316000}
INFO:transformers.trainer:{'loss': 2.200664657473564, 'learning_rate': 3.6077415892127236e-06, 'epoch': 2.7835355046472365, 'step': 2316500}
INFO:transformers.trainer:{'loss': 2.208823904633522, 'learning_rate': 3.597728151178882e-06, 'epoch': 2.784136310929267, 'step': 2317000}
INFO:transformers.trainer:{'loss': 2.210175447225571, 'learning_rate': 3.5877147131450407e-06, 'epoch': 2.7847371172112974, 'step': 2317500}
INFO:transformers.trainer:{'loss': 2.2069393675327302, 'learning_rate': 3.577701275111199e-06, 'epoch': 2.785337923493328, 'step': 2318000}
INFO:transformers.trainer:{'loss': 2.2148774082660676, 'learning_rate': 3.5676878370773582e-06, 'epoch': 2.7859387297753586, 'step': 2318500}
INFO:transformers.trainer:{'loss': 2.129466073423624, 'learning_rate': 3.5576743990435166e-06, 'epoch': 2.786539536057389, 'step': 2319000}
INFO:transformers.trainer:{'loss': 2.1720583987236024, 'learning_rate': 3.5476609610096753e-06, 'epoch': 2.7871403423394194, 'step': 2319500}
INFO:transformers.trainer:{'loss': 2.185827957034111, 'learning_rate': 3.5376475229758337e-06, 'epoch': 2.78774114862145, 'step': 2320000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2320000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2320000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2320000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2300000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.16131812864542, 'learning_rate': 3.527634084941992e-06, 'epoch': 2.7883419549034807, 'step': 2320500}
INFO:transformers.trainer:{'loss': 2.223732407927513, 'learning_rate': 3.5176206469081508e-06, 'epoch': 2.788942761185511, 'step': 2321000}
INFO:transformers.trainer:{'loss': 2.193279183983803, 'learning_rate': 3.50760720887431e-06, 'epoch': 2.7895435674675415, 'step': 2321500}
INFO:transformers.trainer:{'loss': 2.206878665804863, 'learning_rate': 3.4975937708404683e-06, 'epoch': 2.790144373749572, 'step': 2322000}
INFO:transformers.trainer:{'loss': 2.198827725291252, 'learning_rate': 3.4875803328066266e-06, 'epoch': 2.7907451800316023, 'step': 2322500}
INFO:transformers.trainer:{'loss': 2.2155046504735947, 'learning_rate': 3.4775668947727854e-06, 'epoch': 2.7913459863136327, 'step': 2323000}
INFO:transformers.trainer:{'loss': 2.205770387291908, 'learning_rate': 3.4675534567389437e-06, 'epoch': 2.791946792595663, 'step': 2323500}
INFO:transformers.trainer:{'loss': 2.177404754281044, 'learning_rate': 3.457540018705102e-06, 'epoch': 2.792547598877694, 'step': 2324000}
INFO:transformers.trainer:{'loss': 2.1680418847799303, 'learning_rate': 3.4475265806712612e-06, 'epoch': 2.7931484051597244, 'step': 2324500}
INFO:transformers.trainer:{'loss': 2.1776627066135408, 'learning_rate': 3.43751314263742e-06, 'epoch': 2.793749211441755, 'step': 2325000}
INFO:transformers.trainer:{'loss': 2.2296095657348634, 'learning_rate': 3.4274997046035783e-06, 'epoch': 2.794350017723785, 'step': 2325500}
INFO:transformers.trainer:{'loss': 2.207411995410919, 'learning_rate': 3.4174862665697366e-06, 'epoch': 2.7949508240058156, 'step': 2326000}
INFO:transformers.trainer:{'loss': 2.156634571015835, 'learning_rate': 3.4074728285358954e-06, 'epoch': 2.7955516302878465, 'step': 2326500}
INFO:transformers.trainer:{'loss': 2.219769621491432, 'learning_rate': 3.3974593905020537e-06, 'epoch': 2.796152436569877, 'step': 2327000}
INFO:transformers.trainer:{'loss': 2.23314061999321, 'learning_rate': 3.387445952468213e-06, 'epoch': 2.7967532428519073, 'step': 2327500}
INFO:transformers.trainer:{'loss': 2.163482001423836, 'learning_rate': 3.3774325144343712e-06, 'epoch': 2.7973540491339377, 'step': 2328000}
INFO:transformers.trainer:{'loss': 2.1833098074197768, 'learning_rate': 3.3674190764005296e-06, 'epoch': 2.797954855415968, 'step': 2328500}
INFO:transformers.trainer:{'loss': 2.186469534277916, 'learning_rate': 3.3574056383666883e-06, 'epoch': 2.798555661697999, 'step': 2329000}
INFO:transformers.trainer:{'loss': 2.1857473425865175, 'learning_rate': 3.3473922003328467e-06, 'epoch': 2.799156467980029, 'step': 2329500}
INFO:transformers.trainer:{'loss': 2.2407006157636644, 'learning_rate': 3.337378762299005e-06, 'epoch': 2.79975727426206, 'step': 2330000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2330000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2330000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2330000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2310000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.219129478096962, 'learning_rate': 3.3273653242651638e-06, 'epoch': 2.80035808054409, 'step': 2330500}
INFO:transformers.trainer:{'loss': 2.248031204819679, 'learning_rate': 3.317351886231323e-06, 'epoch': 2.8009588868261206, 'step': 2331000}
INFO:transformers.trainer:{'loss': 2.1870238432884217, 'learning_rate': 3.3073384481974813e-06, 'epoch': 2.801559693108151, 'step': 2331500}
INFO:transformers.trainer:{'loss': 2.264500559091568, 'learning_rate': 3.2973250101636396e-06, 'epoch': 2.8021604993901814, 'step': 2332000}
INFO:transformers.trainer:{'loss': 2.175346549630165, 'learning_rate': 3.2873115721297984e-06, 'epoch': 2.8027613056722123, 'step': 2332500}
INFO:transformers.trainer:{'loss': 2.2059866676926614, 'learning_rate': 3.2772981340959567e-06, 'epoch': 2.8033621119542427, 'step': 2333000}
INFO:transformers.trainer:{'loss': 2.1824187086820603, 'learning_rate': 3.267284696062115e-06, 'epoch': 2.803962918236273, 'step': 2333500}
INFO:transformers.trainer:{'loss': 2.142325627744198, 'learning_rate': 3.2572712580282742e-06, 'epoch': 2.8045637245183035, 'step': 2334000}
INFO:transformers.trainer:{'loss': 2.226983258128166, 'learning_rate': 3.247257819994433e-06, 'epoch': 2.805164530800334, 'step': 2334500}
INFO:transformers.trainer:{'loss': 2.2251253964304922, 'learning_rate': 3.2372443819605913e-06, 'epoch': 2.8057653370823648, 'step': 2335000}
INFO:transformers.trainer:{'loss': 2.1892984560728075, 'learning_rate': 3.2272309439267497e-06, 'epoch': 2.806366143364395, 'step': 2335500}
INFO:transformers.trainer:{'loss': 2.1869822630882263, 'learning_rate': 3.2172175058929084e-06, 'epoch': 2.8069669496464256, 'step': 2336000}
INFO:transformers.trainer:{'loss': 2.1880729619264603, 'learning_rate': 3.2072040678590668e-06, 'epoch': 2.807567755928456, 'step': 2336500}
INFO:transformers.trainer:{'loss': 2.195404928088188, 'learning_rate': 3.197190629825226e-06, 'epoch': 2.8081685622104864, 'step': 2337000}
INFO:transformers.trainer:{'loss': 2.15343749332428, 'learning_rate': 3.1871771917913843e-06, 'epoch': 2.808769368492517, 'step': 2337500}
INFO:transformers.trainer:{'loss': 2.1591027847528457, 'learning_rate': 3.177163753757543e-06, 'epoch': 2.809370174774547, 'step': 2338000}
INFO:transformers.trainer:{'loss': 2.197423236966133, 'learning_rate': 3.1671503157237014e-06, 'epoch': 2.809970981056578, 'step': 2338500}
INFO:transformers.trainer:{'loss': 2.2092379289865494, 'learning_rate': 3.1571368776898597e-06, 'epoch': 2.8105717873386085, 'step': 2339000}
INFO:transformers.trainer:{'loss': 2.2422266454696653, 'learning_rate': 3.1471234396560185e-06, 'epoch': 2.811172593620639, 'step': 2339500}
INFO:transformers.trainer:{'loss': 2.236880110025406, 'learning_rate': 3.1371100016221776e-06, 'epoch': 2.8117733999026693, 'step': 2340000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2340000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2340000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2340000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2320000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.198291309595108, 'learning_rate': 3.127096563588336e-06, 'epoch': 2.8123742061846997, 'step': 2340500}
INFO:transformers.trainer:{'loss': 2.178749223589897, 'learning_rate': 3.1170831255544943e-06, 'epoch': 2.8129750124667305, 'step': 2341000}
INFO:transformers.trainer:{'loss': 2.212200247645378, 'learning_rate': 3.107069687520653e-06, 'epoch': 2.813575818748761, 'step': 2341500}
INFO:transformers.trainer:{'loss': 2.1960796966552736, 'learning_rate': 3.0970562494868114e-06, 'epoch': 2.8141766250307914, 'step': 2342000}
INFO:transformers.trainer:{'loss': 2.237258871436119, 'learning_rate': 3.08704281145297e-06, 'epoch': 2.8147774313128218, 'step': 2342500}
INFO:transformers.trainer:{'loss': 2.186176784455776, 'learning_rate': 3.0770293734191285e-06, 'epoch': 2.815378237594852, 'step': 2343000}
INFO:transformers.trainer:{'loss': 2.1601011234521867, 'learning_rate': 3.0670159353852873e-06, 'epoch': 2.815979043876883, 'step': 2343500}
INFO:transformers.trainer:{'loss': 2.1712936131954192, 'learning_rate': 3.057002497351446e-06, 'epoch': 2.816579850158913, 'step': 2344000}
INFO:transformers.trainer:{'loss': 2.182659153521061, 'learning_rate': 3.0469890593176043e-06, 'epoch': 2.817180656440944, 'step': 2344500}
INFO:transformers.trainer:{'loss': 2.25338667845726, 'learning_rate': 3.0369756212837627e-06, 'epoch': 2.8177814627229743, 'step': 2345000}
INFO:transformers.trainer:{'loss': 2.222532575368881, 'learning_rate': 3.026962183249922e-06, 'epoch': 2.8183822690050047, 'step': 2345500}
INFO:transformers.trainer:{'loss': 2.2341319653987886, 'learning_rate': 3.01694874521608e-06, 'epoch': 2.818983075287035, 'step': 2346000}
INFO:transformers.trainer:{'loss': 2.207699746966362, 'learning_rate': 3.0069353071822385e-06, 'epoch': 2.8195838815690655, 'step': 2346500}
INFO:transformers.trainer:{'loss': 2.168853539824486, 'learning_rate': 2.9969218691483973e-06, 'epoch': 2.8201846878510963, 'step': 2347000}
INFO:transformers.trainer:{'loss': 2.1677427580356596, 'learning_rate': 2.986908431114556e-06, 'epoch': 2.8207854941331267, 'step': 2347500}
INFO:transformers.trainer:{'loss': 2.2018001452684404, 'learning_rate': 2.9768949930807144e-06, 'epoch': 2.821386300415157, 'step': 2348000}
INFO:transformers.trainer:{'loss': 2.17877225330472, 'learning_rate': 2.9668815550468727e-06, 'epoch': 2.8219871066971876, 'step': 2348500}
INFO:transformers.trainer:{'loss': 2.2247147971987724, 'learning_rate': 2.956868117013032e-06, 'epoch': 2.822587912979218, 'step': 2349000}
INFO:transformers.trainer:{'loss': 2.176132256984711, 'learning_rate': 2.9468546789791902e-06, 'epoch': 2.823188719261249, 'step': 2349500}
INFO:transformers.trainer:{'loss': 2.1894261600971223, 'learning_rate': 2.9368412409453486e-06, 'epoch': 2.823789525543279, 'step': 2350000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2350000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2350000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2350000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2330000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.168692119240761, 'learning_rate': 2.9268278029115073e-06, 'epoch': 2.8243903318253096, 'step': 2350500}
INFO:transformers.trainer:{'loss': 2.1826925061941145, 'learning_rate': 2.916814364877666e-06, 'epoch': 2.82499113810734, 'step': 2351000}
INFO:transformers.trainer:{'loss': 2.2347872363328936, 'learning_rate': 2.9068009268438244e-06, 'epoch': 2.8255919443893704, 'step': 2351500}
INFO:transformers.trainer:{'loss': 2.166256752371788, 'learning_rate': 2.896787488809983e-06, 'epoch': 2.826192750671401, 'step': 2352000}
INFO:transformers.trainer:{'loss': 2.214969861268997, 'learning_rate': 2.8867740507761415e-06, 'epoch': 2.8267935569534313, 'step': 2352500}
INFO:transformers.trainer:{'loss': 2.1928917354345323, 'learning_rate': 2.8767606127423003e-06, 'epoch': 2.827394363235462, 'step': 2353000}
INFO:transformers.trainer:{'loss': 2.2134248214960097, 'learning_rate': 2.866747174708459e-06, 'epoch': 2.8279951695174925, 'step': 2353500}
INFO:transformers.trainer:{'loss': 2.1973018157482147, 'learning_rate': 2.8567337366746174e-06, 'epoch': 2.828595975799523, 'step': 2354000}
INFO:transformers.trainer:{'loss': 2.224125537097454, 'learning_rate': 2.846720298640776e-06, 'epoch': 2.8291967820815533, 'step': 2354500}
INFO:transformers.trainer:{'loss': 2.1274841840267182, 'learning_rate': 2.836706860606935e-06, 'epoch': 2.8297975883635837, 'step': 2355000}
INFO:transformers.trainer:{'loss': 2.1918261677026747, 'learning_rate': 2.8266934225730932e-06, 'epoch': 2.8303983946456146, 'step': 2355500}
INFO:transformers.trainer:{'loss': 2.1781621837615965, 'learning_rate': 2.8166799845392515e-06, 'epoch': 2.830999200927645, 'step': 2356000}
INFO:transformers.trainer:{'loss': 2.2021164034605025, 'learning_rate': 2.8066665465054107e-06, 'epoch': 2.8316000072096754, 'step': 2356500}
INFO:transformers.trainer:{'loss': 2.2279362639188767, 'learning_rate': 2.796653108471569e-06, 'epoch': 2.832200813491706, 'step': 2357000}
INFO:transformers.trainer:{'loss': 2.1532437089681626, 'learning_rate': 2.7866396704377274e-06, 'epoch': 2.8328016197737362, 'step': 2357500}
INFO:transformers.trainer:{'loss': 2.1671464458703995, 'learning_rate': 2.776626232403886e-06, 'epoch': 2.833402426055767, 'step': 2358000}
INFO:transformers.trainer:{'loss': 2.2149667588472366, 'learning_rate': 2.766612794370045e-06, 'epoch': 2.834003232337797, 'step': 2358500}
INFO:transformers.trainer:{'loss': 2.1909262762069703, 'learning_rate': 2.7565993563362033e-06, 'epoch': 2.834604038619828, 'step': 2359000}
INFO:transformers.trainer:{'loss': 2.1715184171199797, 'learning_rate': 2.746585918302362e-06, 'epoch': 2.8352048449018583, 'step': 2359500}
INFO:transformers.trainer:{'loss': 2.2183980338573455, 'learning_rate': 2.7365724802685203e-06, 'epoch': 2.8358056511838887, 'step': 2360000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2360000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2360000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2360000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2340000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1865779759585857, 'learning_rate': 2.726559042234679e-06, 'epoch': 2.836406457465919, 'step': 2360500}
INFO:transformers.trainer:{'loss': 2.2077295882701873, 'learning_rate': 2.716545604200838e-06, 'epoch': 2.8370072637479495, 'step': 2361000}
INFO:transformers.trainer:{'loss': 2.207756438970566, 'learning_rate': 2.706532166166996e-06, 'epoch': 2.8376080700299804, 'step': 2361500}
INFO:transformers.trainer:{'loss': 2.2113790808916094, 'learning_rate': 2.696518728133155e-06, 'epoch': 2.838208876312011, 'step': 2362000}
INFO:transformers.trainer:{'loss': 2.1525818498134615, 'learning_rate': 2.6865052900993133e-06, 'epoch': 2.838809682594041, 'step': 2362500}
INFO:transformers.trainer:{'loss': 2.1897355551719664, 'learning_rate': 2.676491852065472e-06, 'epoch': 2.8394104888760716, 'step': 2363000}
INFO:transformers.trainer:{'loss': 2.1983830051422117, 'learning_rate': 2.6664784140316304e-06, 'epoch': 2.840011295158102, 'step': 2363500}
INFO:transformers.trainer:{'loss': 2.1872789605855942, 'learning_rate': 2.656464975997789e-06, 'epoch': 2.840612101440133, 'step': 2364000}
INFO:transformers.trainer:{'loss': 2.1986327783465387, 'learning_rate': 2.646451537963948e-06, 'epoch': 2.8412129077221633, 'step': 2364500}
INFO:transformers.trainer:{'loss': 2.187576721906662, 'learning_rate': 2.6364380999301062e-06, 'epoch': 2.8418137140041937, 'step': 2365000}
INFO:transformers.trainer:{'loss': 2.1594797121286393, 'learning_rate': 2.6264246618962646e-06, 'epoch': 2.842414520286224, 'step': 2365500}
INFO:transformers.trainer:{'loss': 2.1560749117732048, 'learning_rate': 2.6164112238624238e-06, 'epoch': 2.8430153265682545, 'step': 2366000}
INFO:transformers.trainer:{'loss': 2.189079791665077, 'learning_rate': 2.606397785828582e-06, 'epoch': 2.843616132850285, 'step': 2366500}
INFO:transformers.trainer:{'loss': 2.2039693727493286, 'learning_rate': 2.5963843477947404e-06, 'epoch': 2.8442169391323153, 'step': 2367000}
INFO:transformers.trainer:{'loss': 2.1946109331846237, 'learning_rate': 2.586370909760899e-06, 'epoch': 2.844817745414346, 'step': 2367500}
INFO:transformers.trainer:{'loss': 2.1536397558450697, 'learning_rate': 2.576357471727058e-06, 'epoch': 2.8454185516963766, 'step': 2368000}
INFO:transformers.trainer:{'loss': 2.119995144486427, 'learning_rate': 2.5663440336932163e-06, 'epoch': 2.846019357978407, 'step': 2368500}
INFO:transformers.trainer:{'loss': 2.1950948767662046, 'learning_rate': 2.556330595659375e-06, 'epoch': 2.8466201642604374, 'step': 2369000}
INFO:transformers.trainer:{'loss': 2.1863343292474746, 'learning_rate': 2.546317157625534e-06, 'epoch': 2.847220970542468, 'step': 2369500}
INFO:transformers.trainer:{'loss': 2.1460192823410034, 'learning_rate': 2.536303719591692e-06, 'epoch': 2.8478217768244987, 'step': 2370000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2370000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2370000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2370000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2350000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1277591931819915, 'learning_rate': 2.526290281557851e-06, 'epoch': 2.848422583106529, 'step': 2370500}
INFO:transformers.trainer:{'loss': 2.144353089928627, 'learning_rate': 2.5162768435240092e-06, 'epoch': 2.8490233893885595, 'step': 2371000}
INFO:transformers.trainer:{'loss': 2.207311148047447, 'learning_rate': 2.506263405490168e-06, 'epoch': 2.84962419567059, 'step': 2371500}
INFO:transformers.trainer:{'loss': 2.1894139519929885, 'learning_rate': 2.4962499674563267e-06, 'epoch': 2.8502250019526203, 'step': 2372000}
INFO:transformers.trainer:{'loss': 2.1734689046144484, 'learning_rate': 2.486236529422485e-06, 'epoch': 2.850825808234651, 'step': 2372500}
INFO:transformers.trainer:{'loss': 2.207940976858139, 'learning_rate': 2.4762230913886434e-06, 'epoch': 2.851426614516681, 'step': 2373000}
INFO:transformers.trainer:{'loss': 2.1864222960472106, 'learning_rate': 2.4662096533548026e-06, 'epoch': 2.852027420798712, 'step': 2373500}
INFO:transformers.trainer:{'loss': 2.2084807925224306, 'learning_rate': 2.456196215320961e-06, 'epoch': 2.8526282270807424, 'step': 2374000}
INFO:transformers.trainer:{'loss': 2.158150451898575, 'learning_rate': 2.4461827772871193e-06, 'epoch': 2.853229033362773, 'step': 2374500}
INFO:transformers.trainer:{'loss': 2.172475315570831, 'learning_rate': 2.436169339253278e-06, 'epoch': 2.853829839644803, 'step': 2375000}
INFO:transformers.trainer:{'loss': 2.142343880057335, 'learning_rate': 2.4261559012194368e-06, 'epoch': 2.8544306459268336, 'step': 2375500}
INFO:transformers.trainer:{'loss': 2.210401125788689, 'learning_rate': 2.416142463185595e-06, 'epoch': 2.8550314522088645, 'step': 2376000}
INFO:transformers.trainer:{'loss': 2.246501541495323, 'learning_rate': 2.4061290251517534e-06, 'epoch': 2.855632258490895, 'step': 2376500}
INFO:transformers.trainer:{'loss': 2.194134257614613, 'learning_rate': 2.3961155871179126e-06, 'epoch': 2.8562330647729253, 'step': 2377000}
INFO:transformers.trainer:{'loss': 2.17476909160614, 'learning_rate': 2.386102149084071e-06, 'epoch': 2.8568338710549557, 'step': 2377500}
INFO:transformers.trainer:{'loss': 2.1971294023990633, 'learning_rate': 2.3760887110502293e-06, 'epoch': 2.857434677336986, 'step': 2378000}
INFO:transformers.trainer:{'loss': 2.163820811867714, 'learning_rate': 2.366075273016388e-06, 'epoch': 2.858035483619017, 'step': 2378500}
INFO:transformers.trainer:{'loss': 2.16883492898941, 'learning_rate': 2.356061834982547e-06, 'epoch': 2.8586362899010473, 'step': 2379000}
INFO:transformers.trainer:{'loss': 2.198551448225975, 'learning_rate': 2.346048396948705e-06, 'epoch': 2.8592370961830778, 'step': 2379500}
INFO:transformers.trainer:{'loss': 2.2139959218502043, 'learning_rate': 2.336034958914864e-06, 'epoch': 2.859837902465108, 'step': 2380000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2380000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2380000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2380000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2360000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.177893619894981, 'learning_rate': 2.3260215208810222e-06, 'epoch': 2.8604387087471386, 'step': 2380500}
INFO:transformers.trainer:{'loss': 2.102305884599686, 'learning_rate': 2.316008082847181e-06, 'epoch': 2.8610395150291694, 'step': 2381000}
INFO:transformers.trainer:{'loss': 2.10653549516201, 'learning_rate': 2.3059946448133398e-06, 'epoch': 2.8616403213111994, 'step': 2381500}
INFO:transformers.trainer:{'loss': 2.13316664814949, 'learning_rate': 2.295981206779498e-06, 'epoch': 2.8622411275932302, 'step': 2382000}
INFO:transformers.trainer:{'loss': 2.1858116171360016, 'learning_rate': 2.285967768745657e-06, 'epoch': 2.8628419338752606, 'step': 2382500}
INFO:transformers.trainer:{'loss': 2.187800480246544, 'learning_rate': 2.2759543307118156e-06, 'epoch': 2.863442740157291, 'step': 2383000}
INFO:transformers.trainer:{'loss': 2.210911116719246, 'learning_rate': 2.265940892677974e-06, 'epoch': 2.8640435464393215, 'step': 2383500}
INFO:transformers.trainer:{'loss': 2.199258099079132, 'learning_rate': 2.2559274546441323e-06, 'epoch': 2.864644352721352, 'step': 2384000}
INFO:transformers.trainer:{'loss': 2.1321269122362136, 'learning_rate': 2.2459140166102915e-06, 'epoch': 2.8652451590033827, 'step': 2384500}
INFO:transformers.trainer:{'loss': 2.226487101793289, 'learning_rate': 2.23590057857645e-06, 'epoch': 2.865845965285413, 'step': 2385000}
INFO:transformers.trainer:{'loss': 2.1552669398188593, 'learning_rate': 2.225887140542608e-06, 'epoch': 2.8664467715674435, 'step': 2385500}
INFO:transformers.trainer:{'loss': 2.12431359064579, 'learning_rate': 2.215873702508767e-06, 'epoch': 2.867047577849474, 'step': 2386000}
INFO:transformers.trainer:{'loss': 2.1895940507650375, 'learning_rate': 2.2058602644749256e-06, 'epoch': 2.8676483841315044, 'step': 2386500}
INFO:transformers.trainer:{'loss': 2.1928936536312102, 'learning_rate': 2.195846826441084e-06, 'epoch': 2.868249190413535, 'step': 2387000}
INFO:transformers.trainer:{'loss': 2.151710785150528, 'learning_rate': 2.1858333884072423e-06, 'epoch': 2.868849996695565, 'step': 2387500}
INFO:transformers.trainer:{'loss': 2.181374531030655, 'learning_rate': 2.175819950373401e-06, 'epoch': 2.869450802977596, 'step': 2388000}
INFO:transformers.trainer:{'loss': 2.175714309334755, 'learning_rate': 2.16580651233956e-06, 'epoch': 2.8700516092596264, 'step': 2388500}
INFO:transformers.trainer:{'loss': 2.255750244140625, 'learning_rate': 2.155793074305718e-06, 'epoch': 2.870652415541657, 'step': 2389000}
INFO:transformers.trainer:{'loss': 2.1940003420114516, 'learning_rate': 2.145779636271877e-06, 'epoch': 2.8712532218236873, 'step': 2389500}
INFO:transformers.trainer:{'loss': 2.173419613778591, 'learning_rate': 2.1357661982380357e-06, 'epoch': 2.8718540281057177, 'step': 2390000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2390000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2390000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2390000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2370000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.222640654325485, 'learning_rate': 2.125752760204194e-06, 'epoch': 2.8724548343877485, 'step': 2390500}
INFO:transformers.trainer:{'loss': 2.1415579721927642, 'learning_rate': 2.1157393221703528e-06, 'epoch': 2.873055640669779, 'step': 2391000}
INFO:transformers.trainer:{'loss': 2.2037206292152405, 'learning_rate': 2.105725884136511e-06, 'epoch': 2.8736564469518093, 'step': 2391500}
INFO:transformers.trainer:{'loss': 2.2178664467334746, 'learning_rate': 2.09571244610267e-06, 'epoch': 2.8742572532338397, 'step': 2392000}
INFO:transformers.trainer:{'loss': 2.200817890167236, 'learning_rate': 2.0856990080688286e-06, 'epoch': 2.87485805951587, 'step': 2392500}
INFO:transformers.trainer:{'loss': 2.194899633049965, 'learning_rate': 2.075685570034987e-06, 'epoch': 2.875458865797901, 'step': 2393000}
INFO:transformers.trainer:{'loss': 2.1528570837974548, 'learning_rate': 2.0656721320011457e-06, 'epoch': 2.8760596720799314, 'step': 2393500}
INFO:transformers.trainer:{'loss': 2.1740541647672655, 'learning_rate': 2.0556586939673045e-06, 'epoch': 2.876660478361962, 'step': 2394000}
INFO:transformers.trainer:{'loss': 2.1783164781332016, 'learning_rate': 2.045645255933463e-06, 'epoch': 2.8772612846439922, 'step': 2394500}
INFO:transformers.trainer:{'loss': 2.183706981897354, 'learning_rate': 2.035631817899621e-06, 'epoch': 2.8778620909260226, 'step': 2395000}
INFO:transformers.trainer:{'loss': 2.159509041070938, 'learning_rate': 2.02561837986578e-06, 'epoch': 2.8784628972080535, 'step': 2395500}
INFO:transformers.trainer:{'loss': 2.1818910392522812, 'learning_rate': 2.0156049418319387e-06, 'epoch': 2.8790637034900834, 'step': 2396000}
INFO:transformers.trainer:{'loss': 2.131474083721638, 'learning_rate': 2.005591503798097e-06, 'epoch': 2.8796645097721143, 'step': 2396500}
INFO:transformers.trainer:{'loss': 2.1695937197208406, 'learning_rate': 1.9955780657642558e-06, 'epoch': 2.8802653160541447, 'step': 2397000}
INFO:transformers.trainer:{'loss': 2.2039594184160234, 'learning_rate': 1.9855646277304145e-06, 'epoch': 2.880866122336175, 'step': 2397500}
INFO:transformers.trainer:{'loss': 2.191320481300354, 'learning_rate': 1.975551189696573e-06, 'epoch': 2.8814669286182055, 'step': 2398000}
INFO:transformers.trainer:{'loss': 2.2136355308294298, 'learning_rate': 1.9655377516627316e-06, 'epoch': 2.882067734900236, 'step': 2398500}
INFO:transformers.trainer:{'loss': 2.208952274084091, 'learning_rate': 1.95552431362889e-06, 'epoch': 2.882668541182267, 'step': 2399000}
INFO:transformers.trainer:{'loss': 2.187807373166084, 'learning_rate': 1.9455108755950487e-06, 'epoch': 2.883269347464297, 'step': 2399500}
INFO:transformers.trainer:{'loss': 2.2052748839855196, 'learning_rate': 1.935497437561207e-06, 'epoch': 2.8838701537463276, 'step': 2400000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2400000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2400000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2400000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2380000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1478053854703902, 'learning_rate': 1.925483999527366e-06, 'epoch': 2.884470960028358, 'step': 2400500}
INFO:transformers.trainer:{'loss': 2.206964810013771, 'learning_rate': 1.9154705614935245e-06, 'epoch': 2.8850717663103884, 'step': 2401000}
INFO:transformers.trainer:{'loss': 2.155129665851593, 'learning_rate': 1.9054571234596829e-06, 'epoch': 2.8856725725924193, 'step': 2401500}
INFO:transformers.trainer:{'loss': 2.1489896144866942, 'learning_rate': 1.8954436854258416e-06, 'epoch': 2.8862733788744492, 'step': 2402000}
INFO:transformers.trainer:{'loss': 2.20514701795578, 'learning_rate': 1.8854302473920002e-06, 'epoch': 2.88687418515648, 'step': 2402500}
INFO:transformers.trainer:{'loss': 2.225038468956947, 'learning_rate': 1.8754168093581585e-06, 'epoch': 2.8874749914385105, 'step': 2403000}
INFO:transformers.trainer:{'loss': 2.191981335759163, 'learning_rate': 1.8654033713243175e-06, 'epoch': 2.888075797720541, 'step': 2403500}
INFO:transformers.trainer:{'loss': 2.220072393655777, 'learning_rate': 1.8553899332904758e-06, 'epoch': 2.8886766040025713, 'step': 2404000}
INFO:transformers.trainer:{'loss': 2.174598918914795, 'learning_rate': 1.8453764952566344e-06, 'epoch': 2.8892774102846017, 'step': 2404500}
INFO:transformers.trainer:{'loss': 2.172176326394081, 'learning_rate': 1.8353630572227931e-06, 'epoch': 2.8898782165666326, 'step': 2405000}
INFO:transformers.trainer:{'loss': 2.1663773099184036, 'learning_rate': 1.8253496191889517e-06, 'epoch': 2.890479022848663, 'step': 2405500}
INFO:transformers.trainer:{'loss': 2.154862332344055, 'learning_rate': 1.8153361811551102e-06, 'epoch': 2.8910798291306934, 'step': 2406000}
INFO:transformers.trainer:{'loss': 2.191316749095917, 'learning_rate': 1.805322743121269e-06, 'epoch': 2.891680635412724, 'step': 2406500}
INFO:transformers.trainer:{'loss': 2.1836140192747115, 'learning_rate': 1.7953093050874273e-06, 'epoch': 2.892281441694754, 'step': 2407000}
INFO:transformers.trainer:{'loss': 2.180309160113335, 'learning_rate': 1.7852958670535859e-06, 'epoch': 2.892882247976785, 'step': 2407500}
INFO:transformers.trainer:{'loss': 2.200483450293541, 'learning_rate': 1.7752824290197446e-06, 'epoch': 2.8934830542588155, 'step': 2408000}
INFO:transformers.trainer:{'loss': 2.1850297023057936, 'learning_rate': 1.7652689909859032e-06, 'epoch': 2.894083860540846, 'step': 2408500}
INFO:transformers.trainer:{'loss': 2.1930287247896194, 'learning_rate': 1.7552555529520617e-06, 'epoch': 2.8946846668228763, 'step': 2409000}
INFO:transformers.trainer:{'loss': 2.163478202700615, 'learning_rate': 1.7452421149182205e-06, 'epoch': 2.8952854731049067, 'step': 2409500}
INFO:transformers.trainer:{'loss': 2.2008676011562347, 'learning_rate': 1.735228676884379e-06, 'epoch': 2.8958862793869375, 'step': 2410000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2410000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2410000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2410000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2390000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.203860503435135, 'learning_rate': 1.7252152388505374e-06, 'epoch': 2.8964870856689675, 'step': 2410500}
INFO:transformers.trainer:{'loss': 2.1681122238636017, 'learning_rate': 1.7152018008166963e-06, 'epoch': 2.8970878919509984, 'step': 2411000}
INFO:transformers.trainer:{'loss': 2.1797942111492157, 'learning_rate': 1.7051883627828547e-06, 'epoch': 2.8976886982330288, 'step': 2411500}
INFO:transformers.trainer:{'loss': 2.1430095505714415, 'learning_rate': 1.6951749247490132e-06, 'epoch': 2.898289504515059, 'step': 2412000}
INFO:transformers.trainer:{'loss': 2.239230621755123, 'learning_rate': 1.6851614867151718e-06, 'epoch': 2.8988903107970896, 'step': 2412500}
INFO:transformers.trainer:{'loss': 2.151928602039814, 'learning_rate': 1.6751480486813305e-06, 'epoch': 2.89949111707912, 'step': 2413000}
INFO:transformers.trainer:{'loss': 2.1861319533586503, 'learning_rate': 1.665134610647489e-06, 'epoch': 2.900091923361151, 'step': 2413500}
INFO:transformers.trainer:{'loss': 2.1620389178991317, 'learning_rate': 1.6551211726136474e-06, 'epoch': 2.9006927296431813, 'step': 2414000}
INFO:transformers.trainer:{'loss': 2.209243864297867, 'learning_rate': 1.6451077345798062e-06, 'epoch': 2.9012935359252117, 'step': 2414500}
INFO:transformers.trainer:{'loss': 2.1724566075801848, 'learning_rate': 1.6350942965459647e-06, 'epoch': 2.901894342207242, 'step': 2415000}
INFO:transformers.trainer:{'loss': 2.1971940088272093, 'learning_rate': 1.6250808585121232e-06, 'epoch': 2.9024951484892725, 'step': 2415500}
INFO:transformers.trainer:{'loss': 2.1912351620197295, 'learning_rate': 1.615067420478282e-06, 'epoch': 2.9030959547713033, 'step': 2416000}
INFO:transformers.trainer:{'loss': 2.189987866282463, 'learning_rate': 1.6050539824444406e-06, 'epoch': 2.9036967610533333, 'step': 2416500}
INFO:transformers.trainer:{'loss': 2.1992324734926223, 'learning_rate': 1.5950405444105989e-06, 'epoch': 2.904297567335364, 'step': 2417000}
INFO:transformers.trainer:{'loss': 2.218443536520004, 'learning_rate': 1.5850271063767579e-06, 'epoch': 2.9048983736173946, 'step': 2417500}
INFO:transformers.trainer:{'loss': 2.2032853145599365, 'learning_rate': 1.5750136683429162e-06, 'epoch': 2.905499179899425, 'step': 2418000}
INFO:transformers.trainer:{'loss': 2.187456506371498, 'learning_rate': 1.5650002303090747e-06, 'epoch': 2.9060999861814554, 'step': 2418500}
INFO:transformers.trainer:{'loss': 2.1980212655067444, 'learning_rate': 1.5549867922752333e-06, 'epoch': 2.906700792463486, 'step': 2419000}
INFO:transformers.trainer:{'loss': 2.1670345833301545, 'learning_rate': 1.544973354241392e-06, 'epoch': 2.9073015987455166, 'step': 2419500}
INFO:transformers.trainer:{'loss': 2.2224655847549437, 'learning_rate': 1.5349599162075506e-06, 'epoch': 2.907902405027547, 'step': 2420000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2420000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2420000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2420000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2400000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1918420077562333, 'learning_rate': 1.5249464781737091e-06, 'epoch': 2.9085032113095775, 'step': 2420500}
INFO:transformers.trainer:{'loss': 2.1792485568523405, 'learning_rate': 1.5149330401398679e-06, 'epoch': 2.909104017591608, 'step': 2421000}
INFO:transformers.trainer:{'loss': 2.204545893549919, 'learning_rate': 1.5049196021060264e-06, 'epoch': 2.9097048238736383, 'step': 2421500}
INFO:transformers.trainer:{'loss': 2.2047292656302453, 'learning_rate': 1.494906164072185e-06, 'epoch': 2.910305630155669, 'step': 2422000}
INFO:transformers.trainer:{'loss': 2.153640809059143, 'learning_rate': 1.4848927260383435e-06, 'epoch': 2.9109064364376995, 'step': 2422500}
INFO:transformers.trainer:{'loss': 2.2123730565309523, 'learning_rate': 1.474879288004502e-06, 'epoch': 2.91150724271973, 'step': 2423000}
INFO:transformers.trainer:{'loss': 2.1562562962770464, 'learning_rate': 1.4648658499706606e-06, 'epoch': 2.9121080490017603, 'step': 2423500}
INFO:transformers.trainer:{'loss': 2.189032853126526, 'learning_rate': 1.4548524119368194e-06, 'epoch': 2.9127088552837908, 'step': 2424000}
INFO:transformers.trainer:{'loss': 2.195491795539856, 'learning_rate': 1.4448389739029777e-06, 'epoch': 2.9133096615658216, 'step': 2424500}
INFO:transformers.trainer:{'loss': 2.193520009279251, 'learning_rate': 1.4348255358691365e-06, 'epoch': 2.9139104678478516, 'step': 2425000}
INFO:transformers.trainer:{'loss': 2.1165466992259026, 'learning_rate': 1.424812097835295e-06, 'epoch': 2.9145112741298824, 'step': 2425500}
INFO:transformers.trainer:{'loss': 2.169197228074074, 'learning_rate': 1.4147986598014536e-06, 'epoch': 2.915112080411913, 'step': 2426000}
INFO:transformers.trainer:{'loss': 2.169981511235237, 'learning_rate': 1.4047852217676121e-06, 'epoch': 2.9157128866939432, 'step': 2426500}
INFO:transformers.trainer:{'loss': 2.1075056816339495, 'learning_rate': 1.3947717837337709e-06, 'epoch': 2.9163136929759736, 'step': 2427000}
INFO:transformers.trainer:{'loss': 2.2198500561714174, 'learning_rate': 1.3847583456999294e-06, 'epoch': 2.916914499258004, 'step': 2427500}
INFO:transformers.trainer:{'loss': 2.1738453842401504, 'learning_rate': 1.374744907666088e-06, 'epoch': 2.917515305540035, 'step': 2428000}
INFO:transformers.trainer:{'loss': 2.181271471142769, 'learning_rate': 1.3647314696322465e-06, 'epoch': 2.9181161118220653, 'step': 2428500}
INFO:transformers.trainer:{'loss': 2.2043676359653475, 'learning_rate': 1.354718031598405e-06, 'epoch': 2.9187169181040957, 'step': 2429000}
INFO:transformers.trainer:{'loss': 2.11837397480011, 'learning_rate': 1.3447045935645638e-06, 'epoch': 2.919317724386126, 'step': 2429500}
INFO:transformers.trainer:{'loss': 2.1772818895578383, 'learning_rate': 1.3346911555307222e-06, 'epoch': 2.9199185306681565, 'step': 2430000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2430000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2430000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2430000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2410000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.2127034657001494, 'learning_rate': 1.324677717496881e-06, 'epoch': 2.9205193369501874, 'step': 2430500}
INFO:transformers.trainer:{'loss': 2.168997595667839, 'learning_rate': 1.3146642794630395e-06, 'epoch': 2.9211201432322174, 'step': 2431000}
INFO:transformers.trainer:{'loss': 2.2021839788556097, 'learning_rate': 1.304650841429198e-06, 'epoch': 2.921720949514248, 'step': 2431500}
INFO:transformers.trainer:{'loss': 2.1546356679201124, 'learning_rate': 1.2946374033953566e-06, 'epoch': 2.9223217557962786, 'step': 2432000}
INFO:transformers.trainer:{'loss': 2.1927738225460054, 'learning_rate': 1.2846239653615153e-06, 'epoch': 2.922922562078309, 'step': 2432500}
INFO:transformers.trainer:{'loss': 2.1938527746200562, 'learning_rate': 1.2746105273276739e-06, 'epoch': 2.9235233683603394, 'step': 2433000}
INFO:transformers.trainer:{'loss': 2.197008389353752, 'learning_rate': 1.2645970892938324e-06, 'epoch': 2.92412417464237, 'step': 2433500}
INFO:transformers.trainer:{'loss': 2.1484020466804505, 'learning_rate': 1.254583651259991e-06, 'epoch': 2.9247249809244007, 'step': 2434000}
INFO:transformers.trainer:{'loss': 2.1785546916723253, 'learning_rate': 1.2445702132261495e-06, 'epoch': 2.925325787206431, 'step': 2434500}
INFO:transformers.trainer:{'loss': 2.130037643969059, 'learning_rate': 1.2345567751923083e-06, 'epoch': 2.9259265934884615, 'step': 2435000}
INFO:transformers.trainer:{'loss': 2.21204534304142, 'learning_rate': 1.2245433371584666e-06, 'epoch': 2.926527399770492, 'step': 2435500}
INFO:transformers.trainer:{'loss': 2.1609599152803423, 'learning_rate': 1.2145298991246253e-06, 'epoch': 2.9271282060525223, 'step': 2436000}
INFO:transformers.trainer:{'loss': 2.1642502468824385, 'learning_rate': 1.204516461090784e-06, 'epoch': 2.927729012334553, 'step': 2436500}
INFO:transformers.trainer:{'loss': 2.168909643650055, 'learning_rate': 1.1945030230569424e-06, 'epoch': 2.9283298186165836, 'step': 2437000}
INFO:transformers.trainer:{'loss': 2.1617159323096273, 'learning_rate': 1.184489585023101e-06, 'epoch': 2.928930624898614, 'step': 2437500}
INFO:transformers.trainer:{'loss': 2.1629265159368516, 'learning_rate': 1.1744761469892597e-06, 'epoch': 2.9295314311806444, 'step': 2438000}
INFO:transformers.trainer:{'loss': 2.195300632119179, 'learning_rate': 1.164462708955418e-06, 'epoch': 2.930132237462675, 'step': 2438500}
INFO:transformers.trainer:{'loss': 2.2195535081326963, 'learning_rate': 1.1544492709215768e-06, 'epoch': 2.9307330437447057, 'step': 2439000}
INFO:transformers.trainer:{'loss': 2.1570773082971573, 'learning_rate': 1.1444358328877354e-06, 'epoch': 2.9313338500267356, 'step': 2439500}
INFO:transformers.trainer:{'loss': 2.1686127986907957, 'learning_rate': 1.134422394853894e-06, 'epoch': 2.9319346563087665, 'step': 2440000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2440000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2440000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2440000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2420000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1695135424733163, 'learning_rate': 1.1244089568200527e-06, 'epoch': 2.932535462590797, 'step': 2440500}
INFO:transformers.trainer:{'loss': 2.186255357384682, 'learning_rate': 1.1143955187862112e-06, 'epoch': 2.9331362688728273, 'step': 2441000}
INFO:transformers.trainer:{'loss': 2.1463210694789887, 'learning_rate': 1.1043820807523698e-06, 'epoch': 2.9337370751548577, 'step': 2441500}
INFO:transformers.trainer:{'loss': 2.200935268521309, 'learning_rate': 1.0943686427185283e-06, 'epoch': 2.934337881436888, 'step': 2442000}
INFO:transformers.trainer:{'loss': 2.216617588162422, 'learning_rate': 1.0843552046846869e-06, 'epoch': 2.934938687718919, 'step': 2442500}
INFO:transformers.trainer:{'loss': 2.1986538672447207, 'learning_rate': 1.0743417666508454e-06, 'epoch': 2.9355394940009494, 'step': 2443000}
INFO:transformers.trainer:{'loss': 2.1961690833568572, 'learning_rate': 1.0643283286170042e-06, 'epoch': 2.93614030028298, 'step': 2443500}
INFO:transformers.trainer:{'loss': 2.1973299170732496, 'learning_rate': 1.0543148905831625e-06, 'epoch': 2.93674110656501, 'step': 2444000}
INFO:transformers.trainer:{'loss': 2.140378700017929, 'learning_rate': 1.0443014525493213e-06, 'epoch': 2.9373419128470406, 'step': 2444500}
INFO:transformers.trainer:{'loss': 2.156545680522919, 'learning_rate': 1.0342880145154798e-06, 'epoch': 2.9379427191290715, 'step': 2445000}
INFO:transformers.trainer:{'loss': 2.176825914859772, 'learning_rate': 1.0242745764816384e-06, 'epoch': 2.938543525411102, 'step': 2445500}
INFO:transformers.trainer:{'loss': 2.1897119863033296, 'learning_rate': 1.014261138447797e-06, 'epoch': 2.9391443316931323, 'step': 2446000}
INFO:transformers.trainer:{'loss': 2.173143987059593, 'learning_rate': 1.0042477004139557e-06, 'epoch': 2.9397451379751627, 'step': 2446500}
INFO:transformers.trainer:{'loss': 2.242425024628639, 'learning_rate': 9.942342623801142e-07, 'epoch': 2.940345944257193, 'step': 2447000}
INFO:transformers.trainer:{'loss': 2.155958151817322, 'learning_rate': 9.842208243462728e-07, 'epoch': 2.9409467505392235, 'step': 2447500}
INFO:transformers.trainer:{'loss': 2.1406847759485244, 'learning_rate': 9.742073863124313e-07, 'epoch': 2.941547556821254, 'step': 2448000}
INFO:transformers.trainer:{'loss': 2.177071383476257, 'learning_rate': 9.641939482785899e-07, 'epoch': 2.9421483631032848, 'step': 2448500}
INFO:transformers.trainer:{'loss': 2.2068558403253555, 'learning_rate': 9.541805102447486e-07, 'epoch': 2.942749169385315, 'step': 2449000}
INFO:transformers.trainer:{'loss': 2.1705707404613497, 'learning_rate': 9.441670722109071e-07, 'epoch': 2.9433499756673456, 'step': 2449500}
INFO:transformers.trainer:{'loss': 2.1814515548944473, 'learning_rate': 9.341536341770657e-07, 'epoch': 2.943950781949376, 'step': 2450000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2450000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2450000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2450000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2430000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1827265326976777, 'learning_rate': 9.241401961432243e-07, 'epoch': 2.9445515882314064, 'step': 2450500}
INFO:transformers.trainer:{'loss': 2.188228038072586, 'learning_rate': 9.141267581093828e-07, 'epoch': 2.9451523945134372, 'step': 2451000}
INFO:transformers.trainer:{'loss': 2.2122171227931977, 'learning_rate': 9.041133200755415e-07, 'epoch': 2.9457532007954677, 'step': 2451500}
INFO:transformers.trainer:{'loss': 2.2252625924348832, 'learning_rate': 8.940998820417001e-07, 'epoch': 2.946354007077498, 'step': 2452000}
INFO:transformers.trainer:{'loss': 2.2048610944747926, 'learning_rate': 8.840864440078585e-07, 'epoch': 2.9469548133595285, 'step': 2452500}
INFO:transformers.trainer:{'loss': 2.163138826727867, 'learning_rate': 8.740730059740172e-07, 'epoch': 2.947555619641559, 'step': 2453000}
INFO:transformers.trainer:{'loss': 2.1439533565640447, 'learning_rate': 8.640595679401759e-07, 'epoch': 2.9481564259235897, 'step': 2453500}
INFO:transformers.trainer:{'loss': 2.176984900593758, 'learning_rate': 8.540461299063343e-07, 'epoch': 2.9487572322056197, 'step': 2454000}
INFO:transformers.trainer:{'loss': 2.1589706428050994, 'learning_rate': 8.440326918724929e-07, 'epoch': 2.9493580384876505, 'step': 2454500}
INFO:transformers.trainer:{'loss': 2.1676788758039476, 'learning_rate': 8.340192538386514e-07, 'epoch': 2.949958844769681, 'step': 2455000}
INFO:transformers.trainer:{'loss': 2.1658986014127732, 'learning_rate': 8.2400581580481e-07, 'epoch': 2.9505596510517114, 'step': 2455500}
INFO:transformers.trainer:{'loss': 2.169833317279816, 'learning_rate': 8.139923777709687e-07, 'epoch': 2.9511604573337418, 'step': 2456000}
INFO:transformers.trainer:{'loss': 2.142711472570896, 'learning_rate': 8.039789397371272e-07, 'epoch': 2.951761263615772, 'step': 2456500}
INFO:transformers.trainer:{'loss': 2.1999758538007734, 'learning_rate': 7.939655017032859e-07, 'epoch': 2.952362069897803, 'step': 2457000}
INFO:transformers.trainer:{'loss': 2.193366478204727, 'learning_rate': 7.839520636694445e-07, 'epoch': 2.9529628761798334, 'step': 2457500}
INFO:transformers.trainer:{'loss': 2.1606054871082305, 'learning_rate': 7.739386256356031e-07, 'epoch': 2.953563682461864, 'step': 2458000}
INFO:transformers.trainer:{'loss': 2.1361923355460166, 'learning_rate': 7.639251876017616e-07, 'epoch': 2.9541644887438943, 'step': 2458500}
INFO:transformers.trainer:{'loss': 2.1587735514640807, 'learning_rate': 7.539117495679202e-07, 'epoch': 2.9547652950259247, 'step': 2459000}
INFO:transformers.trainer:{'loss': 2.1995965022444723, 'learning_rate': 7.438983115340787e-07, 'epoch': 2.9553661013079555, 'step': 2459500}
INFO:transformers.trainer:{'loss': 2.1573670796453954, 'learning_rate': 7.338848735002374e-07, 'epoch': 2.955966907589986, 'step': 2460000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2460000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2460000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2460000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2440000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.206649321436882, 'learning_rate': 7.238714354663959e-07, 'epoch': 2.9565677138720163, 'step': 2460500}
INFO:transformers.trainer:{'loss': 2.1942034004926683, 'learning_rate': 7.138579974325545e-07, 'epoch': 2.9571685201540467, 'step': 2461000}
INFO:transformers.trainer:{'loss': 2.197721373796463, 'learning_rate': 7.038445593987131e-07, 'epoch': 2.957769326436077, 'step': 2461500}
INFO:transformers.trainer:{'loss': 2.207824983358383, 'learning_rate': 6.938311213648717e-07, 'epoch': 2.9583701327181076, 'step': 2462000}
INFO:transformers.trainer:{'loss': 2.171304684996605, 'learning_rate': 6.838176833310302e-07, 'epoch': 2.958970939000138, 'step': 2462500}
INFO:transformers.trainer:{'loss': 2.160435942053795, 'learning_rate': 6.738042452971889e-07, 'epoch': 2.959571745282169, 'step': 2463000}
INFO:transformers.trainer:{'loss': 2.1903919353485106, 'learning_rate': 6.637908072633475e-07, 'epoch': 2.9601725515641992, 'step': 2463500}
INFO:transformers.trainer:{'loss': 2.181715576529503, 'learning_rate': 6.537773692295061e-07, 'epoch': 2.9607733578462296, 'step': 2464000}
INFO:transformers.trainer:{'loss': 2.161567937493324, 'learning_rate': 6.437639311956646e-07, 'epoch': 2.96137416412826, 'step': 2464500}
INFO:transformers.trainer:{'loss': 2.157201635479927, 'learning_rate': 6.337504931618233e-07, 'epoch': 2.9619749704102905, 'step': 2465000}
INFO:transformers.trainer:{'loss': 2.1424927788972856, 'learning_rate': 6.237370551279818e-07, 'epoch': 2.9625757766923213, 'step': 2465500}
INFO:transformers.trainer:{'loss': 2.1461662422418595, 'learning_rate': 6.137236170941404e-07, 'epoch': 2.9631765829743517, 'step': 2466000}
INFO:transformers.trainer:{'loss': 2.2255818165540697, 'learning_rate': 6.037101790602989e-07, 'epoch': 2.963777389256382, 'step': 2466500}
INFO:transformers.trainer:{'loss': 2.2011545140445232, 'learning_rate': 5.936967410264576e-07, 'epoch': 2.9643781955384125, 'step': 2467000}
INFO:transformers.trainer:{'loss': 2.1779916666746137, 'learning_rate': 5.836833029926161e-07, 'epoch': 2.964979001820443, 'step': 2467500}
INFO:transformers.trainer:{'loss': 2.1557204979658127, 'learning_rate': 5.736698649587747e-07, 'epoch': 2.965579808102474, 'step': 2468000}
INFO:transformers.trainer:{'loss': 2.19825526368618, 'learning_rate': 5.636564269249332e-07, 'epoch': 2.9661806143845038, 'step': 2468500}
INFO:transformers.trainer:{'loss': 2.205980763733387, 'learning_rate': 5.536429888910919e-07, 'epoch': 2.9667814206665346, 'step': 2469000}
INFO:transformers.trainer:{'loss': 2.1469620180130007, 'learning_rate': 5.436295508572504e-07, 'epoch': 2.967382226948565, 'step': 2469500}
INFO:transformers.trainer:{'loss': 2.1702562325000763, 'learning_rate': 5.336161128234091e-07, 'epoch': 2.9679830332305954, 'step': 2470000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2470000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2470000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2470000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2450000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1947590754032134, 'learning_rate': 5.236026747895677e-07, 'epoch': 2.968583839512626, 'step': 2470500}
INFO:transformers.trainer:{'loss': 2.17485031914711, 'learning_rate': 5.135892367557263e-07, 'epoch': 2.9691846457946562, 'step': 2471000}
INFO:transformers.trainer:{'loss': 2.1675370209217073, 'learning_rate': 5.035757987218848e-07, 'epoch': 2.969785452076687, 'step': 2471500}
INFO:transformers.trainer:{'loss': 2.211246886253357, 'learning_rate': 4.935623606880433e-07, 'epoch': 2.9703862583587175, 'step': 2472000}
INFO:transformers.trainer:{'loss': 2.212486415684223, 'learning_rate': 4.83548922654202e-07, 'epoch': 2.970987064640748, 'step': 2472500}
INFO:transformers.trainer:{'loss': 2.1964736627340318, 'learning_rate': 4.7353548462036054e-07, 'epoch': 2.9715878709227783, 'step': 2473000}
INFO:transformers.trainer:{'loss': 2.137422917842865, 'learning_rate': 4.635220465865191e-07, 'epoch': 2.9721886772048087, 'step': 2473500}
INFO:transformers.trainer:{'loss': 2.196244579911232, 'learning_rate': 4.5350860855267774e-07, 'epoch': 2.9727894834868396, 'step': 2474000}
INFO:transformers.trainer:{'loss': 2.1572047262191774, 'learning_rate': 4.434951705188363e-07, 'epoch': 2.97339028976887, 'step': 2474500}
INFO:transformers.trainer:{'loss': 2.1508691298961637, 'learning_rate': 4.334817324849949e-07, 'epoch': 2.9739910960509004, 'step': 2475000}
INFO:transformers.trainer:{'loss': 2.213141560077667, 'learning_rate': 4.2346829445115344e-07, 'epoch': 2.974591902332931, 'step': 2475500}
INFO:transformers.trainer:{'loss': 2.148143541097641, 'learning_rate': 4.134548564173121e-07, 'epoch': 2.975192708614961, 'step': 2476000}
INFO:transformers.trainer:{'loss': 2.119611179947853, 'learning_rate': 4.0344141838347064e-07, 'epoch': 2.9757935148969916, 'step': 2476500}
INFO:transformers.trainer:{'loss': 2.202195093154907, 'learning_rate': 3.934279803496292e-07, 'epoch': 2.976394321179022, 'step': 2477000}
INFO:transformers.trainer:{'loss': 2.170121011734009, 'learning_rate': 3.834145423157878e-07, 'epoch': 2.976995127461053, 'step': 2477500}
INFO:transformers.trainer:{'loss': 2.1501438415050504, 'learning_rate': 3.734011042819464e-07, 'epoch': 2.9775959337430833, 'step': 2478000}
INFO:transformers.trainer:{'loss': 2.1128614785671234, 'learning_rate': 3.63387666248105e-07, 'epoch': 2.9781967400251137, 'step': 2478500}
INFO:transformers.trainer:{'loss': 2.1082533259391782, 'learning_rate': 3.533742282142636e-07, 'epoch': 2.978797546307144, 'step': 2479000}
INFO:transformers.trainer:{'loss': 2.1744947657585145, 'learning_rate': 3.433607901804221e-07, 'epoch': 2.9793983525891745, 'step': 2479500}
INFO:transformers.trainer:{'loss': 2.1354715789556504, 'learning_rate': 3.333473521465807e-07, 'epoch': 2.9799991588712054, 'step': 2480000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2480000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2480000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2480000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2460000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1895431134700774, 'learning_rate': 3.2333391411273927e-07, 'epoch': 2.9805999651532358, 'step': 2480500}
INFO:transformers.trainer:{'loss': 2.208173665046692, 'learning_rate': 3.1332047607889787e-07, 'epoch': 2.981200771435266, 'step': 2481000}
INFO:transformers.trainer:{'loss': 2.175124487042427, 'learning_rate': 3.033070380450565e-07, 'epoch': 2.9818015777172966, 'step': 2481500}
INFO:transformers.trainer:{'loss': 2.241840494990349, 'learning_rate': 2.9329360001121507e-07, 'epoch': 2.982402383999327, 'step': 2482000}
INFO:transformers.trainer:{'loss': 2.1505315692424776, 'learning_rate': 2.8328016197737367e-07, 'epoch': 2.983003190281358, 'step': 2482500}
INFO:transformers.trainer:{'loss': 2.1685010679960253, 'learning_rate': 2.732667239435322e-07, 'epoch': 2.983603996563388, 'step': 2483000}
INFO:transformers.trainer:{'loss': 2.14641796708107, 'learning_rate': 2.632532859096908e-07, 'epoch': 2.9842048028454187, 'step': 2483500}
INFO:transformers.trainer:{'loss': 2.171956799507141, 'learning_rate': 2.5323984787584936e-07, 'epoch': 2.984805609127449, 'step': 2484000}
INFO:transformers.trainer:{'loss': 2.1860881642103194, 'learning_rate': 2.4322640984200796e-07, 'epoch': 2.9854064154094795, 'step': 2484500}
INFO:transformers.trainer:{'loss': 2.146122621178627, 'learning_rate': 2.332129718081666e-07, 'epoch': 2.98600722169151, 'step': 2485000}
INFO:transformers.trainer:{'loss': 2.183598104476929, 'learning_rate': 2.2319953377432516e-07, 'epoch': 2.9866080279735403, 'step': 2485500}
INFO:transformers.trainer:{'loss': 2.13286925470829, 'learning_rate': 2.1318609574048376e-07, 'epoch': 2.987208834255571, 'step': 2486000}
INFO:transformers.trainer:{'loss': 2.1753450372219088, 'learning_rate': 2.031726577066423e-07, 'epoch': 2.9878096405376016, 'step': 2486500}
INFO:transformers.trainer:{'loss': 2.2024201697111128, 'learning_rate': 1.931592196728009e-07, 'epoch': 2.988410446819632, 'step': 2487000}
INFO:transformers.trainer:{'loss': 2.1943144830465315, 'learning_rate': 1.831457816389595e-07, 'epoch': 2.9890112531016624, 'step': 2487500}
INFO:transformers.trainer:{'loss': 2.2156280732154845, 'learning_rate': 1.7313234360511808e-07, 'epoch': 2.989612059383693, 'step': 2488000}
INFO:transformers.trainer:{'loss': 2.140612644433975, 'learning_rate': 1.6311890557127665e-07, 'epoch': 2.9902128656657236, 'step': 2488500}
INFO:transformers.trainer:{'loss': 2.116709619283676, 'learning_rate': 1.5310546753743525e-07, 'epoch': 2.990813671947754, 'step': 2489000}
INFO:transformers.trainer:{'loss': 2.196740676820278, 'learning_rate': 1.4309202950359382e-07, 'epoch': 2.9914144782297845, 'step': 2489500}
INFO:transformers.trainer:{'loss': 2.1864748771190645, 'learning_rate': 1.330785914697524e-07, 'epoch': 2.992015284511815, 'step': 2490000}
INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft/checkpoint-2490000
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/checkpoint-2490000/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/checkpoint-2490000/pytorch_model.bin
INFO:transformers.trainer:Deleting older checkpoint [../outputs/edin_tuned/bert-ft/checkpoint-2470000] due to args.save_total_limit
INFO:transformers.trainer:{'loss': 2.1386345126628874, 'learning_rate': 1.23065153435911e-07, 'epoch': 2.9926160907938453, 'step': 2490500}
INFO:transformers.trainer:{'loss': 2.1954340000152586, 'learning_rate': 1.130517154020696e-07, 'epoch': 2.9932168970758757, 'step': 2491000}
INFO:transformers.trainer:{'loss': 2.2009409668445588, 'learning_rate': 1.0303827736822817e-07, 'epoch': 2.993817703357906, 'step': 2491500}
INFO:transformers.trainer:{'loss': 2.145612557053566, 'learning_rate': 9.302483933438676e-08, 'epoch': 2.994418509639937, 'step': 2492000}
INFO:transformers.trainer:{'loss': 2.1544948620796203, 'learning_rate': 8.301140130054534e-08, 'epoch': 2.9950193159219674, 'step': 2492500}
INFO:transformers.trainer:{'loss': 2.230990404367447, 'learning_rate': 7.299796326670392e-08, 'epoch': 2.9956201222039978, 'step': 2493000}
INFO:transformers.trainer:{'loss': 2.171047080039978, 'learning_rate': 6.29845252328625e-08, 'epoch': 2.996220928486028, 'step': 2493500}
INFO:transformers.trainer:{'loss': 2.16094828915596, 'learning_rate': 5.2971087199021094e-08, 'epoch': 2.9968217347680586, 'step': 2494000}
INFO:transformers.trainer:{'loss': 2.150404299378395, 'learning_rate': 4.2957649165179674e-08, 'epoch': 2.9974225410500894, 'step': 2494500}
INFO:transformers.trainer:{'loss': 2.1916592972278597, 'learning_rate': 3.2944211131338254e-08, 'epoch': 2.99802334733212, 'step': 2495000}
INFO:transformers.trainer:{'loss': 2.2601095476150515, 'learning_rate': 2.293077309749684e-08, 'epoch': 2.9986241536141502, 'step': 2495500}
INFO:transformers.trainer:{'loss': 2.1770978233814238, 'learning_rate': 1.2917335063655426e-08, 'epoch': 2.9992249598961807, 'step': 2496000}
INFO:transformers.trainer:{'loss': 2.1570496723651886, 'learning_rate': 2.90389702981401e-09, 'epoch': 2.999825766178211, 'step': 2496500}
INFO:transformers.trainer:

Training completed. Do not forget to share your model on huggingface.co/models =)


INFO:transformers.trainer:Saving model checkpoint to ../outputs/edin_tuned/bert-ft
INFO:transformers.configuration_utils:Configuration saved in ../outputs/edin_tuned/bert-ft/config.json
INFO:transformers.modeling_utils:Model weights saved in ../outputs/edin_tuned/bert-ft/pytorch_model.bin
INFO:transformers.modelcard:Model card: {
  "caveats_and_recommendations": {},
  "ethical_considerations": {},
  "evaluation_data": {},
  "factors": {},
  "intended_use": {},
  "metrics": {},
  "model_details": {},
  "quantitative_analyses": {},
  "training_data": {}
}

INFO:transformers.configuration_utils:loading configuration file ../outputs/edin_tuned/bert-ft/config.json
INFO:transformers.configuration_utils:Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file ../outputs/edin_tuned/bert-ft/pytorch_model.bin
INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertForMaskedLM.

INFO:transformers.modeling_utils:All the weights of BertForMaskedLM were initialized from the model checkpoint at ../outputs/edin_tuned/bert-ft.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
